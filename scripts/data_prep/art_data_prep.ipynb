{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "cfe1d738",
      "metadata": {},
      "source": [
        "# ART Data Preparation and Cleaning\n",
        "\n",
        "This notebook loads the raw ART pretest data, runs diagnostics, applies cleaning decisions,\n",
        "and saves the cleaned dataset for downstream IRT analysis.\n",
        "\n",
        "**Output:** `data/processed/art_cleaned/`\n",
        "\n",
        "**Documentation:** `docs/art_cleaning/CLEANING_DECISIONS.md`"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6737be1e",
      "metadata": {},
      "source": [
        "## Section 0: Setup and Configuration\n",
        "\n",
        "Import libraries, set paths, and convert Excel files to CSV."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "8a2bea28",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Imports loaded.\n",
            "Started: 2026-02-16 17:36:55\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import re\n",
        "from datetime import datetime\n",
        "\n",
        "print(\"Imports loaded.\")\n",
        "print(f\"Started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "91c70342",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Configuration loaded:\n",
            "  Project root: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor\n",
            "  Output dir:   /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/art_cleaned\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "PROJECT_ROOT = Path(\"../..\")\n",
        "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"art_cleaned\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "ART_FILE = DATA_DIR / \"ART_pretest_merged_EN.xlsx\"\n",
        "REAL_AUTHORS_FILE = DATA_DIR / \"author_lists\" / \"real_authors.xls\"\n",
        "FOILS_FILE = DATA_DIR / \"author_lists\" / \"not_real_authors.xls\"\n",
        "\n",
        "ART_CSV = DATA_DIR / \"ART_pretest_merged_EN.csv\"\n",
        "REAL_AUTHORS_CSV = DATA_DIR / \"author_lists\" / \"real_authors.csv\"\n",
        "FOILS_CSV = DATA_DIR / \"author_lists\" / \"not_real_authors.csv\"\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  Project root: {PROJECT_ROOT.resolve()}\")\n",
        "print(f\"  Output dir:   {OUTPUT_DIR.resolve()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "d9d736bb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: ART_pretest_merged_EN.csv (1837 rows x 220 cols)\n",
            "Saved: real_authors.csv (100 rows)\n",
            "Saved: not_real_authors.csv (107 rows)\n",
            "All Excel files converted to CSV.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CONVERT XLSX/XLS TO CSV\n",
        "# ============================================================\n",
        "df_art = pd.read_excel(ART_FILE, header=None)\n",
        "df_art.to_csv(ART_CSV, index=False, header=False)\n",
        "print(f\"Saved: {ART_CSV.name} ({df_art.shape[0]} rows x {df_art.shape[1]} cols)\")\n",
        "\n",
        "df_real = pd.read_excel(REAL_AUTHORS_FILE, header=None)\n",
        "df_real.to_csv(REAL_AUTHORS_CSV, index=False, header=False)\n",
        "print(f\"Saved: {REAL_AUTHORS_CSV.name} ({df_real.shape[0]} rows)\")\n",
        "\n",
        "df_foils = pd.read_excel(FOILS_FILE, header=None)\n",
        "df_foils.to_csv(FOILS_CSV, index=False, header=False)\n",
        "print(f\"Saved: {FOILS_CSV.name} ({df_foils.shape[0]} rows)\")\n",
        "print(\"All Excel files converted to CSV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b5aa251f",
      "metadata": {},
      "source": [
        "## Section 1: Data Loading and Preprocessing\n",
        "\n",
        "Load the merged ART dataset, identify real authors vs foils, classify items, and run diagnostics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "5c5968f3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Loading ART pretest dataset...\n",
            "   Raw shape: 1837 rows x 220 columns\n",
            "   Data rows: 1835 participants\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD RAW DATA\n",
        "# ============================================================\n",
        "print(\"1. Loading ART pretest dataset...\")\n",
        "raw_df = pd.read_csv(ART_CSV, header=None)\n",
        "labels_row = raw_df.iloc[0].tolist()\n",
        "codes_row = raw_df.iloc[1].tolist()\n",
        "data_df = raw_df.iloc[2:].reset_index(drop=True)\n",
        "data_df.columns = labels_row\n",
        "print(f\"   Raw shape: {raw_df.shape[0]} rows x {raw_df.shape[1]} columns\")\n",
        "print(f\"   Data rows: {len(data_df)} participants\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bee9b916",
      "metadata": {},
      "source": [
        "### Load reference lists and identify items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "5bf3f1b6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2. Loading author reference lists...\n",
            "   Real authors: 100 rows\n",
            "   Foils: 107 rows\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD AUTHOR REFERENCE LISTS\n",
        "# ============================================================\n",
        "print(\"2. Loading author reference lists...\")\n",
        "real_authors_df = pd.read_excel(REAL_AUTHORS_FILE, header=None)\n",
        "foils_df = pd.read_csv(FOILS_CSV, header=None)\n",
        "print(f\"   Real authors: {real_authors_df.shape[0]} rows\")\n",
        "print(f\"   Foils: {foils_df.shape[0]} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "0cb493e2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "3. Identifying item columns...\n",
            "   Item columns: 214\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# IDENTIFY REAL AUTHORS VS FOILS IN DATASET\n",
        "# ============================================================\n",
        "print(\"3. Identifying item columns...\")\n",
        "n_demographics = 5\n",
        "item_labels = labels_row[n_demographics:-1]\n",
        "item_codes = codes_row[n_demographics:-1]\n",
        "data_df.columns = labels_row\n",
        "participant_ids = pd.Index(range(1, len(data_df) + 1), name=\"participant_id\")\n",
        "item_data = data_df[item_labels].copy()\n",
        "source_col = data_df.iloc[:, -1].reset_index(drop=True)\n",
        "print(f\"   Item columns: {len(item_labels)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2659a890",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4. Classifying items as real authors or foils...\n",
            "   Real authors: 106, Foils: 108\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# CLASSIFY ITEMS AS REAL AUTHORS OR FOILS\n",
        "# ============================================================\n",
        "print(\"4. Classifying items as real authors or foils...\")\n",
        "\n",
        "def extract_fill_code(text):\n",
        "    if pd.isna(text): return None\n",
        "    s = re.sub(r'fil\\s+l(\\d+)', r'fill\\1', str(text), flags=re.I)\n",
        "    m = re.search(r'fill\\s*\\d+', s, re.I)\n",
        "    return m.group(0).replace(' ', '') if m else None\n",
        "\n",
        "def is_foil_code(c): return bool(re.match(r'^fill\\s*\\d+$', str(c).strip(), re.I)) if pd.notna(c) else False\n",
        "def is_real_author_code(c): return bool(re.match(r'^(mod|cla|soc)\\d+$', str(c).strip(), re.I)) if pd.notna(c) else False\n",
        "def normalize_name(n): return re.sub(r'\\s*fill\\s*\\d+', '', str(n).strip(), flags=re.I).strip().lower() if pd.notna(n) else \"\"\n",
        "\n",
        "foil_codes_from_ref = {extract_fill_code(n).lower() for n in foils_df.iloc[:, 0] if extract_fill_code(n)}\n",
        "real_names_list = real_authors_df.iloc[:, 0].dropna().tolist()\n",
        "foil_names_list = foils_df.iloc[:, 0].dropna().tolist()\n",
        "real_names_normalized = set(normalize_name(n) for n in real_names_list)\n",
        "\n",
        "real_author_indices, foil_indices = [], []\n",
        "real_author_names, foil_names = [], []\n",
        "for i, (label, code) in enumerate(zip(item_labels, item_codes)):\n",
        "    cn = str(code).strip().lower()\n",
        "    ln = normalize_name(label)\n",
        "    if is_foil_code(code) or cn in foil_codes_from_ref or extract_fill_code(label):\n",
        "        foil_indices.append(i); foil_names.append(label)\n",
        "    elif ln in real_names_normalized or is_real_author_code(code):\n",
        "        real_author_indices.append(i); real_author_names.append(label)\n",
        "\n",
        "item_classification = pd.DataFrame({\n",
        "    'item_index': range(len(item_labels)), 'item_label': item_labels, 'item_code': item_codes,\n",
        "    'is_real_author': [i in real_author_indices for i in range(len(item_labels))],\n",
        "    'is_foil': [i in foil_indices for i in range(len(item_labels))]\n",
        "})\n",
        "print(f\"   Real authors: {len(real_author_indices)}, Foils: {len(foil_indices)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9a07aa1",
      "metadata": {},
      "source": [
        "### Detailed Data Exploration — Pre-Cleaning Diagnostics\n",
        "\n",
        "Run diagnostics before cleaning. See `docs/art_cleaning/CLEANING_DECISIONS.md` for full output and decisions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "e57f9834",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Raw value counts: {'0': 276317, '1': 104812, nan: 11560, 'falce': 1}\n",
            "Non-binary cols: [(0, 'Gerrit HoogenbuM fill1', 'fill1')]\n",
            "Duplicate labels: ['Ian Fleming']\n",
            "Items >40% missing: 9 — [('Vladimir Gusakov fill 101', 'fill101', np.float64(43.596730245231605)), ('Ivan Savin fill 102', 'fill102', np.float64(43.596730245231605)), ('Sergey Nikitin fill 103', 'fill103', np.float64(87.52043596730246)), ('Evgenia Serova fill 104', 'fill104', np.float64(43.596730245231605)), ('Chabo Chucky fill 105', 'fill105', np.float64(43.596730245231605))]\n",
            "Diagnostics complete. See CLEANING_DECISIONS.md for full details.\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# PRE-CLEANING DIAGNOSTICS (summary)\n",
        "# ============================================================\n",
        "from collections import Counter\n",
        "N_PARTICIPANTS = len(data_df)\n",
        "N_ITEMS = len(item_labels)\n",
        "ITEM_START, ITEM_END = n_demographics, len(labels_row) - 1\n",
        "item_block = data_df.iloc[:, ITEM_START:ITEM_END].copy()\n",
        "item_block.columns = range(N_ITEMS)\n",
        "\n",
        "# Raw values audit\n",
        "flat = pd.Series(item_block.values.flatten())\n",
        "print(\"Raw value counts:\", flat.value_counts(dropna=False).head(10).to_dict())\n",
        "non_b = [(i, item_labels[i], item_codes[i]) for i in range(N_ITEMS)\n",
        "         if set(item_block.iloc[:, i].dropna().astype(str).str.strip()) - {\"0\",\"1\",\"0.0\",\"1.0\"}]\n",
        "if non_b: print(\"Non-binary cols:\", non_b)\n",
        "\n",
        "# Duplicates\n",
        "dup_labels = [l for l, c in Counter(item_labels).items() if c > 1]\n",
        "print(\"Duplicate labels:\", dup_labels)\n",
        "\n",
        "# Missing\n",
        "miss_pct = item_block.isnull().sum(axis=0) / N_PARTICIPANTS * 100\n",
        "high_miss = [(item_labels[i], item_codes[i], miss_pct.iloc[i]) for i in range(N_ITEMS) if miss_pct.iloc[i] > 40]\n",
        "print(\"Items >40% missing:\", len(high_miss), \"—\", high_miss[:5])\n",
        "print(\"Diagnostics complete. See CLEANING_DECISIONS.md for full details.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e38aa62b",
      "metadata": {},
      "source": [
        "## Section 2: Data Cleaning\n",
        "\n",
        "Apply cleaning decisions (see `docs/art_cleaning/CLEANING_DECISIONS.md`):\n",
        "\n",
        "1. Recode 'falce' → 0 in Gerrit HoogenbuM fill1  \n",
        "2. Drop Ian Fleming (mod33) — duplicate, 99.9% missing  \n",
        "3. Drop 8 items with >40% missing  \n",
        "4. Keep items with 5–40% missing  \n",
        "5. Ignore duplicate codes (metadata only)  \n",
        "6. Keep near-floor/ceiling items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "4594855d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Step 1: Recoded 1 'falce' → 0 in Gerrit HoogenbuM fill1\n",
            "Step 2–3: Dropping 9 items: ['Vladimir Gusakov fill 101', 'Ivan Savin fill 102', 'Sergey Nikitin fill 103', 'Evgenia Serova fill 104', 'Chabo Chucky fill 105', 'Ilya Ilf', 'Lyudmila Ulitskaya', 'Alexander Tvardovsky', 'Ian Fleming']\n",
            "   Retained: 205 items (205 real + foil)\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# DATA CLEANING — Apply decisions\n",
        "# ============================================================\n",
        "# Work with positional item block to avoid duplicate-name issues\n",
        "item_block = data_df.iloc[:, ITEM_START:ITEM_END].copy()\n",
        "item_block.columns = range(N_ITEMS)\n",
        "\n",
        "# Step 1: Recode 'falce' → 0 in Gerrit HoogenbuM fill1 (item position 0)\n",
        "col_fill1 = 0  # first item column\n",
        "ser = item_block.iloc[:, col_fill1].astype(str).str.strip()\n",
        "n_falce = (ser == \"falce\").sum()\n",
        "# Series replacement and type conversion outside of assignment prevents dtype conflict\n",
        "ser = ser.replace(\"falce\", \"0\")\n",
        "ser = pd.to_numeric(ser, errors=\"coerce\")\n",
        "# Replace entire column with a new Series to avoid dtype conflict with string column\n",
        "item_block[col_fill1] = ser.values\n",
        "print(f\"Step 1: Recoded {n_falce} 'falce' → 0 in Gerrit HoogenbuM fill1\")\n",
        "\n",
        "# Step 2 & 3: Items to DROP (by label, code) — Ian Fleming mod33 + 8 high-missing\n",
        "ITEMS_TO_DROP = [\n",
        "    (\"Ian Fleming\", \"mod33\"),           # duplicate, 99.9% missing\n",
        "    (\"Sergey Nikitin fill 103\", \"fill103\"),\n",
        "    (\"Lyudmila Ulitskaya\", \"mod32\"),\n",
        "    (\"Alexander Tvardovsky\", \"cla27\"),\n",
        "    (\"Ilya Ilf\", \"soc15\"),\n",
        "    (\"Ivan Savin fill 102\", \"fill102\"),\n",
        "    (\"Evgenia Serova fill 104\", \"fill104\"),\n",
        "    (\"Vladimir Gusakov fill 101\", \"fill101\"),\n",
        "    (\"Chabo Chucky fill 105\", \"fill105\"),\n",
        "]\n",
        "indices_to_drop = [i for i in range(N_ITEMS) if (item_labels[i], str(item_codes[i])) in ITEMS_TO_DROP]\n",
        "indices_to_keep = [i for i in range(N_ITEMS) if i not in indices_to_drop]\n",
        "\n",
        "print(f\"Step 2–3: Dropping {len(indices_to_drop)} items: {[item_labels[i] for i in indices_to_drop]}\")\n",
        "\n",
        "# Build cleaned item block\n",
        "item_block_clean = item_block.iloc[:, indices_to_keep].copy()\n",
        "labels_clean = [item_labels[i] for i in indices_to_keep]\n",
        "codes_clean = [item_codes[i] for i in indices_to_keep]\n",
        "N_ITEMS_CLEAN = len(labels_clean)\n",
        "print(f\"   Retained: {N_ITEMS_CLEAN} items ({len(indices_to_keep)} real + foil)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "06a9e8fc",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saved: ART_pretest_merged_EN_cleaned.csv\n",
            "   Shape: 1837 rows x 211 columns\n",
            "   Participants: 1835\n",
            "   Items: 205 (dropped 9)\n",
            "Saved: item_metadata.csv\n",
            "Saved: excluded_items.csv\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# SAVE CLEANED OUTPUT\n",
        "# ============================================================\n",
        "# Build output: same structure as input (row 0 = labels, row 1 = codes, rows 2+ = data)\n",
        "demo_cols = labels_row[:n_demographics]\n",
        "source_label = labels_row[-1]\n",
        "\n",
        "out_labels = demo_cols + labels_clean + [source_label]\n",
        "out_codes = codes_row[:n_demographics] + codes_clean + [codes_row[-1]]  # source has no code, use last\n",
        "\n",
        "# Data: demographics + cleaned items + source\n",
        "data_clean = pd.concat([\n",
        "    data_df.iloc[:, :n_demographics].reset_index(drop=True),\n",
        "    item_block_clean.reset_index(drop=True),\n",
        "    source_col.reset_index(drop=True)\n",
        "], axis=1)\n",
        "data_clean.columns = range(len(out_labels))\n",
        "\n",
        "# Header rows\n",
        "header_df = pd.DataFrame([out_labels, out_codes])\n",
        "out_df = pd.concat([header_df, data_clean], ignore_index=True)\n",
        "\n",
        "# Save main cleaned CSV\n",
        "OUTPUT_CSV = OUTPUT_DIR / \"ART_pretest_merged_EN_cleaned.csv\"\n",
        "out_df.to_csv(OUTPUT_CSV, index=False, header=False)\n",
        "print(f\"Saved: {OUTPUT_CSV.name}\")\n",
        "print(f\"   Shape: {out_df.shape[0]} rows x {out_df.shape[1]} columns\")\n",
        "print(f\"   Participants: {len(data_clean)}\")\n",
        "print(f\"   Items: {N_ITEMS_CLEAN} (dropped {N_ITEMS - N_ITEMS_CLEAN})\")\n",
        "\n",
        "# Save item metadata (retained items only)\n",
        "meta = pd.DataFrame({\n",
        "    \"item_index\": range(N_ITEMS_CLEAN),\n",
        "    \"item_label\": labels_clean,\n",
        "    \"item_code\": codes_clean,\n",
        "    \"is_real_author\": [i in real_author_indices for i in indices_to_keep],\n",
        "    \"is_foil\": [i in foil_indices for i in indices_to_keep],\n",
        "})\n",
        "meta.to_csv(OUTPUT_DIR / \"item_metadata.csv\", index=False)\n",
        "print(f\"Saved: item_metadata.csv\")\n",
        "\n",
        "# Save excluded items log\n",
        "excluded = pd.DataFrame([\n",
        "    {\"item_label\": item_labels[i], \"item_code\": str(item_codes[i]), \"reason\": \">40% missing or duplicate\"}\n",
        "    for i in indices_to_drop\n",
        "])\n",
        "excluded.to_csv(OUTPUT_DIR / \"excluded_items.csv\", index=False)\n",
        "print(f\"Saved: excluded_items.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "d3370b55",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. Loading ART pretest dataset...\n",
            "   Raw shape: 1837 rows x 220 columns\n",
            "   Data rows: 1835 participants\n",
            "   First 5 labels: ['Submited', 'age', 'sex ', 'humanities or not', 'education and profession']\n",
            "   Last 5 labels: ['Ilya Ilf', 'Lyudmila Ulitskaya', 'Alexander Tvardovsky', 'Ian Fleming', 'source']\n"
          ]
        }
      ],
      "source": [
        "# ============================================================\n",
        "# LOAD RAW DATA\n",
        "# ============================================================\n",
        "print(\"1. Loading ART pretest dataset...\")\n",
        "raw_df = pd.read_csv(ART_CSV, header=None)\n",
        "labels_row = raw_df.iloc[0].tolist()\n",
        "codes_row = raw_df.iloc[1].tolist()\n",
        "data_df = raw_df.iloc[2:].reset_index(drop=True)\n",
        "\n",
        "print(f\"   Raw shape: {raw_df.shape[0]} rows x {raw_df.shape[1]} columns\")\n",
        "print(f\"   Data rows: {len(data_df)} participants\")\n",
        "print(f\"   First 5 labels: {labels_row[:5]}\")\n",
        "print(f\"   Last 5 labels: {labels_row[-5:]}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
