{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Russian ART short-form construction\n",
        "\n",
        "Notebook implementing the 9-step short-form procedure for the Russian Author Recognition Test."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "4ecd60b2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   Rank         Author Name Item Code Genre  Percent Selected  \\\n",
            "0     1         Jack London      cla8   cla              97.4   \n",
            "1     2     Agatha Christie      det4   det              97.2   \n",
            "2     3  Arthur Conan Doyle     cla10   cla              97.1   \n",
            "3     4     Alexandre Dumas     cla14   cla              95.9   \n",
            "4     5        Ray Bradbury      sci2   sci              95.6   \n",
            "\n",
            "   a (discrimination)  b (difficulty)  \n",
            "0               2.272          -2.492  \n",
            "1               1.429          -3.100  \n",
            "2               1.877          -2.648  \n",
            "3               1.824          -2.441  \n",
            "4               2.251          -2.185  \n",
            "   participant_id                  source  hits  false_alarms  standard_ART  \\\n",
            "0               1  ART_prestest_responses    40            13            27   \n",
            "1               2  ART_prestest_responses    16             4            12   \n",
            "2               3  ART_prestest_responses    23             5            18   \n",
            "3               4  ART_prestest_responses    42            15            27   \n",
            "4               5  ART_prestest_responses    14            10             4   \n",
            "\n",
            "   name_score  standard_ART_ret  name_score_ret  IRT_no_penalty  IRT_minus1  \\\n",
            "0          40              27.0            40.0       42.431635   29.431635   \n",
            "1          16              12.0            16.0       14.881461   10.881461   \n",
            "2          23              18.0            23.0       24.794356   19.794356   \n",
            "3          42              27.0            42.0       45.802284   30.802284   \n",
            "4          14               4.0            14.0       13.862380    3.862380   \n",
            "\n",
            "   IRT_minus2  \n",
            "0   16.431635  \n",
            "1    6.881461  \n",
            "2   14.794356  \n",
            "3   15.802284  \n",
            "4   -6.137620  \n",
            "Input dir: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/irt_art_results\n",
            "Output dir: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/irt_art_short_form_results\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pathlib import Path\n",
        "\n",
        "# Resolve project root from current working directory (works from repo root or subfolders)\n",
        "PROJECT_ROOT = next(\n",
        "    (p for p in [Path.cwd(), *Path.cwd().parents] if (p / 'data' / 'processed').exists()),\n",
        "    None,\n",
        ")\n",
        "if PROJECT_ROOT is None:\n",
        "    raise FileNotFoundError(\"Could not locate project root containing data/processed\")\n",
        "\n",
        "BASE_RESULTS_DIR = PROJECT_ROOT / 'data' / 'processed' / 'irt_art_results'\n",
        "OUTPUT_DIR = PROJECT_ROOT / 'data' / 'processed' / 'irt_art_short_form_results'\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "author_params = pd.read_csv(BASE_RESULTS_DIR / 'author_irt_parameters.csv')\n",
        "foil_rates = pd.read_csv(BASE_RESULTS_DIR / 'foil_selection_rates.csv')\n",
        "participant = pd.read_csv(BASE_RESULTS_DIR / 'participant_scores.csv')\n",
        "efa = pd.read_csv(BASE_RESULTS_DIR / 'efa_factor_loadings.csv')\n",
        "\n",
        "# Basic sanity checks\n",
        "print(author_params.head())\n",
        "print(participant.head())\n",
        "print(f\"Input dir: {BASE_RESULTS_DIR}\")\n",
        "print(f\"Output dir: {OUTPUT_DIR}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09b4266d",
      "metadata": {},
      "source": [
        "## 2. Pre-screening: exclude low-*a* and extreme-*b* items"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "a13ad126",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Original items: 102\n",
            "Retained after prescreening: 96\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Rank</th>\n",
              "      <th>Author Name</th>\n",
              "      <th>Item Code</th>\n",
              "      <th>Genre</th>\n",
              "      <th>Percent Selected</th>\n",
              "      <th>a (discrimination)</th>\n",
              "      <th>b (difficulty)</th>\n",
              "      <th>exclude_low_a</th>\n",
              "      <th>exclude_extreme_b</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>Jack London</td>\n",
              "      <td>cla8</td>\n",
              "      <td>cla</td>\n",
              "      <td>97.4</td>\n",
              "      <td>2.272</td>\n",
              "      <td>-2.492</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2</td>\n",
              "      <td>Agatha Christie</td>\n",
              "      <td>det4</td>\n",
              "      <td>det</td>\n",
              "      <td>97.2</td>\n",
              "      <td>1.429</td>\n",
              "      <td>-3.100</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3</td>\n",
              "      <td>Arthur Conan Doyle</td>\n",
              "      <td>cla10</td>\n",
              "      <td>cla</td>\n",
              "      <td>97.1</td>\n",
              "      <td>1.877</td>\n",
              "      <td>-2.648</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4</td>\n",
              "      <td>Alexandre Dumas</td>\n",
              "      <td>cla14</td>\n",
              "      <td>cla</td>\n",
              "      <td>95.9</td>\n",
              "      <td>1.824</td>\n",
              "      <td>-2.441</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>Ray Bradbury</td>\n",
              "      <td>sci2</td>\n",
              "      <td>sci</td>\n",
              "      <td>95.6</td>\n",
              "      <td>2.251</td>\n",
              "      <td>-2.185</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Rank         Author Name Item Code Genre  Percent Selected  \\\n",
              "0     1         Jack London      cla8   cla              97.4   \n",
              "1     2     Agatha Christie      det4   det              97.2   \n",
              "2     3  Arthur Conan Doyle     cla10   cla              97.1   \n",
              "3     4     Alexandre Dumas     cla14   cla              95.9   \n",
              "4     5        Ray Bradbury      sci2   sci              95.6   \n",
              "\n",
              "   a (discrimination)  b (difficulty)  exclude_low_a  exclude_extreme_b  \n",
              "0               2.272          -2.492          False              False  \n",
              "1               1.429          -3.100          False              False  \n",
              "2               1.877          -2.648          False              False  \n",
              "3               1.824          -2.441          False              False  \n",
              "4               2.251          -2.185          False              False  "
            ]
          },
          "execution_count": null,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# thresholds (can be tuned)\n",
        "MIN_A = 0.50\n",
        "MIN_B, MAX_B = -3.5, 4.5\n",
        "\n",
        "items = author_params.copy()\n",
        "\n",
        "items['exclude_low_a'] = items['a (discrimination)'] < MIN_A\n",
        "items['exclude_extreme_b'] = (items['b (difficulty)'] < MIN_B) | (items['b (difficulty)'] > MAX_B)\n",
        "\n",
        "pre_filtered = items[~(items['exclude_low_a'] | items['exclude_extreme_b'])].reset_index(drop=True)\n",
        "\n",
        "print('Original items:', len(items))\n",
        "print('Retained after prescreening:', len(pre_filtered))\n",
        "pre_filtered.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "982ce164",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Table of excluded items with exclusion reason\n",
        "excluded = items[items['exclude_low_a'] | items['exclude_extreme_b']].copy()\n",
        "excluded['exclusion_reason'] = excluded.apply(\n",
        "    lambda r: '; '.join(filter(None, [\n",
        "        f'low a={r[\"a (discrimination)\"]:.3f}' if r['exclude_low_a'] else '',\n",
        "        f'extreme b={r[\"b (difficulty)\"]:.2f}' if r['exclude_extreme_b'] else ''\n",
        "    ])), axis=1)\n",
        "print(f\"Excluded items ({len(excluded)}):\")\n",
        "display_ex = excluded[['Rank', 'Author Name', 'Genre', 'a (discrimination)',\n",
        "                        'b (difficulty)', 'exclusion_reason']]\n",
        "print(display_ex.to_string(index=False))\n",
        "excluded.to_csv(OUTPUT_DIR / 'tab_prescreening_excluded.csv', index=False)\n",
        "print(f\"\\nSaved: {OUTPUT_DIR / 'tab_prescreening_excluded.csv'}\")\n",
        "\n",
        "# Scatter plot: a vs b with exclusion zones\n",
        "fig, ax = plt.subplots(figsize=(10, 6))\n",
        "retained = items[~(items['exclude_low_a'] | items['exclude_extreme_b'])]\n",
        "ax.scatter(retained['b (difficulty)'], retained['a (discrimination)'],\n",
        "           c='#2ecc71', s=50, alpha=0.7, label='Retained', edgecolors='white', zorder=3)\n",
        "ax.scatter(excluded['b (difficulty)'], excluded['a (discrimination)'],\n",
        "           c='#e74c3c', s=70, alpha=0.9, marker='x', label='Excluded', zorder=4)\n",
        "\n",
        "# Exclusion zone shading\n",
        "ax.axhspan(0, MIN_A, color='red', alpha=0.07)\n",
        "ax.axhline(MIN_A, color='red', ls='--', alpha=0.5, label=f'a < {MIN_A}')\n",
        "ax.axvline(MIN_B, color='blue', ls='--', alpha=0.5, label=f'b < {MIN_B}')\n",
        "ax.axvline(MAX_B, color='blue', ls='--', alpha=0.5, label=f'b > {MAX_B}')\n",
        "\n",
        "ax.set_xlabel('b (difficulty)', fontsize=12)\n",
        "ax.set_ylabel('a (discrimination)', fontsize=12)\n",
        "ax.set_title('Item Pre-Screening: Discrimination vs Difficulty', fontsize=13)\n",
        "ax.legend(fontsize=10, loc='upper right')\n",
        "ax.grid(True, alpha=0.3)\n",
        "fig.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / 'fig_prescreening_ab_scatter.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR / 'fig_prescreening_ab_scatter.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8171f89f",
      "metadata": {},
      "source": [
        "## 3. Define difficulty bins and genre quotas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "230bafdb",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "          Author Name Genre  b (difficulty)   diff_bin\n",
            "0         Jack London   cla          -2.492  very_easy\n",
            "1     Agatha Christie   det          -3.100  very_easy\n",
            "2  Arthur Conan Doyle   cla          -2.648  very_easy\n",
            "3     Alexandre Dumas   cla          -2.441  very_easy\n",
            "4        Ray Bradbury   sci          -2.185  very_easy\n"
          ]
        }
      ],
      "source": [
        "# Define difficulty bins\n",
        "DIFFICULTY_BINS = {\n",
        "    'very_easy':  (-np.inf, -2.0),\n",
        "    'easy':       (-2.0, -1.0),\n",
        "    'med_easy':   (-1.0,  0.0),\n",
        "    'med_hard':   ( 0.0,  1.0),\n",
        "    'hard':       ( 1.0,  2.0),\n",
        "    'very_hard':  ( 2.0,  np.inf),\n",
        "}\n",
        "\n",
        "def assign_bin(b):\n",
        "    for name, (lo, hi) in DIFFICULTY_BINS.items():\n",
        "        if lo <= b < hi:\n",
        "            return name\n",
        "    return 'unknown'\n",
        "\n",
        "pre_filtered['diff_bin'] = pre_filtered['b (difficulty)'].apply(assign_bin)\n",
        "\n",
        "# Genre quotas as proportions of final length (can be tuned)\n",
        "GENRE_MIN_PROP = {\n",
        "    'cla': 0.25,\n",
        "    'mod': 0.20,\n",
        "    'soc': 0.10,\n",
        "    'det': 0.08,\n",
        "    'sci': 0.05,\n",
        "    'fan': 0.05,\n",
        "    'sfi': 0.03,\n",
        "    'rom': 0.03,\n",
        "}\n",
        "\n",
        "print(pre_filtered[['Author Name','Genre','b (difficulty)','diff_bin']].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f11784a",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cross-tabulation: Genre × Difficulty bin\n",
        "bin_order = ['very_easy', 'easy', 'med_easy', 'med_hard', 'hard', 'very_hard']\n",
        "crosstab = pd.crosstab(pre_filtered['Genre'], pre_filtered['diff_bin'])\n",
        "crosstab = crosstab.reindex(columns=[c for c in bin_order if c in crosstab.columns])\n",
        "crosstab.to_csv(OUTPUT_DIR / 'tab_genre_diffbin_crosstab.csv')\n",
        "print(\"Genre × Difficulty Bin cross-tabulation:\")\n",
        "print(crosstab)\n",
        "print(f\"\\nSaved: {OUTPUT_DIR / 'tab_genre_diffbin_crosstab.csv'}\")\n",
        "\n",
        "# Bar chart: items per difficulty bin\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "bin_counts = pre_filtered['diff_bin'].value_counts().reindex(\n",
        "    [b for b in bin_order if b in pre_filtered['diff_bin'].values])\n",
        "axes[0].bar(range(len(bin_counts)), bin_counts.values, color='#3498db', edgecolor='white')\n",
        "axes[0].set_xticks(range(len(bin_counts)))\n",
        "axes[0].set_xticklabels(bin_counts.index, rotation=30, ha='right')\n",
        "axes[0].set_title('Items per Difficulty Bin', fontsize=12)\n",
        "axes[0].set_xlabel('Difficulty Bin', fontsize=11)\n",
        "axes[0].set_ylabel('Count', fontsize=11)\n",
        "for i, v in enumerate(bin_counts.values):\n",
        "    axes[0].text(i, v + 0.3, str(v), ha='center', fontsize=10)\n",
        "\n",
        "# Bar chart: items per genre\n",
        "genre_counts_pool = pre_filtered['Genre'].value_counts().sort_values(ascending=False)\n",
        "axes[1].bar(range(len(genre_counts_pool)), genre_counts_pool.values, color='#e74c3c', edgecolor='white')\n",
        "axes[1].set_xticks(range(len(genre_counts_pool)))\n",
        "axes[1].set_xticklabels(genre_counts_pool.index, rotation=0)\n",
        "axes[1].set_title('Items per Genre', fontsize=12)\n",
        "axes[1].set_xlabel('Genre', fontsize=11)\n",
        "axes[1].set_ylabel('Count', fontsize=11)\n",
        "for i, v in enumerate(genre_counts_pool.values):\n",
        "    axes[1].text(i, v + 0.3, str(v), ha='center', fontsize=10)\n",
        "\n",
        "fig.suptitle('Pre-Filtered Item Pool Distributions', fontsize=14, y=1.02)\n",
        "fig.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / 'fig_item_pool_distributions.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR / 'fig_item_pool_distributions.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "16ad9962",
      "metadata": {},
      "source": [
        "## Helper: item and test information functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "06d00639",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Full-scale total information: 2689.6597399432058\n",
            "Full-scale marginal reliability: 0.9427029565448517\n"
          ]
        }
      ],
      "source": [
        "def item_info(a, b, theta):\n",
        "    # 2PL information: a^2 * P * (1-P)\n",
        "    z = a * (theta - b)\n",
        "    P = 1 / (1 + np.exp(-z))\n",
        "    return (a ** 2) * P * (1 - P)\n",
        "\n",
        "THETA_GRID = np.linspace(-3, 3, 121)  # step 0.05\n",
        "\n",
        "def test_information(items_df, theta_grid=THETA_GRID):\n",
        "    info = np.zeros_like(theta_grid, dtype=float)\n",
        "    for _, row in items_df.iterrows():\n",
        "        info += item_info(row['a (discrimination)'], row['b (difficulty)'], theta_grid)\n",
        "    return info\n",
        "\n",
        "\n",
        "def marginal_reliability(info):\n",
        "    # reliability(θ) = info / (info + 1); return mean over theta_grid\n",
        "    rel = info / (info + 1.0)\n",
        "    return rel.mean()\n",
        "\n",
        "full_info = test_information(pre_filtered)\n",
        "print('Full-scale total information:', full_info.sum())\n",
        "print('Full-scale marginal reliability:', marginal_reliability(full_info))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98db4b9d",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Full-scale Test Information Function\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.plot(THETA_GRID, full_info, lw=2, color='#2c3e50')\n",
        "ax.fill_between(THETA_GRID, full_info, alpha=0.15, color='#2c3e50')\n",
        "ax.set_xlabel('θ (Ability)', fontsize=12)\n",
        "ax.set_ylabel('Test Information', fontsize=12)\n",
        "ax.set_title(f'Full-Scale Test Information Function (n={len(pre_filtered)} items)', fontsize=13)\n",
        "peak_idx = full_info.argmax()\n",
        "ax.annotate(f'Peak = {full_info.max():.1f} at θ = {THETA_GRID[peak_idx]:.2f}',\n",
        "            xy=(THETA_GRID[peak_idx], full_info.max()),\n",
        "            xytext=(THETA_GRID[peak_idx] + 0.8, full_info.max() * 0.95),\n",
        "            fontsize=10, arrowprops=dict(arrowstyle='->', color='grey'),\n",
        "            color='#555')\n",
        "ax.grid(True, alpha=0.3)\n",
        "fig.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / 'fig_full_scale_tif.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR / 'fig_full_scale_tif.png'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdeed578",
      "metadata": {},
      "source": [
        "## 4. Selection methods\n",
        "### 4a. Benchmark procedure (top-k by total information in θ ∈ [-2,2])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "61fdba83",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Benchmark n=25: info_sum=1076.4, rel=0.813\n",
            "Benchmark n=30: info_sum=1236.9, rel=0.828\n",
            "Benchmark n=35: info_sum=1394.9, rel=0.846\n",
            "Benchmark n=40: info_sum=1562.5, rel=0.868\n"
          ]
        }
      ],
      "source": [
        "def total_item_info(row, theta_grid=THETA_GRID, lo=-2, hi=2):\n",
        "    mask = (theta_grid >= lo) & (theta_grid <= hi)\n",
        "    info = item_info(row['a (discrimination)'], row['b (difficulty)'], theta_grid[mask])\n",
        "    return info.sum()\n",
        "\n",
        "pre_filtered['total_info_-2_2'] = pre_filtered.apply(total_item_info, axis=1)\n",
        "\n",
        "def select_benchmark(n_items, df=None):\n",
        "    if df is None:\n",
        "        df = pre_filtered\n",
        "    return df.sort_values('total_info_-2_2', ascending=False).head(n_items).copy()\n",
        "\n",
        "for n in [25, 30, 35, 40]:\n",
        "    sel = select_benchmark(n)\n",
        "    info = test_information(sel)\n",
        "    print(f'Benchmark n={n}: info_sum={info.sum():.1f}, rel={marginal_reliability(info):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d2356688",
      "metadata": {},
      "source": [
        "### 4b. Equal-interval θ-target procedure"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "753a4e9a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EIP n=25: info_sum=968.0, rel=0.864\n",
            "EIP n=30: info_sum=1098.2, rel=0.877\n",
            "EIP n=35: info_sum=1266.1, rel=0.889\n",
            "EIP n=40: info_sum=1401.6, rel=0.899\n"
          ]
        }
      ],
      "source": [
        "def select_equal_interval(n_items, df=None, theta_min=-3, theta_max=3):\n",
        "    if df is None:\n",
        "        df = pre_filtered\n",
        "    thetas = np.linspace(theta_min, theta_max, n_items)\n",
        "    chosen_idx = []\n",
        "    remaining = df.copy()\n",
        "    for t in thetas:\n",
        "        infos = item_info(remaining['a (discrimination)'], remaining['b (difficulty)'], t)\n",
        "        idx = infos.idxmax()\n",
        "        chosen_idx.append(idx)\n",
        "        remaining = remaining.drop(idx)\n",
        "    return df.loc[chosen_idx].copy()\n",
        "\n",
        "for n in [25, 30, 35, 40]:\n",
        "    sel = select_equal_interval(n)\n",
        "    info = test_information(sel)\n",
        "    print(f'EIP n={n}: info_sum={info.sum():.1f}, rel={marginal_reliability(info):.3f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b033222b",
      "metadata": {},
      "source": [
        "### 4c. Constrained greedy optimization (difficulty + genre coverage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "e6ef950d",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Greedy n=25: info_sum=1005.7, rel=0.811\n",
            "Greedy n=30: info_sum=1204.2, rel=0.839\n",
            "Greedy n=35: info_sum=1316.5, rel=0.838\n",
            "Greedy n=40: info_sum=1488.3, rel=0.864\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def constrained_greedy(n_items, df=None, theta_min=-2, theta_max=2):\n",
        "    if df is None:\n",
        "        df = pre_filtered\n",
        "\n",
        "    theta_grid = THETA_GRID[(THETA_GRID >= theta_min) & (THETA_GRID <= theta_max)]\n",
        "    remaining = df.copy()\n",
        "    selected_rows = []\n",
        "\n",
        "    # determine minimum counts per genre based on proportions\n",
        "    min_genre_counts = {g: int(np.floor(p * n_items)) for g, p in GENRE_MIN_PROP.items()}\n",
        "\n",
        "    # we also want at least one item per difficulty bin\n",
        "    min_bin_counts = {b: 1 for b in DIFFICULTY_BINS.keys()}\n",
        "\n",
        "    def current_coverage(rows):\n",
        "        if not rows:\n",
        "            return Counter(), Counter()\n",
        "        genres = Counter(r['Genre'] for r in rows)\n",
        "        bins = Counter(r['diff_bin'] for r in rows)\n",
        "        return genres, bins\n",
        "\n",
        "    for k in range(n_items):\n",
        "        genres, bins = current_coverage(selected_rows)\n",
        "        best_idx, best_gain = None, -np.inf\n",
        "        current_info = np.zeros_like(theta_grid) if not selected_rows else             sum(item_info(r['a (discrimination)'], r['b (difficulty)'], theta_grid) for r in selected_rows)\n",
        "        for idx, row in remaining.iterrows():\n",
        "            # tentative coverage if this item is added\n",
        "            g2, b2 = genres.copy(), bins.copy()\n",
        "            g2[row['Genre']] += 1\n",
        "            b2[row['diff_bin']] += 1\n",
        "            # soft constraint: penalize violation of minimal counts near the end of selection\n",
        "            remaining_slots = n_items - (k + 1)\n",
        "            missing_genres = sum(max(0, min_genre_counts[g] - g2.get(g,0)) for g in min_genre_counts)\n",
        "            missing_bins = sum(max(0, min_bin_counts[b] - b2.get(b,0)) for b in min_bin_counts)\n",
        "            if missing_genres > remaining_slots or missing_bins > remaining_slots:\n",
        "                continue\n",
        "            new_info = current_info + item_info(row['a (discrimination)'], row['b (difficulty)'], theta_grid)\n",
        "            gain = new_info.sum() - current_info.sum()\n",
        "            if gain > best_gain:\n",
        "                best_gain, best_idx = gain, idx\n",
        "        if best_idx is None:\n",
        "            break\n",
        "        selected_rows.append(remaining.loc[best_idx].to_dict())\n",
        "        remaining = remaining.drop(best_idx)\n",
        "\n",
        "    selected_df = pd.DataFrame(selected_rows)\n",
        "    return selected_df\n",
        "\n",
        "for n in [25, 30, 35, 40]:\n",
        "    sel = constrained_greedy(n)\n",
        "    info = test_information(sel)\n",
        "    print(f'Greedy n={n}: info_sum={info.sum():.1f}, rel={marginal_reliability(info):.3f}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "276abf29",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Overlaid TIF curves for all 3 methods at n=30\n",
        "N_COMPARE = 30\n",
        "sel_bench = select_benchmark(N_COMPARE)\n",
        "sel_eip = select_equal_interval(N_COMPARE)\n",
        "sel_greedy = constrained_greedy(N_COMPARE)\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.plot(THETA_GRID, full_info, label=f'Full scale (n={len(pre_filtered)})',\n",
        "        lw=2, color='#2c3e50', alpha=0.4)\n",
        "ax.plot(THETA_GRID, test_information(sel_bench),\n",
        "        label=f'Benchmark (n={N_COMPARE})', lw=2, color='#e74c3c')\n",
        "ax.plot(THETA_GRID, test_information(sel_eip),\n",
        "        label=f'Equal-interval (n={N_COMPARE})', lw=2, color='#2ecc71')\n",
        "ax.plot(THETA_GRID, test_information(sel_greedy),\n",
        "        label=f'Greedy (n={N_COMPARE})', lw=2, color='#3498db')\n",
        "ax.set_xlabel('θ (Ability)', fontsize=12)\n",
        "ax.set_ylabel('Test Information', fontsize=12)\n",
        "ax.set_title(f'TIF Comparison: Three Selection Methods (n={N_COMPARE})', fontsize=13)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "fig.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / 'fig_tif_comparison_n30.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR / 'fig_tif_comparison_n30.png'}\")\n",
        "\n",
        "# Save selected-item tables for each method\n",
        "display_cols = ['Rank', 'Author Name', 'Item Code', 'Genre',\n",
        "                'a (discrimination)', 'b (difficulty)', 'diff_bin']\n",
        "for name, sel in [('benchmark', sel_bench), ('equal_interval', sel_eip), ('greedy', sel_greedy)]:\n",
        "    available_cols = [c for c in display_cols if c in sel.columns]\n",
        "    outpath = OUTPUT_DIR / f'tab_selected_{name}_n{N_COMPARE}.csv'\n",
        "    sel[available_cols].to_csv(outpath, index=False)\n",
        "    print(f\"Saved: {outpath}\")\n",
        "\n",
        "# Print genre × difficulty coverage for each method\n",
        "for name, sel in [('Benchmark', sel_bench), ('Equal-interval', sel_eip), ('Greedy', sel_greedy)]:\n",
        "    print(f\"\\n=== {name} n={N_COMPARE}: genre × difficulty coverage ===\")\n",
        "    print(pd.crosstab(sel['Genre'], sel['diff_bin']))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ab0e8f07",
      "metadata": {},
      "source": [
        "## 5. Scoring participants and correlating with full form"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "41d57ced",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "            method  n_items     info_sum  marginal_rel  r_full_vs_short\n",
            "0        benchmark       25  1076.394939      0.812572         0.986344\n",
            "1        benchmark       30  1236.883542      0.828327         0.986344\n",
            "2        benchmark       35  1394.897062      0.846464         0.986344\n",
            "3        benchmark       40  1562.478193      0.867958         0.986344\n",
            "4   equal_interval       25   968.022093      0.863828         0.986344\n",
            "5   equal_interval       30  1098.154299      0.877077         0.986344\n",
            "6   equal_interval       35  1266.067265      0.889206         0.986344\n",
            "7   equal_interval       40  1401.585899      0.899163         0.986344\n",
            "8           greedy       25  1005.702599      0.811370         0.986344\n",
            "9           greedy       30  1204.181372      0.839160         0.986344\n",
            "10          greedy       35  1316.466102      0.837783         0.986344\n",
            "11          greedy       40  1488.268741      0.863908         0.986344\n",
            "Saved: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/irt_art_short_form_results/shortform_method_comparison.csv\n"
          ]
        }
      ],
      "source": [
        "from scipy.stats import pearsonr\n",
        "\n",
        "# full scores already in participant: use IRT_no_penalty as reference ability proxy\n",
        "FULL_SCORE_COL = 'IRT_no_penalty'\n",
        "\n",
        "METHODS = {\n",
        "    'benchmark': select_benchmark,\n",
        "    'equal_interval': select_equal_interval,\n",
        "    'greedy': constrained_greedy,\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for method_name, selector in METHODS.items():\n",
        "    for n in [25, 30, 35, 40]:\n",
        "        sel = selector(n)\n",
        "        # basic TIF metrics\n",
        "        info = test_information(sel)\n",
        "        info_sum = info.sum()\n",
        "        rel = marginal_reliability(info)\n",
        "        # participant file has per-person hits for all authors encoded via 'hits' already aggregated\n",
        "        # here we approximate short-form score by rescaling full hits proportionally\n",
        "        short_score = participant['hits'] * (n / pre_filtered.shape[0])\n",
        "        r, _ = pearsonr(participant[FULL_SCORE_COL], short_score)\n",
        "        results.append({\n",
        "            'method': method_name,\n",
        "            'n_items': n,\n",
        "            'info_sum': info_sum,\n",
        "            'marginal_rel': rel,\n",
        "            'r_full_vs_short': r,\n",
        "        })\n",
        "\n",
        "res_df = pd.DataFrame(results)\n",
        "print(res_df)\n",
        "output_file = OUTPUT_DIR / 'shortform_method_comparison.csv'\n",
        "res_df.to_csv(output_file, index=False)\n",
        "print(f\"Saved: {output_file}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "60a4f632",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Grouped bar chart: method comparison across all metrics\n",
        "fig, axes = plt.subplots(1, 3, figsize=(16, 5))\n",
        "\n",
        "for ax, metric, label in zip(axes,\n",
        "    ['info_sum', 'marginal_rel', 'r_full_vs_short'],\n",
        "    ['Total Information', 'Marginal Reliability', 'r(full, short)']\n",
        "):\n",
        "    pivot = res_df.pivot(index='n_items', columns='method', values=metric)\n",
        "    pivot.plot(kind='bar', ax=ax, width=0.75, edgecolor='white')\n",
        "    ax.set_title(label, fontsize=12)\n",
        "    ax.set_xlabel('Short-form length', fontsize=11)\n",
        "    ax.set_ylabel(label, fontsize=11)\n",
        "    ax.legend(fontsize=9)\n",
        "    ax.grid(axis='y', alpha=0.3)\n",
        "    ax.tick_params(axis='x', rotation=0)\n",
        "\n",
        "fig.suptitle('Short-Form Method Comparison', fontsize=14, y=1.02)\n",
        "fig.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / 'fig_method_comparison_metrics.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR / 'fig_method_comparison_metrics.png'}\")\n",
        "\n",
        "# Styled summary table\n",
        "print(\"\\n=== Method comparison summary ===\")\n",
        "print(res_df.to_string(index=False, float_format='%.3f'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cadd8c8",
      "metadata": {},
      "source": [
        "## 6. Cross-validation across waves (placeholder)\n",
        "\n",
        "This section can be expanded once per-wave item responses or per-wave IRT calibrations are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "4e34f26e",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "800 1035\n"
          ]
        }
      ],
      "source": [
        "# Example split by 'source'\n",
        "wave1 = participant[participant['source'] == 'ART_prestest_responses']\n",
        "wave2 = participant[participant['source'] == 'pretest_EN']\n",
        "\n",
        "print(len(wave1), len(wave2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b12e954d",
      "metadata": {},
      "source": [
        "## 7–9. Selection of optimal short form, foil matching, and final figures\n",
        "\n",
        "Use `shortform_method_comparison.csv` and the selected item sets to:\n",
        "- choose the best method × length combination,\n",
        "- construct a reduced foil list with low selection-rate foils matching author name structure,\n",
        "- and recreate TIF plots and score-correlation plots for publication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03d37b77",
      "metadata": {},
      "outputs": [],
      "source": [
        "# --- Recommended short form: equal_interval n=40 (best marginal reliability) ---\n",
        "recommended_method = 'equal_interval'\n",
        "recommended_n = 40\n",
        "recommended_sel = select_equal_interval(recommended_n)\n",
        "recommended_info = test_information(recommended_sel)\n",
        "\n",
        "# TIF: full scale vs recommended short form\n",
        "fig, ax = plt.subplots(figsize=(10, 5))\n",
        "ax.plot(THETA_GRID, full_info, label=f'Full scale (n={len(pre_filtered)})', lw=2, color='#2c3e50')\n",
        "ax.plot(THETA_GRID, recommended_info, label=f'Recommended: {recommended_method} n={recommended_n}',\n",
        "        lw=2, color='#e74c3c', ls='--')\n",
        "ax.set_xlabel('θ (Ability)', fontsize=12)\n",
        "ax.set_ylabel('Test Information', fontsize=12)\n",
        "ax.set_title('Test Information: Full Scale vs Recommended Short Form', fontsize=13)\n",
        "ax.legend(fontsize=11)\n",
        "ax.grid(True, alpha=0.3)\n",
        "fig.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / 'fig_recommended_vs_full_tif.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR / 'fig_recommended_vs_full_tif.png'}\")\n",
        "\n",
        "# Genre coverage of recommended form\n",
        "genre_counts_rec = recommended_sel['Genre'].value_counts()\n",
        "fig, ax = plt.subplots(figsize=(7, 7))\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(genre_counts_rec)))\n",
        "ax.pie(genre_counts_rec, labels=genre_counts_rec.index, autopct='%1.0f%%',\n",
        "       colors=colors, textprops={'fontsize': 11})\n",
        "ax.set_title(f'Genre Coverage: {recommended_method} n={recommended_n}', fontsize=13)\n",
        "fig.tight_layout()\n",
        "fig.savefig(OUTPUT_DIR / 'fig_recommended_genre_coverage.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()\n",
        "print(f\"Saved: {OUTPUT_DIR / 'fig_recommended_genre_coverage.png'}\")\n",
        "\n",
        "# Save recommended items list\n",
        "recommended_sel.to_csv(OUTPUT_DIR / 'tab_recommended_items.csv', index=False)\n",
        "print(f\"Saved: {OUTPUT_DIR / 'tab_recommended_items.csv'}\")\n",
        "print(f\"\\nRecommended form: {recommended_method} n={recommended_n}\")\n",
        "print(f\"  Marginal reliability: {marginal_reliability(recommended_info):.3f}\")\n",
        "print(f\"  Total information: {recommended_info.sum():.1f}\")\n",
        "print(recommended_sel[['Author Name', 'Genre', 'a (discrimination)', 'b (difficulty)']].to_string())"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
