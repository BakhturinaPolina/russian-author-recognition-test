{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8f03d85a",
      "metadata": {},
      "source": [
        "# Moore & Gordon–style ART pre-scoring (run before factor analysis)\n",
        "\n",
        "This notebook computes Moore & Gordon ART-style scores and item statistics (no IRT). It detects author/foil items via the code row of the cleaned ART file, enforces binary responses, performs scoring + diagnostics, and saves outputs to `data/processed/results/01_pre_scoring_moore_gordon` (timestamped run folder).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8574d11e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 1: Setup environment, paths, results dir\n",
        "# ============================================\n",
        "\n",
        "import os\n",
        "import re\n",
        "import json\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from datetime import datetime\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Project root: walk up until we find the actual data file (handles cwd = repo root or scripts/analysis)\n",
        "_DATA_FILE = \"data/processed/art_cleaned/ART_pretest_merged_EN_cleaned.csv\"\n",
        "PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
        "while PROJECT_ROOT:\n",
        "    candidate = os.path.join(PROJECT_ROOT, _DATA_FILE)\n",
        "    if os.path.isfile(candidate):\n",
        "        break\n",
        "    _parent = os.path.dirname(PROJECT_ROOT)\n",
        "    if _parent == PROJECT_ROOT:\n",
        "        PROJECT_ROOT = os.path.abspath(os.getcwd())\n",
        "        candidate = os.path.join(PROJECT_ROOT, _DATA_FILE)\n",
        "        break\n",
        "    PROJECT_ROOT = _parent\n",
        "\n",
        "# Input path\n",
        "DATA_PATH = os.path.join(PROJECT_ROOT, _DATA_FILE)\n",
        "\n",
        "# Results folder (timestamped run under base directory)\n",
        "BASE_RESULTS_DIR = os.path.join(PROJECT_ROOT, \"data\", \"processed\", \"results\", \"01_pre_scoring_moore_gordon\")\n",
        "RUN_ID = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "RESULTS_DIR = os.path.join(BASE_RESULTS_DIR, f\"run_{RUN_ID}\")\n",
        "os.makedirs(RESULTS_DIR, exist_ok=True)\n",
        "\n",
        "print(\"PROJECT_ROOT:\", PROJECT_ROOT)\n",
        "print(\"DATA_PATH:\", DATA_PATH)\n",
        "print(\"RESULTS_DIR:\", RESULTS_DIR)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eed64e50",
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================\n",
        "# CELL 2: Load dataset and basic checks\n",
        "# ============================================\n",
        "\n",
        "df_raw = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# First row holds item codes; participant responses start on the next row\n",
        "codes_row = df_raw.iloc[0].fillna(\"\").astype(str).str.strip()\n",
        "df = df_raw.iloc[1:].reset_index(drop=True)\n",
        "\n",
        "print(\"Loaded dataset (responses only after removing code row).\")\n",
        "print(\"Shape (rows, cols):\", df.shape)\n",
        "print(\"First 5 columns:\", list(df.columns[:5]))\n",
        "print(\"Last 5 columns:\", list(df.columns[-5:]))\n",
        "\n",
        "# Quick missingness snapshot\n",
        "na_rate = df.isna().mean().sort_values(ascending=False).head(10)\n",
        "print(\"\\nTop-10 columns by missing rate:\")\n",
        "print(na_rate)\n",
        "\n",
        "# Save basic metadata\n",
        "meta = {\n",
        "    \"data_path\": DATA_PATH,\n",
        "    \"n_rows\": int(df.shape[0]),\n",
        "    \"n_cols\": int(df.shape[1]),\n",
        "    \"run_id\": RUN_ID,\n",
        "}\n",
        "with open(os.path.join(RESULTS_DIR, \"metadata.json\"), \"w\") as f:\n",
        "    json.dump(meta, f, indent=2)\n",
        "\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1371b9be",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 3: Identify real-author and foil item columns\n",
        "# =========================================================\n",
        "# Auto: use code row prefixes (authors: cla/det/fan/mod/sci/sfi/rom/soc; foils: fill*)\n",
        "# Manual override: populate AUTHOR_COLS_MANUAL / FOIL_COLS_MANUAL if needed.\n",
        "\n",
        "ALLOWED_AUTHOR_PREFIXES = (\"cla\", \"det\", \"fan\", \"mod\", \"sci\", \"sfi\", \"rom\", \"soc\")\n",
        "FOIL_PREFIX = \"fill\"\n",
        "\n",
        "# =======================\n",
        "# PATCH 1: Code-row audit\n",
        "# =======================\n",
        "\n",
        "# Build an auditable column↔code mapping\n",
        "codes_row = codes_row.astype(str)  # ensure string\n",
        "colmap = pd.DataFrame({\n",
        "    \"column_name\": df_raw.columns,\n",
        "    \"code_row_value\": codes_row.values\n",
        "})\n",
        "colmap[\"code_norm\"] = (\n",
        "    colmap[\"code_row_value\"]\n",
        "    .str.strip().str.lower()\n",
        "    .str.replace(r\"\\s+\", \"\", regex=True)\n",
        ")\n",
        "\n",
        "# Save mapping for later verification\n",
        "colmap_path = os.path.join(RESULTS_DIR, \"column_code_map.csv\")\n",
        "colmap.to_csv(colmap_path, index=False)\n",
        "print(\"Saved column↔code map:\", colmap_path)\n",
        "\n",
        "# Sanity-check that the code row looks like a code row (not participant data)\n",
        "ALLOWED_AUTHOR_PREFIXES = (\"cla\",\"det\",\"fan\",\"mod\",\"sci\",\"sfi\",\"rom\",\"soc\")\n",
        "FOIL_PREFIX = \"fill\"\n",
        "\n",
        "n_author_like = int(colmap[\"code_norm\"].str.startswith(ALLOWED_AUTHOR_PREFIXES).sum())\n",
        "n_foil_like   = int(colmap[\"code_norm\"].str.startswith(FOIL_PREFIX).sum())\n",
        "\n",
        "print(\"Sanity counts from code row:\")\n",
        "print(\"  author-coded columns:\", n_author_like)\n",
        "print(\"  foil-coded columns  :\", n_foil_like)\n",
        "\n",
        "# Fail-fast thresholds (tune if needed, but do not remove)\n",
        "if n_author_like < 20 or n_foil_like < 20:\n",
        "    raise ValueError(\n",
        "        \"Code-row classification looks wrong (too few author/foil-coded columns). \"\n",
        "        \"Check whether row 0 is truly the code row and whether columns are aligned.\"\n",
        "    )\n",
        "\n",
        "code_series = codes_row.str.lower().str.strip()\n",
        "\n",
        "author_cols_auto = code_series[code_series.str.startswith(ALLOWED_AUTHOR_PREFIXES)].index.tolist()\n",
        "foil_cols_auto = code_series[code_series.str.startswith(FOIL_PREFIX)].index.tolist()\n",
        "\n",
        "print(\"AUTO DETECTION:\")\n",
        "print(\"# author cols detected:\", len(author_cols_auto))\n",
        "print(\"# foil cols detected:\", len(foil_cols_auto))\n",
        "\n",
        "# Manual overrides (leave empty to use auto)\n",
        "AUTHOR_COLS_MANUAL = []\n",
        "FOIL_COLS_MANUAL = []\n",
        "\n",
        "author_cols = AUTHOR_COLS_MANUAL if len(AUTHOR_COLS_MANUAL) > 0 else author_cols_auto\n",
        "foil_cols = FOIL_COLS_MANUAL if len(FOIL_COLS_MANUAL) > 0 else foil_cols_auto\n",
        "\n",
        "# =========================================\n",
        "# Exclude late-added items (not administered to full sample / high missingness)\n",
        "# =========================================\n",
        "ITEMS_EXCLUDED_FROM_ANALYSIS = [\n",
        "    # Late-added / not administered to full sample\n",
        "    \"Yuri Tsypkin\",\n",
        "    \"Victor Khlystun fill 92\",\n",
        "    \"Andrea Segre fill 93\",\n",
        "    \"Natalya Shagaida fill 94\",\n",
        "    \"Ivan Buzdalov fill 95\",\n",
        "    \"Ivan Ushachev fill 96\",\n",
        "    \"Holger Magel fill 97\",\n",
        "    \"Vasily Uzun fill 98\",\n",
        "    \"Sergey Siptits fill 99\",\n",
        "    \"Valentina Shirokova fill 100\",\n",
        "    # Top missing proportion among remainder (per review)\n",
        "    \"Ian Fleming\",\n",
        "    \"Gerrit HoogenbuM fill1\",\n",
        "    \"Lawrense Stern\",\n",
        "    \"Yakushkina Gilyan fill 83\",\n",
        "]\n",
        "author_cols = [c for c in author_cols if c not in ITEMS_EXCLUDED_FROM_ANALYSIS]\n",
        "foil_cols = [c for c in foil_cols if c not in ITEMS_EXCLUDED_FROM_ANALYSIS]\n",
        "print(\"Excluded from analysis (late-added / high missingness):\", len(ITEMS_EXCLUDED_FROM_ANALYSIS), \"items\")\n",
        "print(\"  Authors after exclusion:\", len(author_cols))\n",
        "print(\"  Foils after exclusion:\", len(foil_cols))\n",
        "\n",
        "# =========================================\n",
        "# PATCH 2: Ensure author/foil sets disjoint\n",
        "# =========================================\n",
        "author_set = set(author_cols)\n",
        "foil_set = set(foil_cols)\n",
        "overlap = sorted(list(author_set & foil_set))\n",
        "if overlap:\n",
        "    raise ValueError(f\"Author/Foil column overlap detected (should be impossible): {overlap[:20]}\")\n",
        "\n",
        "if len(author_cols) == 0 or len(foil_cols) == 0:\n",
        "    raise ValueError(\n",
        "        \"MISSING item columns: Could not identify author/foil columns. \"\n",
        "        \"Rename with auth_/foil_ prefixes or fill AUTHOR_COLS_MANUAL/FOIL_COLS_MANUAL.\"\n",
        "    )\n",
        "\n",
        "print(\"\\nFINAL ITEM COUNTS:\")\n",
        "print(\"Authors:\", len(author_cols))\n",
        "print(\"Foils:\", len(foil_cols))\n",
        "print(\"Total items:\", len(author_cols) + len(foil_cols))\n",
        "\n",
        "# Save item lists\n",
        "pd.Series(author_cols, name=\"author_item\").to_csv(\n",
        "    os.path.join(RESULTS_DIR, \"author_item_list.csv\"), index=False\n",
        ")\n",
        "pd.Series(foil_cols, name=\"foil_item\").to_csv(\n",
        "    os.path.join(RESULTS_DIR, \"foil_item_list.csv\"), index=False\n",
        ")\n",
        "print(\"\\nSaved item lists to results.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "68f7f832",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 4: Extract and validate binary response matrix\n",
        "# =========================================================\n",
        "\n",
        "item_cols = author_cols + foil_cols\n",
        "X = df[item_cols].copy()\n",
        "\n",
        "# ======================================================\n",
        "# PATCH 3: Strict binary validation + missingness tracing\n",
        "# ======================================================\n",
        "\n",
        "# Convert to numeric but DO NOT silently coerce bad strings into usable values without logging\n",
        "X_raw = X.copy()\n",
        "X_num = X_raw.apply(pd.to_numeric, errors=\"coerce\")\n",
        "\n",
        "# Log columns that created NaNs after coercion (often indicates parsing problems)\n",
        "na_by_col = X_num.isna().mean().sort_values(ascending=False)\n",
        "na_report_path = os.path.join(RESULTS_DIR, \"missingness_by_item_column.csv\")\n",
        "na_by_col.to_csv(na_report_path, header=[\"missing_rate\"])\n",
        "print(\"Saved missingness report:\", na_report_path)\n",
        "\n",
        "cols_with_na = na_by_col[na_by_col > 0].index.tolist()\n",
        "if cols_with_na:\n",
        "    sample_col = cols_with_na[0]\n",
        "    print(\"Example column with NaNs after coercion:\", sample_col)\n",
        "    print(\"Raw values (first 20):\", X_raw[sample_col].head(20).tolist())\n",
        "\n",
        "    # Show where the NaNs actually are and what the raw values look like\n",
        "    nan_idx = X_num[sample_col].isna()\n",
        "    print(f\"NaNs in {sample_col}: {nan_idx.sum()} rows\")\n",
        "    if nan_idx.any():\n",
        "        print(\"Example rows with NaN in this column (first 10 indices):\", nan_idx[nan_idx].index[:10].tolist())\n",
        "        print(\"Raw values at those rows:\", X_raw.loc[nan_idx, sample_col].head(10).tolist())\n",
        "\n",
        "# --- Enhanced missingness + coercion logging ---\n",
        "before_na = int(X_raw.isna().sum().sum())\n",
        "missing_count = int(X_num.isna().sum().sum())\n",
        "new_na = missing_count - before_na\n",
        "\n",
        "print(f\"Missing cells before numeric coercion: {before_na}\")\n",
        "print(f\"Missing cells after numeric coercion:  {missing_count}\")\n",
        "print(f\"New NaNs introduced by coercion:       {new_na}\")\n",
        "\n",
        "# Preserve NaN-containing numeric matrix for downstream missingness analysis (Cells 16-17)\n",
        "X_num_prefill = X_num.copy()\n",
        "\n",
        "# --- Strict binary validation (on non-NaN values only) ---\n",
        "nonbinary = ~X_num.isin([0, 1]) & X_num.notna()\n",
        "if nonbinary.any().any():\n",
        "    offenders = X_num[nonbinary].stack().value_counts()\n",
        "    print(\"Non-binary values found (will raise error):\")\n",
        "    print(offenders.head(20))\n",
        "    raise ValueError(\n",
        "        f\"Non-binary values present in item matrix. Fix upstream cleaning.\"\n",
        "    )\n",
        "\n",
        "# --- Strict NaN handling: do NOT fill missing with 0 ---\n",
        "# Scores will be NaN for any participant with missing item(s).\n",
        "X = X_num  # float matrix: 0.0, 1.0, or NaN\n",
        "n_participants_any_missing = int(X.isna().any(axis=1).sum())\n",
        "print(f\"\\nBinary matrix (NaN-preserving): {X.shape}\")\n",
        "print(f\"Total missing cells: {missing_count}\")\n",
        "print(f\"Participants with any missing item: {n_participants_any_missing} / {X.shape[0]}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08a79a2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 5: Compute scoring per Moore & Gordon definitions\n",
        "#   Strict NaN + prorated-with-threshold option\n",
        "# =========================================================\n",
        "\n",
        "Xa = X[author_cols]\n",
        "Xf = X[foil_cols]\n",
        "\n",
        "# Strict NaN: score = NaN if any item in the group is missing\n",
        "hits = Xa.sum(axis=1, min_count=len(author_cols))\n",
        "false_alarms = Xf.sum(axis=1, min_count=len(foil_cols))\n",
        "standard_score = hits - false_alarms\n",
        "name_score = hits\n",
        "\n",
        "# Prorate-with-threshold: scale to full length, require high completeness\n",
        "min_frac = 0.975\n",
        "nA = Xa.notna().sum(axis=1)\n",
        "nF = Xf.notna().sum(axis=1)\n",
        "\n",
        "hits_raw = Xa.sum(axis=1, skipna=True)\n",
        "fa_raw = Xf.sum(axis=1, skipna=True)\n",
        "\n",
        "hits_pr = hits_raw * (len(author_cols) / nA.replace(0, np.nan))\n",
        "fa_pr = fa_raw * (len(foil_cols) / nF.replace(0, np.nan))\n",
        "\n",
        "ok = (nA >= min_frac * len(author_cols)) & (nF >= min_frac * len(foil_cols))\n",
        "\n",
        "hits_pr = hits_pr.where(ok)\n",
        "fa_pr = fa_pr.where(ok)\n",
        "name_score_pr = hits_pr\n",
        "standard_score_pr = hits_pr - fa_pr\n",
        "\n",
        "scores = pd.DataFrame({\n",
        "    \"hits\": hits,\n",
        "    \"false_alarms\": false_alarms,\n",
        "    \"standard_score\": standard_score,\n",
        "    \"name_score\": name_score,\n",
        "})\n",
        "\n",
        "scores_prorated = pd.DataFrame({\n",
        "    \"hits\": hits_pr,\n",
        "    \"false_alarms\": fa_pr,\n",
        "    \"standard_score\": standard_score_pr,\n",
        "    \"name_score\": name_score_pr,\n",
        "})\n",
        "\n",
        "n_nan_scores = int(scores[\"standard_score\"].isna().sum())\n",
        "n_valid_scores = int(scores[\"standard_score\"].notna().sum())\n",
        "n_valid_prorated = int(scores_prorated[\"standard_score\"].notna().sum())\n",
        "\n",
        "print(f\"Participants with valid strict scores: {n_valid_scores}\")\n",
        "print(f\"Participants with NaN scores (missing item data): {n_nan_scores}\")\n",
        "print(f\"Participants with valid prorated scores (min_frac={min_frac}): {n_valid_prorated}\")\n",
        "\n",
        "# Identity check: mean errors equals sum of foil probs (strict only)\n",
        "valid = false_alarms.notna()\n",
        "foil_probs = X.loc[valid, foil_cols].mean(axis=0)\n",
        "mean_errors_from_scores = float(false_alarms[valid].mean())\n",
        "mean_errors_from_probs = float(foil_probs.sum())\n",
        "\n",
        "print(f\"\\nIdentity check (on {int(valid.sum())} complete-case participants):\")\n",
        "print(\"Mean false alarms per participant (from sums):\", mean_errors_from_scores)\n",
        "print(\"Mean false alarms per participant (sum of foil probs):\", mean_errors_from_probs)\n",
        "\n",
        "if abs(mean_errors_from_scores - mean_errors_from_probs) > 1e-8:\n",
        "    raise ValueError(\n",
        "        \"Mismatch: mean false alarms per participant != sum of foil endorsement probabilities. \"\n",
        "        \"This indicates a logic/column-selection problem.\"\n",
        "    )\n",
        "\n",
        "print(\"Score columns computed.\")\n",
        "scores.describe().T\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bdfda478",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 6: Scoring integrity checks (required)\n",
        "# =========================================================\n",
        "\n",
        "N = scores.shape[0]\n",
        "n_auth = len(author_cols)\n",
        "n_foil = len(foil_cols)\n",
        "\n",
        "# 1) Confirm corrected score = hits − false alarms on random participants\n",
        "random.seed(12345)\n",
        "check_idx = random.sample(range(N), k=min(10, N))\n",
        "check_df = scores.iloc[check_idx].copy()\n",
        "check_df[\"recomputed\"] = check_df[\"hits\"] - check_df[\"false_alarms\"]\n",
        "check_df[\"matches\"] = (\n",
        "    (check_df[\"standard_score\"].isna() & check_df[\"recomputed\"].isna())\n",
        "    | (check_df[\"standard_score\"].notna() & check_df[\"recomputed\"].notna() & (check_df[\"standard_score\"] == check_df[\"recomputed\"]))\n",
        ")\n",
        "\n",
        "print(\"Random participant scoring checks (10 or fewer):\")\n",
        "print(check_df)\n",
        "\n",
        "check_df.to_csv(os.path.join(RESULTS_DIR, \"scoring_random_checks.csv\"), index=True)\n",
        "\n",
        "# 2) Hard bounds assertion on valid (non-NaN) scores\n",
        "min_allowed = -n_foil\n",
        "max_allowed = n_auth\n",
        "valid_std = scores[\"standard_score\"].dropna()\n",
        "viol = (valid_std < min_allowed) | (valid_std > max_allowed)\n",
        "\n",
        "print(f\"\\nBounds check on {len(valid_std)} valid scores: [{min_allowed}, {max_allowed}]\")\n",
        "print(f\"Impossible standard_score cases: {int(viol.sum())}\")\n",
        "\n",
        "if viol.any():\n",
        "    bad_ids = valid_std.index[viol][:20].tolist()\n",
        "    impossible = scores.loc[viol[viol].index]\n",
        "    impossible.to_csv(os.path.join(RESULTS_DIR, \"impossible_standard_score_cases.csv\"), index=True)\n",
        "    raise ValueError(f\"Standard score bound violation for first cases: {bad_ids}\")\n",
        "\n",
        "# 3) Save assumptions\n",
        "n_nan = int(scores[\"standard_score\"].isna().sum())\n",
        "assumptions = {\n",
        "    \"missing_cells_in_item_matrix\": int(missing_count),\n",
        "    \"missing_handling\": \"strict_nan_if_any_item_missing\",\n",
        "    \"binary_value_enforcement\": True,\n",
        "    \"n_participants_total\": int(N),\n",
        "    \"n_participants_with_nan_scores\": n_nan,\n",
        "    \"n_participants_with_valid_scores\": int(N - n_nan),\n",
        "    \"items_excluded_from_analysis\": ITEMS_EXCLUDED_FROM_ANALYSIS,\n",
        "    \"n_authors_after_exclusion\": len(author_cols),\n",
        "    \"n_foils_after_exclusion\": len(foil_cols),\n",
        "}\n",
        "with open(os.path.join(RESULTS_DIR, \"scoring_assumptions.json\"), \"w\") as f:\n",
        "    json.dump(assumptions, f, indent=2)\n",
        "\n",
        "print(\"\\nSaved integrity outputs + assumptions.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40440de6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 7: Score distributions (Moore & Gordon-style)\n",
        "# =========================================================\n",
        "\n",
        "def summarize_series(s: pd.Series):\n",
        "    return pd.Series({\n",
        "        \"mean\": s.mean(),\n",
        "        \"sd\": s.std(ddof=1),\n",
        "        \"min\": s.min(),\n",
        "        \"max\": s.max(),\n",
        "        \"skewness\": s.skew(),\n",
        "    })\n",
        "\n",
        "score_summary = pd.DataFrame({\n",
        "    \"standard_score\": summarize_series(scores[\"standard_score\"]),\n",
        "    \"name_score\": summarize_series(scores[\"name_score\"]),\n",
        "    \"hits\": summarize_series(scores[\"hits\"]),\n",
        "    \"false_alarms\": summarize_series(scores[\"false_alarms\"]),\n",
        "}).T\n",
        "\n",
        "print(\"Score summary:\")\n",
        "print(score_summary)\n",
        "\n",
        "score_summary.to_csv(os.path.join(RESULTS_DIR, \"score_summary.csv\"), index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f9425d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 8: Plots (histograms) + save\n",
        "# =========================================================\n",
        "\n",
        "def save_hist(series, title, filename, bins=30):\n",
        "    clean = series.dropna()\n",
        "    n_dropped = len(series) - len(clean)\n",
        "    if n_dropped > 0:\n",
        "        print(f\"[{title}] Dropping {n_dropped} NaN values before plotting.\")\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(clean, bins=bins)\n",
        "    plt.title(title)\n",
        "    plt.xlabel(title)\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.tight_layout()\n",
        "    outpath = os.path.join(RESULTS_DIR, filename)\n",
        "    plt.savefig(outpath, dpi=200)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", outpath)\n",
        "\n",
        "save_hist(scores[\"standard_score\"], \"Standard ART score (Hits - False Alarms)\", \"hist_standard_score.png\")\n",
        "save_hist(scores[\"name_score\"], \"Name score (Hits)\", \"hist_name_score.png\")\n",
        "save_hist(scores[\"false_alarms\"], \"False alarms (Foils selected)\", \"hist_false_alarms.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7b61628",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 9: Real-author item selection rates (observed denominator)\n",
        "# =========================================================\n",
        "\n",
        "X_items = X[author_cols + foil_cols]\n",
        "\n",
        "def selection_rates(cols):\n",
        "    x = X_items[cols]\n",
        "    n_obs = x.notna().sum(axis=0)\n",
        "    pct = x.mean(axis=0, skipna=True) * 100\n",
        "    out = pd.DataFrame({\n",
        "        \"pct_selected\": pct,\n",
        "        \"n_observed\": n_obs,\n",
        "        \"missing_prop\": 1 - (n_obs / len(x)),\n",
        "    })\n",
        "    return out.sort_values(\"pct_selected\", ascending=False)\n",
        "\n",
        "author_rates = selection_rates(author_cols)\n",
        "author_rates.index.name = \"author_item\"\n",
        "\n",
        "mean_author_sel = author_rates[\"pct_selected\"].mean()\n",
        "max_author = author_rates.iloc[0]\n",
        "min_author = author_rates.iloc[-1]\n",
        "\n",
        "print(\"Author selection rates summary (observed-denominator):\")\n",
        "print(\"Mean author selection rate (%):\", mean_author_sel)\n",
        "print(\"Highest-selected author:\", author_rates.index[0], \"=\", float(max_author[\"pct_selected\"]))\n",
        "print(\"Lowest-selected author:\", author_rates.index[-1], \"=\", float(min_author[\"pct_selected\"]))\n",
        "\n",
        "top10_authors = author_rates.head(10)\n",
        "bottom10_authors = author_rates.tail(10)\n",
        "\n",
        "print(\"\\nTop 10 authors by selection rate:\")\n",
        "print(top10_authors)\n",
        "\n",
        "print(\"\\nBottom 10 authors by selection rate:\")\n",
        "print(bottom10_authors)\n",
        "\n",
        "# Save tables\n",
        "author_rates.to_csv(os.path.join(RESULTS_DIR, \"author_selection_rates_computed.csv\"), index=True)\n",
        "top10_authors.to_csv(os.path.join(RESULTS_DIR, \"top10_authors_selection.csv\"), index=True)\n",
        "bottom10_authors.to_csv(os.path.join(RESULTS_DIR, \"bottom10_authors_selection.csv\"), index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc13403b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 10: Plot top/bottom author rates + save\n",
        "# =========================================================\n",
        "\n",
        "def plot_bar(df_rates, title, filename):\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    plt.bar(df_rates.index.astype(str), df_rates[\"pct_selected\"].values)\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"% selected\")\n",
        "    plt.xticks(rotation=75, ha=\"right\")\n",
        "    plt.tight_layout()\n",
        "    outpath = os.path.join(RESULTS_DIR, filename)\n",
        "    plt.savefig(outpath, dpi=200)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", outpath)\n",
        "\n",
        "plot_bar(top10_authors, \"Top 10 author items by selection rate\", \"bar_top10_authors.png\")\n",
        "plot_bar(bottom10_authors, \"Bottom 10 author items by selection rate\", \"bar_bottom10_authors.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8f249c20",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 11: Foil selection rates and “alluring foils”\n",
        "# =========================================================\n",
        "\n",
        "foil_rates = selection_rates(foil_cols)\n",
        "foil_rates.index.name = \"foil_item\"\n",
        "\n",
        "mean_false_alarms_per_person = scores[\"false_alarms\"].mean()\n",
        "sd_false_alarms_per_person = scores[\"false_alarms\"].std(ddof=1)\n",
        "\n",
        "never_selected = foil_rates[foil_rates[\"pct_selected\"] == 0.0]\n",
        "most_selected_foil_name = foil_rates.index[0]\n",
        "most_selected_foil_rate = float(foil_rates.iloc[0][\"pct_selected\"])\n",
        "\n",
        "mean_foil_item_rate = foil_rates[\"pct_selected\"].mean()\n",
        "threshold_25x = 2.5 * mean_foil_item_rate\n",
        "high_rate_foils = foil_rates[foil_rates[\"pct_selected\"] >= threshold_25x]\n",
        "\n",
        "print(\"Foil summary (observed-denominator):\")\n",
        "print(\"Mean false alarms per participant:\", mean_false_alarms_per_person)\n",
        "print(\"SD false alarms per participant:\", sd_false_alarms_per_person)\n",
        "print(\"Foils never selected (count):\", never_selected.shape[0])\n",
        "print(\"Most selected foil:\", most_selected_foil_name, \"=\", most_selected_foil_rate)\n",
        "print(\"Mean foil item selection rate (%):\", mean_foil_item_rate)\n",
        "print(\"2.5× mean foil item rate threshold (%):\", threshold_25x)\n",
        "\n",
        "print(\"\\nFoils never selected (names):\")\n",
        "print(list(never_selected.index))\n",
        "\n",
        "print(\"\\nFoils selected ≥ 2.5× mean foil item rate:\")\n",
        "print(high_rate_foils)\n",
        "\n",
        "# Save\n",
        "foil_rates.to_csv(os.path.join(RESULTS_DIR, \"foil_selection_rates_computed.csv\"), index=True)\n",
        "never_selected.to_csv(os.path.join(RESULTS_DIR, \"foils_never_selected.csv\"), index=True)\n",
        "high_rate_foils.to_csv(os.path.join(RESULTS_DIR, \"foils_ge_2p5x_mean_rate.csv\"), index=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8781cab3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL A1: Load high-rate foils (≥ 2.5× mean foil item rate)\n",
        "# =========================================================\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from difflib import SequenceMatcher\n",
        "import re\n",
        "\n",
        "# If you already computed these in-notebook, you can skip reading from disk.\n",
        "# Otherwise point to the file you saved earlier:\n",
        "HIGH_RATE_FOILS_PATH = os.path.join(RESULTS_DIR, \"foils_ge_2p5x_mean_rate.csv\")\n",
        "\n",
        "high_rate_foils_df = pd.read_csv(HIGH_RATE_FOILS_PATH)\n",
        "# Expecting: index column might be saved; handle both cases robustly:\n",
        "if \"foil_item\" in high_rate_foils_df.columns:\n",
        "    high_rate_foil_names = high_rate_foils_df[\"foil_item\"].astype(str).tolist()\n",
        "elif \"Unnamed: 0\" in high_rate_foils_df.columns:\n",
        "    high_rate_foil_names = high_rate_foils_df[\"Unnamed: 0\"].astype(str).tolist()\n",
        "else:\n",
        "    # Fall back: if file contains only the index as first col\n",
        "    high_rate_foil_names = high_rate_foils_df.iloc[:, 0].astype(str).tolist()\n",
        "\n",
        "print(\"Loaded high-rate foils count:\", len(high_rate_foil_names))\n",
        "print(\"First 10 high-rate foils:\", high_rate_foil_names[:10])\n",
        "\n",
        "# Verify they exist in your foil_cols\n",
        "missing_from_foils = sorted(set(high_rate_foil_names) - set(foil_cols))\n",
        "if len(missing_from_foils) > 0:\n",
        "    print(\"WARNING: These high-rate foils are not in foil_cols (check naming):\")\n",
        "    print(missing_from_foils[:50])\n",
        "else:\n",
        "    print(\"All high-rate foils match foil_cols.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "695ce517",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL A2: Alluring-foil audit (string diagnostics only)\n",
        "# =========================================================\n",
        "\n",
        "def normalize_name(s: str) -> str:\n",
        "    s = str(s).strip().lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def split_tokens(s: str):\n",
        "    s = normalize_name(s)\n",
        "    toks = [t for t in re.split(r\"[\\s\\-_,.]+\", s) if t]\n",
        "    return toks\n",
        "\n",
        "def seq_sim(a: str, b: str) -> float:\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "author_names = [str(a) for a in author_cols]\n",
        "author_norm  = [normalize_name(a) for a in author_names]\n",
        "author_toks  = [split_tokens(a) for a in author_names]\n",
        "author_token_set = set(tok for toks in author_toks for tok in toks)\n",
        "\n",
        "def closest_author(foil_norm: str):\n",
        "    sims = [seq_sim(foil_norm, a) for a in author_norm]\n",
        "    j = int(np.argmax(sims))\n",
        "    return float(sims[j]), author_names[j]\n",
        "\n",
        "# For checking first+last token pair matches\n",
        "author_first_last = set()\n",
        "for a in author_names:\n",
        "    toks = split_tokens(a)\n",
        "    if len(toks) >= 2:\n",
        "        author_first_last.add((toks[0], toks[-1]))\n",
        "\n",
        "rows = []\n",
        "for foil_item in high_rate_foil_names:\n",
        "    f_norm = normalize_name(foil_item)\n",
        "    f_toks = split_tokens(foil_item)\n",
        "\n",
        "    max_sim, close_auth = closest_author(f_norm)\n",
        "\n",
        "    overlap_tokens = [t for t in f_toks if t in author_token_set]\n",
        "\n",
        "    first_tok = f_toks[0] if len(f_toks) else \"\"\n",
        "    last_tok  = f_toks[-1] if len(f_toks) else \"\"\n",
        "\n",
        "    first_match = (first_tok in author_token_set) if first_tok else False\n",
        "    last_match  = (last_tok in author_token_set) if last_tok else False\n",
        "\n",
        "    pair_match = False\n",
        "    if first_tok and last_tok and len(f_toks) >= 2:\n",
        "        pair_match = (first_tok, last_tok) in author_first_last\n",
        "\n",
        "    # Grab foil selection rate (%)\n",
        "    foil_pct = float(foil_rates.loc[foil_item, \"pct_selected\"]) if foil_item in foil_rates.index else np.nan\n",
        "\n",
        "    rows.append({\n",
        "        \"foil_item\": foil_item,\n",
        "        \"foil_pct_selected\": foil_pct,\n",
        "        \"max_string_similarity_to_any_author\": max_sim,\n",
        "        \"closest_author_by_string_similarity\": close_auth,\n",
        "        \"foil_tokens\": \" \".join(f_toks),\n",
        "        \"overlapping_tokens_with_author_list\": \" \".join(overlap_tokens),\n",
        "        \"any_token_overlap\": bool(len(overlap_tokens) > 0),\n",
        "        \"first_token_matches_any_author_token\": bool(first_match),\n",
        "        \"last_token_matches_any_author_token\": bool(last_match),\n",
        "        \"first_last_pair_matches_an_author\": bool(pair_match),\n",
        "        \"note\": \"Text-only diagnostics; any real-world identity confusion requires web and is UNVERIFIED.\"\n",
        "    })\n",
        "\n",
        "audit_df = pd.DataFrame(rows).sort_values(\"foil_pct_selected\", ascending=False)\n",
        "\n",
        "print(\"Alluring foil audit (high-rate foils only):\")\n",
        "print(audit_df)\n",
        "\n",
        "audit_path = os.path.join(RESULTS_DIR, \"alluring_foil_audit_high_rate_only.csv\")\n",
        "audit_df.to_csv(audit_path, index=False)\n",
        "print(\"Saved:\", audit_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57de7dd1",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL A3: Rescore excluding high-rate foils\n",
        "# =========================================================\n",
        "\n",
        "# Define the foil set to drop (only among foils)\n",
        "drop_foils = [f for f in high_rate_foil_names if f in foil_cols]\n",
        "keep_foils = [f for f in foil_cols if f not in drop_foils]\n",
        "\n",
        "print(\"Foils to DROP (count):\", len(drop_foils))\n",
        "print(\"Foils to KEEP (count):\", len(keep_foils))\n",
        "\n",
        "# Recompute false alarms using only KEEP foils\n",
        "false_alarms_keep = X[keep_foils].sum(axis=1)\n",
        "\n",
        "# Hits unchanged (still all authors)\n",
        "hits_same = X[author_cols].sum(axis=1)\n",
        "\n",
        "standard_score_keep = hits_same - false_alarms_keep\n",
        "name_score_same = hits_same\n",
        "\n",
        "scores_excl = pd.DataFrame({\n",
        "    \"hits\": hits_same,\n",
        "    \"false_alarms_excluding_high_rate_foils\": false_alarms_keep,\n",
        "    \"standard_score_excluding_high_rate_foils\": standard_score_keep,\n",
        "    \"name_score\": name_score_same\n",
        "})\n",
        "\n",
        "# Compare original vs revised at summary level\n",
        "def summarize(s):\n",
        "    return pd.Series({\"mean\": s.mean(), \"sd\": s.std(ddof=1), \"min\": s.min(), \"max\": s.max(), \"skew\": s.skew()})\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"original_false_alarms\": summarize(scores[\"false_alarms\"]),\n",
        "    \"revised_false_alarms\": summarize(scores_excl[\"false_alarms_excluding_high_rate_foils\"]),\n",
        "    \"original_standard_score\": summarize(scores[\"standard_score\"]),\n",
        "    \"revised_standard_score\": summarize(scores_excl[\"standard_score_excluding_high_rate_foils\"]),\n",
        "}).T\n",
        "\n",
        "print(\"\\nSummary comparison (original vs excluding high-rate foils):\")\n",
        "print(comparison)\n",
        "\n",
        "# Save outputs\n",
        "scores_excl.to_csv(os.path.join(RESULTS_DIR, \"scores_by_participant_excluding_high_rate_foils.csv\"), index=False)\n",
        "comparison.to_csv(os.path.join(RESULTS_DIR, \"score_summary_excluding_high_rate_foils.csv\"), index=True)\n",
        "\n",
        "print(\"\\nSaved rescored participant table and rescored summary.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1e7bd7ca",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL A4: Plots comparing original vs revised scoring\n",
        "# =========================================================\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def hist_compare(a, b, title_a, title_b, filename_prefix, bins=30):\n",
        "    plt.figure(figsize=(8,5))\n",
        "    plt.hist(a, bins=bins, alpha=0.6, label=title_a)\n",
        "    plt.hist(b, bins=bins, alpha=0.6, label=title_b)\n",
        "    plt.title(f\"{title_a} vs {title_b}\")\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    outpath = os.path.join(RESULTS_DIR, f\"{filename_prefix}.png\")\n",
        "    plt.savefig(outpath, dpi=200)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", outpath)\n",
        "\n",
        "hist_compare(scores[\"false_alarms\"], \n",
        "             scores_excl[\"false_alarms_excluding_high_rate_foils\"],\n",
        "             \"False alarms (all foils)\", \n",
        "             \"False alarms (excluding high-rate foils)\",\n",
        "             \"hist_false_alarms_original_vs_revised\")\n",
        "\n",
        "hist_compare(scores[\"standard_score\"],\n",
        "             scores_excl[\"standard_score_excluding_high_rate_foils\"],\n",
        "             \"Standard score (all foils)\",\n",
        "             \"Standard score (excluding high-rate foils)\",\n",
        "             \"hist_standard_score_original_vs_revised\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2fea8fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL B1: Missingness diagnostics (items + participants)\n",
        "# =========================================================\n",
        "\n",
        "# Re-load raw item matrix BEFORE fillna(0), if you overwrote it.\n",
        "X_raw = df[item_cols].copy()\n",
        "\n",
        "missing_by_item = X_raw.isna().mean().sort_values(ascending=False)  # proportion missing per item\n",
        "missing_by_person = X_raw.isna().mean(axis=1)                      # proportion missing per person\n",
        "\n",
        "missing_item_df = missing_by_item.rename(\"missing_prop\").to_frame()\n",
        "missing_item_df.index.name = \"item\"\n",
        "\n",
        "print(\"Participants with any missing in item block:\", int((missing_by_person > 0).sum()))\n",
        "print(\"Max missing proportion within a participant:\", float(missing_by_person.max()))\n",
        "\n",
        "# Save\n",
        "missing_item_df.to_csv(os.path.join(RESULTS_DIR, \"missingness_by_item_column.csv\"), index=True)\n",
        "pd.Series(missing_by_person, name=\"missing_prop\").to_csv(os.path.join(RESULTS_DIR, \"missingness_by_participant.csv\"), index=False)\n",
        "\n",
        "# Plot: histogram of participant missingness\n",
        "plt.figure(figsize=(8,5))\n",
        "plt.hist(missing_by_person, bins=30)\n",
        "plt.title(\"Participant-level missingness (proportion missing in item block)\")\n",
        "plt.xlabel(\"Missing proportion\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "outpath = os.path.join(RESULTS_DIR, \"hist_missingness_by_participant.png\")\n",
        "plt.savefig(outpath, dpi=200)\n",
        "plt.show()\n",
        "print(\"Saved:\", outpath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0216f2dc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL B2: Sensitivity scoring\n",
        "#   A) missing -> 0 (current)\n",
        "#   B) complete-case participants only\n",
        "# =========================================================\n",
        "\n",
        "def compute_scores(X_bin: pd.DataFrame, author_cols, foil_cols):\n",
        "    hits = X_bin[author_cols].sum(axis=1)\n",
        "    fa = X_bin[foil_cols].sum(axis=1)\n",
        "    return pd.DataFrame({\n",
        "        \"hits\": hits,\n",
        "        \"false_alarms\": fa,\n",
        "        \"standard_score\": hits - fa,\n",
        "        \"name_score\": hits\n",
        "    })\n",
        "\n",
        "# Policy A: Missing -> 0 (your current approach)\n",
        "X_A = X_raw.apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(int)\n",
        "scores_A = compute_scores(X_A, author_cols, foil_cols)\n",
        "\n",
        "# Policy B: Complete-case participants only (drop any row with any missing in item block)\n",
        "complete_mask = ~X_raw.isna().any(axis=1)\n",
        "X_B = X_raw.loc[complete_mask].apply(pd.to_numeric, errors=\"coerce\").astype(int)\n",
        "scores_B = compute_scores(X_B, author_cols, foil_cols)\n",
        "\n",
        "def summarize_scores(scores_df, label):\n",
        "    return pd.DataFrame({\n",
        "        \"label\": [label],\n",
        "        \"N\": [scores_df.shape[0]],\n",
        "        \"hits_mean\": [scores_df[\"hits\"].mean()],\n",
        "        \"hits_sd\": [scores_df[\"hits\"].std(ddof=1)],\n",
        "        \"fa_mean\": [scores_df[\"false_alarms\"].mean()],\n",
        "        \"fa_sd\": [scores_df[\"false_alarms\"].std(ddof=1)],\n",
        "        \"standard_mean\": [scores_df[\"standard_score\"].mean()],\n",
        "        \"standard_sd\": [scores_df[\"standard_score\"].std(ddof=1)],\n",
        "        \"name_mean\": [scores_df[\"name_score\"].mean()],\n",
        "        \"name_sd\": [scores_df[\"name_score\"].std(ddof=1)],\n",
        "    })\n",
        "\n",
        "sens = pd.concat([\n",
        "    summarize_scores(scores_A, \"Policy_A_missing_to_0\"),\n",
        "    summarize_scores(scores_B, \"Policy_B_complete_case_only\")\n",
        "], ignore_index=True)\n",
        "\n",
        "print(\"Missingness sensitivity summary:\")\n",
        "print(sens)\n",
        "\n",
        "# Save\n",
        "sens.to_csv(os.path.join(RESULTS_DIR, \"missingness_sensitivity_score_summary.csv\"), index=False)\n",
        "scores_B.to_csv(os.path.join(RESULTS_DIR, \"scores_by_participant_complete_case_only.csv\"), index=False)\n",
        "print(\"Saved sensitivity outputs.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33b8d6cc",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 12: Plot top foils by selection rate\n",
        "# =========================================================\n",
        "\n",
        "top_k = min(20, foil_rates.shape[0])\n",
        "top_foils = foil_rates.head(top_k)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.bar(top_foils.index.astype(str), top_foils[\"pct_selected\"].values)\n",
        "plt.title(f\"Top {top_k} foils by selection rate\")\n",
        "plt.ylabel(\"% selected\")\n",
        "plt.xticks(rotation=75, ha=\"right\")\n",
        "plt.tight_layout()\n",
        "outpath = os.path.join(RESULTS_DIR, f\"bar_top{top_k}_foils.png\")\n",
        "plt.savefig(outpath, dpi=200)\n",
        "plt.show()\n",
        "print(\"Saved:\", outpath)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "52df7e59",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 13: Enhanced alluring-foil audit (text-based only)\n",
        "# =========================================================\n",
        "# For each high-rate foil (>= 2.5× mean foil item rate):\n",
        "#   - max string similarity to any author item\n",
        "#   - token overlap flags (any, first, last)\n",
        "#   - first+last token pair match against authors\n",
        "\n",
        "# Use in-memory high_rate_foils from Cell 11 (no CSV reload needed)\n",
        "high_rate_foil_names = list(high_rate_foils.index)\n",
        "\n",
        "def normalize_name(s: str) -> str:\n",
        "    s = str(s).strip().lower()\n",
        "    s = re.sub(r\"\\s+\", \" \", s)\n",
        "    return s\n",
        "\n",
        "def split_tokens(s: str):\n",
        "    s = normalize_name(s)\n",
        "    return [t for t in re.split(r\"[\\s\\-_,.]+\", s) if t]\n",
        "\n",
        "def seq_sim(a: str, b: str) -> float:\n",
        "    return SequenceMatcher(None, a, b).ratio()\n",
        "\n",
        "author_names = [str(a) for a in author_cols]\n",
        "author_norm = [normalize_name(a) for a in author_names]\n",
        "author_toks = [split_tokens(a) for a in author_names]\n",
        "author_token_set = set(tok for toks in author_toks for tok in toks)\n",
        "\n",
        "def closest_author(foil_norm: str):\n",
        "    sims = [seq_sim(foil_norm, a) for a in author_norm]\n",
        "    j = int(np.argmax(sims))\n",
        "    return float(sims[j]), author_names[j]\n",
        "\n",
        "# Build set of (first_token, last_token) pairs from authors for pair-match check\n",
        "author_first_last = set()\n",
        "for a in author_names:\n",
        "    toks = split_tokens(a)\n",
        "    if len(toks) >= 2:\n",
        "        author_first_last.add((toks[0], toks[-1]))\n",
        "\n",
        "if len(high_rate_foil_names) == 0:\n",
        "    audit_df = pd.DataFrame(columns=[\n",
        "        \"foil_item\", \"foil_pct_selected\",\n",
        "        \"max_string_similarity_to_any_author\", \"closest_author_by_string_similarity\",\n",
        "        \"foil_tokens\", \"overlapping_tokens_with_author_list\",\n",
        "        \"any_token_overlap\",\n",
        "        \"first_token_matches_any_author_token\", \"last_token_matches_any_author_token\",\n",
        "        \"first_last_pair_matches_an_author\",\n",
        "        \"note\",\n",
        "    ])\n",
        "    print(\"No foils met the >= 2.5x mean rate threshold; audit table is empty.\")\n",
        "else:\n",
        "    audit_rows = []\n",
        "    for foil_item in high_rate_foil_names:\n",
        "        f_norm = normalize_name(foil_item)\n",
        "        f_toks = split_tokens(foil_item)\n",
        "\n",
        "        max_sim, close_auth = closest_author(f_norm)\n",
        "\n",
        "        overlap_tokens = [t for t in f_toks if t in author_token_set]\n",
        "\n",
        "        first_tok = f_toks[0] if f_toks else \"\"\n",
        "        last_tok = f_toks[-1] if f_toks else \"\"\n",
        "\n",
        "        first_match = (first_tok in author_token_set) if first_tok else False\n",
        "        last_match = (last_tok in author_token_set) if last_tok else False\n",
        "\n",
        "        pair_match = False\n",
        "        if first_tok and last_tok and len(f_toks) >= 2:\n",
        "            pair_match = (first_tok, last_tok) in author_first_last\n",
        "\n",
        "        foil_pct = float(foil_rates.loc[foil_item, \"pct_selected\"]) if foil_item in foil_rates.index else np.nan\n",
        "\n",
        "        audit_rows.append({\n",
        "            \"foil_item\": foil_item,\n",
        "            \"foil_pct_selected\": foil_pct,\n",
        "            \"max_string_similarity_to_any_author\": max_sim,\n",
        "            \"closest_author_by_string_similarity\": close_auth,\n",
        "            \"foil_tokens\": \" \".join(f_toks),\n",
        "            \"overlapping_tokens_with_author_list\": \" \".join(overlap_tokens),\n",
        "            \"any_token_overlap\": bool(len(overlap_tokens) > 0),\n",
        "            \"first_token_matches_any_author_token\": bool(first_match),\n",
        "            \"last_token_matches_any_author_token\": bool(last_match),\n",
        "            \"first_last_pair_matches_an_author\": bool(pair_match),\n",
        "            \"note\": \"Text-only diagnostics; any real-world identity confusion requires web and is UNVERIFIED.\",\n",
        "        })\n",
        "\n",
        "    audit_df = pd.DataFrame(audit_rows).sort_values(\"foil_pct_selected\", ascending=False)\n",
        "    print(\"Alluring-foil audit (high-rate foils only):\")\n",
        "    print(audit_df)\n",
        "\n",
        "audit_path = os.path.join(RESULTS_DIR, \"alluring_foil_audit_high_rate_only.csv\")\n",
        "audit_df.to_csv(audit_path, index=False)\n",
        "print(\"Saved:\", audit_path)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d8bc4b3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 14: Rescore excluding high-rate foils\n",
        "# =========================================================\n",
        "# Recompute false alarms and standard scores using only foils\n",
        "# that are NOT in the high-rate set (>= 2.5x mean foil item rate).\n",
        "\n",
        "drop_foils = [f for f in high_rate_foil_names if f in foil_cols]\n",
        "keep_foils = [f for f in foil_cols if f not in drop_foils]\n",
        "\n",
        "print(\"Foils to DROP (count):\", len(drop_foils))\n",
        "print(\"Foils to KEEP (count):\", len(keep_foils))\n",
        "\n",
        "false_alarms_keep = X[keep_foils].sum(axis=1, min_count=len(keep_foils))\n",
        "hits_same = X[author_cols].sum(axis=1, min_count=len(author_cols))\n",
        "standard_score_keep = hits_same - false_alarms_keep\n",
        "name_score_same = hits_same\n",
        "\n",
        "scores_excl = pd.DataFrame({\n",
        "    \"hits\": hits_same,\n",
        "    \"false_alarms_excluding_high_rate_foils\": false_alarms_keep,\n",
        "    \"standard_score_excluding_high_rate_foils\": standard_score_keep,\n",
        "    \"name_score\": name_score_same,\n",
        "})\n",
        "\n",
        "# Summary comparison: original vs revised\n",
        "def summarize(s):\n",
        "    return pd.Series({\n",
        "        \"mean\": s.mean(), \"sd\": s.std(ddof=1),\n",
        "        \"min\": s.min(), \"max\": s.max(), \"skew\": s.skew(),\n",
        "    })\n",
        "\n",
        "comparison = pd.DataFrame({\n",
        "    \"original_false_alarms\": summarize(scores[\"false_alarms\"]),\n",
        "    \"revised_false_alarms\": summarize(scores_excl[\"false_alarms_excluding_high_rate_foils\"]),\n",
        "    \"original_standard_score\": summarize(scores[\"standard_score\"]),\n",
        "    \"revised_standard_score\": summarize(scores_excl[\"standard_score_excluding_high_rate_foils\"]),\n",
        "}).T\n",
        "\n",
        "print(\"\\nSummary comparison (original vs excluding high-rate foils):\")\n",
        "print(comparison)\n",
        "\n",
        "scores_excl.to_csv(os.path.join(RESULTS_DIR, \"scores_by_participant_excluding_high_rate_foils.csv\"), index=False)\n",
        "comparison.to_csv(os.path.join(RESULTS_DIR, \"score_summary_excluding_high_rate_foils.csv\"), index=True)\n",
        "print(\"\\nSaved rescored participant table and summary.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9421f9b6",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 15: Comparison histograms — original vs revised scoring\n",
        "# =========================================================\n",
        "\n",
        "def hist_compare(a, b, title_a, title_b, filename_prefix, bins=30):\n",
        "    a_clean, b_clean = a.dropna(), b.dropna()\n",
        "    plt.figure(figsize=(8, 5))\n",
        "    plt.hist(a_clean, bins=bins, alpha=0.6, label=title_a)\n",
        "    plt.hist(b_clean, bins=bins, alpha=0.6, label=title_b)\n",
        "    plt.title(f\"{title_a} vs {title_b}\")\n",
        "    plt.xlabel(\"Value\")\n",
        "    plt.ylabel(\"Count\")\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    outpath = os.path.join(RESULTS_DIR, f\"{filename_prefix}.png\")\n",
        "    plt.savefig(outpath, dpi=200)\n",
        "    plt.show()\n",
        "    print(\"Saved:\", outpath)\n",
        "\n",
        "hist_compare(\n",
        "    scores[\"false_alarms\"],\n",
        "    scores_excl[\"false_alarms_excluding_high_rate_foils\"],\n",
        "    \"False alarms (all foils)\",\n",
        "    \"False alarms (excl. high-rate foils)\",\n",
        "    \"hist_false_alarms_original_vs_revised\",\n",
        ")\n",
        "\n",
        "hist_compare(\n",
        "    scores[\"standard_score\"],\n",
        "    scores_excl[\"standard_score_excluding_high_rate_foils\"],\n",
        "    \"Standard score (all foils)\",\n",
        "    \"Standard score (excl. high-rate foils)\",\n",
        "    \"hist_standard_score_original_vs_revised\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0064f61",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 16: Missingness diagnostics (items + participants)\n",
        "# =========================================================\n",
        "# Uses X_num_prefill (numeric-coerced, NaN-preserving) from Cell 4.\n",
        "\n",
        "missing_by_item = X_num_prefill.isna().mean().sort_values(ascending=False)\n",
        "missing_by_person = X_num_prefill.isna().mean(axis=1)\n",
        "\n",
        "missing_item_df = missing_by_item.rename(\"missing_prop\").to_frame()\n",
        "missing_item_df.index.name = \"item\"\n",
        "\n",
        "n_persons_any_missing = int((missing_by_person > 0).sum())\n",
        "max_person_missing = float(missing_by_person.max())\n",
        "print(f\"Participants with any missing in item block: {n_persons_any_missing}\")\n",
        "print(f\"Max missing proportion within a participant: {max_person_missing:.4f}\")\n",
        "\n",
        "missing_item_df.to_csv(os.path.join(RESULTS_DIR, \"missingness_by_item_detailed.csv\"), index=True)\n",
        "pd.Series(missing_by_person, name=\"missing_prop\").to_csv(\n",
        "    os.path.join(RESULTS_DIR, \"missingness_by_participant.csv\"), index=False\n",
        ")\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.hist(missing_by_person, bins=30)\n",
        "plt.title(\"Participant-level missingness (proportion missing in item block)\")\n",
        "plt.xlabel(\"Missing proportion\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "outpath = os.path.join(RESULTS_DIR, \"hist_missingness_by_participant.png\")\n",
        "plt.savefig(outpath, dpi=200)\n",
        "plt.show()\n",
        "print(\"Saved:\", outpath)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "75550e3b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 17: Sensitivity scoring\n",
        "#   Policy A: strict NaN (current default — score = NaN if any item missing)\n",
        "#   Policy B: lenient fill-0 (old approach — missing treated as \"not selected\")\n",
        "#   Policy C: prorated-with-threshold (keeps mostly-complete participants)\n",
        "# =========================================================\n",
        "\n",
        "def compute_scores_from_matrix(X_bin, author_cols, foil_cols):\n",
        "    hits = X_bin[author_cols].sum(axis=1)\n",
        "    fa = X_bin[foil_cols].sum(axis=1)\n",
        "    return pd.DataFrame({\n",
        "        \"hits\": hits,\n",
        "        \"false_alarms\": fa,\n",
        "        \"standard_score\": hits - fa,\n",
        "        \"name_score\": hits,\n",
        "    })\n",
        "\n",
        "# Policy A: strict NaN (reuse scores from Cell 5)\n",
        "scores_A = scores.copy()\n",
        "\n",
        "# Policy B: lenient fill missing -> 0 (old approach, for comparison)\n",
        "X_filled = X_num_prefill.fillna(0).astype(int)\n",
        "scores_B = compute_scores_from_matrix(X_filled, author_cols, foil_cols)\n",
        "\n",
        "# Policy C: prorated-with-threshold (from Cell 5)\n",
        "scores_C = scores_prorated.copy()\n",
        "\n",
        "# Also compute complete-case mask (useful for reporting)\n",
        "complete_mask = ~X_num_prefill.isna().any(axis=1)\n",
        "\n",
        "\n",
        "def summarize_scores(scores_df, label):\n",
        "    valid = scores_df.dropna(subset=[\"standard_score\"])\n",
        "    return pd.DataFrame({\n",
        "        \"label\": [label],\n",
        "        \"N_total\": [scores_df.shape[0]],\n",
        "        \"N_valid\": [valid.shape[0]],\n",
        "        \"hits_mean\": [valid[\"hits\"].mean()],\n",
        "        \"hits_sd\": [valid[\"hits\"].std(ddof=1)],\n",
        "        \"fa_mean\": [valid[\"false_alarms\"].mean()],\n",
        "        \"fa_sd\": [valid[\"false_alarms\"].std(ddof=1)],\n",
        "        \"standard_mean\": [valid[\"standard_score\"].mean()],\n",
        "        \"standard_sd\": [valid[\"standard_score\"].std(ddof=1)],\n",
        "        \"name_mean\": [valid[\"name_score\"].mean()],\n",
        "        \"name_sd\": [valid[\"name_score\"].std(ddof=1)],\n",
        "    })\n",
        "\n",
        "sens = pd.concat([\n",
        "    summarize_scores(scores_A, \"Policy_A_strict_nan\"),\n",
        "    summarize_scores(scores_B, \"Policy_B_lenient_fill_0\"),\n",
        "    summarize_scores(scores_C, \"Policy_C_prorated_minfrac_0.975\"),\n",
        "], ignore_index=True)\n",
        "\n",
        "print(\"Missingness sensitivity summary:\")\n",
        "print(sens)\n",
        "\n",
        "sens.to_csv(os.path.join(RESULTS_DIR, \"missingness_sensitivity_score_summary.csv\"), index=False)\n",
        "scores_B.to_csv(os.path.join(RESULTS_DIR, \"scores_by_participant_lenient_fill0.csv\"), index=False)\n",
        "scores_C.to_csv(os.path.join(RESULTS_DIR, \"scores_by_participant_prorated.csv\"), index=False)\n",
        "print(\"Saved sensitivity outputs.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7f227a3",
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================\n",
        "# CELL 18: Save artifacts for downstream factor-analysis notebook\n",
        "# =========================================================\n",
        "\n",
        "scores_out = scores.copy()\n",
        "scores_out.to_csv(os.path.join(RESULTS_DIR, \"scores_by_participant.csv\"), index=False)\n",
        "\n",
        "print(\"Saved participant-level scores:\", os.path.join(RESULTS_DIR, \"scores_by_participant.csv\"))\n",
        "print(\"Saved author selection rates:\", os.path.join(RESULTS_DIR, \"author_selection_rates_computed.csv\"))\n",
        "print(\"Saved foil selection rates:\", os.path.join(RESULTS_DIR, \"foil_selection_rates_computed.csv\"))\n",
        "\n",
        "# Compute sensitivity delta for summary (NaN-safe)\n",
        "_valid_std = scores[\"standard_score\"].dropna()\n",
        "_valid_excl_std = scores_excl[\"standard_score_excluding_high_rate_foils\"].dropna()\n",
        "_delta_std = float(_valid_excl_std.mean() - _valid_std.mean())\n",
        "\n",
        "_n_scored = int(_valid_std.shape[0])\n",
        "_n_excluded_missing = int(scores[\"standard_score\"].isna().sum())\n",
        "\n",
        "run_summary = {\n",
        "    \"N\": int(N),\n",
        "    \"n_participants_scored\": _n_scored,\n",
        "    \"n_participants_excluded_missing\": _n_excluded_missing,\n",
        "    \"n_authors\": int(n_auth),\n",
        "    \"n_foils\": int(n_foil),\n",
        "    \"mean_hits\": float(scores[\"hits\"].dropna().mean()),\n",
        "    \"mean_false_alarms\": float(scores[\"false_alarms\"].dropna().mean()),\n",
        "    \"mean_standard_score\": float(_valid_std.mean()),\n",
        "    \"mean_name_score\": float(scores[\"name_score\"].dropna().mean()),\n",
        "    \"mean_author_item_rate_pct\": float(mean_author_sel),\n",
        "    \"mean_foil_item_rate_pct\": float(mean_foil_item_rate),\n",
        "    \"most_selected_foil\": most_selected_foil_name,\n",
        "    \"most_selected_foil_rate_pct\": float(most_selected_foil_rate),\n",
        "    \"never_selected_foils_count\": int(never_selected.shape[0]),\n",
        "    \"high_rate_foils_count_ge_2p5x\": int(high_rate_foils.shape[0]),\n",
        "    \"threshold_foil_2p5x_pct\": float(threshold_25x),\n",
        "    \"missing_handling\": \"strict_nan_if_any_item_missing\",\n",
        "    # Fields from Cells 14-17\n",
        "    \"n_high_rate_foils_excluded\": len(drop_foils),\n",
        "    \"revised_mean_standard_score\": float(_valid_excl_std.mean()),\n",
        "    \"n_complete_case_participants\": int(complete_mask.sum()),\n",
        "    \"sensitivity_delta_standard_score\": _delta_std,\n",
        "}\n",
        "with open(os.path.join(RESULTS_DIR, \"run_summary.json\"), \"w\") as f:\n",
        "    json.dump(run_summary, f, indent=2)\n",
        "\n",
        "print(\"Wrote run_summary.json\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
