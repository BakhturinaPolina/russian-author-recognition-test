{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# IRT Psychometric Analysis of Russian Author Recognition Test (ART)\n",
        "\n",
        "This notebook performs a comprehensive Item Response Theory (IRT) analysis of the ART pretest data.\n",
        "\n",
        "**Goal**: Build a psychometrically sound latent-trait model of ART responses using 2PL IRT,\n",
        "understand item behavior, and prepare clean latent ability scores (θ) for validation.\n",
        "\n",
        "**Dataset**: ~1,835 participants, 214 ART items (real authors + foils)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 0: Setup and Configuration\n",
        "\n",
        "Import libraries, set paths, and create helper functions for saving outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# IMPORTS\n",
        "# ============================================================\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import warnings\n",
        "from datetime import datetime\n",
        "import importlib\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Statistics\n",
        "from scipy import stats\n",
        "from scipy.special import expit  # logistic function\n",
        "\n",
        "# Factor Analysis\n",
        "from factor_analyzer import FactorAnalyzer\n",
        "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
        "\n",
        "# Compatibility shim for scikit-learn check_array signature changes (factor_analyzer uses force_all_finite; older sklearn uses ensure_all_finite)\n",
        "import inspect\n",
        "from sklearn.utils import validation as _sk_validation\n",
        "\n",
        "# If the stored \"original\" is actually our wrapper (e.g. cell run twice), reload to get the real one\n",
        "if hasattr(_sk_validation, \"_cursor_orig_check_array\") and getattr(_sk_validation._cursor_orig_check_array, \"__name__\", \"\") == \"_check_array_compat\":\n",
        "    _sk_validation = importlib.reload(_sk_validation)\n",
        "\n",
        "import factor_analyzer.factor_analyzer as _fa_internal\n",
        "\n",
        "# Store the real check_array only once; never overwrite with a wrapper\n",
        "if not hasattr(_sk_validation, \"_cursor_orig_check_array\"):\n",
        "    _sk_validation._cursor_orig_check_array = _sk_validation.check_array\n",
        "\n",
        "# Use the stored original by attribute lookup inside the wrapper to avoid recursion (no closure over a possibly-stale reference)\n",
        "_real_check_array = _sk_validation._cursor_orig_check_array\n",
        "if \"force_all_finite\" not in inspect.signature(_real_check_array).parameters:\n",
        "    def _check_array_compat(*args, **kwargs):\n",
        "        if \"force_all_finite\" in kwargs:\n",
        "            kwargs[\"ensure_all_finite\"] = kwargs.pop(\"force_all_finite\")\n",
        "        return _sk_validation._cursor_orig_check_array(*args, **kwargs)\n",
        "\n",
        "    _sk_validation.check_array = _check_array_compat\n",
        "    _fa_internal.check_array = _check_array_compat\n",
        "\n",
        "# IRT (girth library)\n",
        "import girth\n",
        "from girth import twopl_mml\n",
        "from girth.synthetic import create_synthetic_irt_dichotomous\n",
        "\n",
        "# Progress bars\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Pretty tables\n",
        "from tabulate import tabulate\n",
        "\n",
        "# Suppress warnings for cleaner output\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "print(\"All libraries imported successfully!\")\n",
        "print(f\"Analysis started: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION\n",
        "# ============================================================\n",
        "\n",
        "# Random seed for reproducibility\n",
        "RANDOM_SEED = 42\n",
        "np.random.seed(RANDOM_SEED)\n",
        "\n",
        "# Project paths\n",
        "PROJECT_ROOT = Path(\"../..\")\n",
        "DATA_DIR = PROJECT_ROOT / \"data\" / \"raw\"\n",
        "RESULTS_DIR = PROJECT_ROOT / \"results\"\n",
        "\n",
        "# Input files\n",
        "ART_FILE = DATA_DIR / \"ART_pretest_merged_EN.xlsx\"\n",
        "REAL_AUTHORS_FILE = DATA_DIR / \"author_lists\" / \"real_authors.xls\"\n",
        "FOILS_FILE = DATA_DIR / \"author_lists\" / \"not_real_authors.xls\"\n",
        "\n",
        "# Output directories (already created)\n",
        "OUTPUT_DIRS = {\n",
        "    'step0': RESULTS_DIR / 'step0_data_prep',\n",
        "    'step1': RESULTS_DIR / 'step1_item_descriptives',\n",
        "    'step2': RESULTS_DIR / 'step2_dimensionality',\n",
        "    'step3': RESULTS_DIR / 'step3_irt_model',\n",
        "    'step4': RESULTS_DIR / 'step4_item_analysis',\n",
        "    'step5': RESULTS_DIR / 'step5_diagnostics',\n",
        "    'step6': RESULTS_DIR / 'step6_theta_scores',\n",
        "    'step7': RESULTS_DIR / 'step7_scoring_comparison',\n",
        "    'step8': RESULTS_DIR / 'step8_short_scale',\n",
        "    'step9': RESULTS_DIR / 'step9_report'\n",
        "}\n",
        "\n",
        "# Plot settings\n",
        "PLOT_DPI = 300\n",
        "FIGSIZE_SMALL = (8, 6)\n",
        "FIGSIZE_MEDIUM = (12, 8)\n",
        "FIGSIZE_LARGE = (16, 12)\n",
        "\n",
        "# IRT thresholds\n",
        "ENDORSEMENT_LOW_THRESHOLD = 0.05   # 5% - floor effect\n",
        "ENDORSEMENT_HIGH_THRESHOLD = 0.95  # 95% - ceiling effect\n",
        "DISCRIMINATION_LOW_THRESHOLD = 0.5  # weak discrimination\n",
        "DIFFICULTY_EXTREME_THRESHOLD = 3.0  # extreme difficulty\n",
        "\n",
        "print(\"Configuration loaded:\")\n",
        "print(f\"  - Random seed: {RANDOM_SEED}\")\n",
        "print(f\"  - Project root: {PROJECT_ROOT.resolve()}\")\n",
        "print(f\"  - Results directory: {RESULTS_DIR.resolve()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Convert Excel files to CSV\n",
        "\n",
        "Convert xlsx/xls data files to CSV first so downstream steps use fast, portable CSV inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONVERT XLSX/XLS TO CSV\n",
        "# ============================================================\n",
        "# Read Excel files and save as CSV so the rest of the notebook uses CSV.\n",
        "\n",
        "# Ensure config is defined before this cell is run!\n",
        "try:\n",
        "    DATA_DIR\n",
        "    ART_FILE\n",
        "    REAL_AUTHORS_FILE\n",
        "    FOILS_FILE\n",
        "except NameError as e:\n",
        "    raise RuntimeError(\n",
        "        \"DATA_DIR and other configuration variables are not defined.\\n\"\n",
        "        \"Please run the configuration cell above before running this one.\"\n",
        "    ) from e\n",
        "\n",
        "# ART pretest (xlsx)\n",
        "ART_CSV = DATA_DIR / \"ART_pretest_merged_EN.csv\"\n",
        "df_art = pd.read_excel(ART_FILE, header=None)\n",
        "df_art.to_csv(ART_CSV, index=False, header=False)\n",
        "print(f\"Saved: {ART_CSV.name} ({df_art.shape[0]} rows x {df_art.shape[1]} cols)\")\n",
        "\n",
        "# Author lists (xls)\n",
        "REAL_AUTHORS_CSV = DATA_DIR / \"author_lists\" / \"real_authors.csv\"\n",
        "FOILS_CSV = DATA_DIR / \"author_lists\" / \"not_real_authors.csv\"\n",
        "\n",
        "df_real = pd.read_excel(REAL_AUTHORS_FILE, header=None)\n",
        "df_real.to_csv(REAL_AUTHORS_CSV, index=False, header=False)\n",
        "print(f\"Saved: {REAL_AUTHORS_CSV.name} ({df_real.shape[0]} rows)\")\n",
        "\n",
        "df_foils = pd.read_excel(FOILS_FILE, header=None)\n",
        "df_foils.to_csv(FOILS_CSV, index=False, header=False)\n",
        "print(f\"Saved: {FOILS_CSV.name} ({df_foils.shape[0]} rows)\")\n",
        "\n",
        "print(\"All Excel files converted to CSV.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 1: Data Loading and Preprocessing (Step 0)\n",
        "\n",
        "Load the merged ART dataset, identify real authors vs foils, and create response matrices."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD RAW DATA\n",
        "# ============================================================\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"STEP 0: DATA LOADING AND PREPROCESSING\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Load the merged ART dataset\n",
        "# Structure: Row 0 = English labels, Row 1 = item codes, Rows 2+ = data\n",
        "print(\"\\n1. Loading ART pretest dataset...\")\n",
        "raw_df = pd.read_csv(ART_CSV, header=None)\n",
        "print(f\"   Raw data shape: {raw_df.shape[0]} rows x {raw_df.shape[1]} columns\")\n",
        "\n",
        "# Extract headers\n",
        "labels_row = raw_df.iloc[0].tolist()  # English labels\n",
        "codes_row = raw_df.iloc[1].tolist()   # Item codes\n",
        "data_df = raw_df.iloc[2:].reset_index(drop=True)  # Actual data\n",
        "\n",
        "print(f\"   Labels extracted: {len(labels_row)} columns\")\n",
        "print(f\"   Data rows: {len(data_df)} participants\")\n",
        "\n",
        "# Show first few column names\n",
        "print(f\"\\n   First 5 column labels: {labels_row[:5]}\")\n",
        "print(f\"   Last 5 column labels: {labels_row[-5:]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# LOAD AUTHOR REFERENCE LISTS\n",
        "# ============================================================\n",
        "print(\"\\n2. Loading author reference lists...\")\n",
        "\n",
        "# Load real authors list\n",
        "real_authors_df = pd.read_excel(REAL_AUTHORS_FILE, header=None)\n",
        "print(f\"   Real authors file shape: {real_authors_df.shape}\")\n",
        "\n",
        "# Load foils list  \n",
        "foils_df = pd.read_csv(FOILS_CSV, header=None)\n",
        "print(f\"   Foils file shape: {foils_df.shape}\")\n",
        "\n",
        "# Display sample of each list\n",
        "print(f\"\\n   Sample real authors:\")\n",
        "print(real_authors_df.head(5).to_string(index=False, header=False))\n",
        "print(f\"\\n   Sample foils:\")\n",
        "print(foils_df.head(5).to_string(index=False, header=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# IDENTIFY REAL AUTHORS VS FOILS IN DATASET\n",
        "# ============================================================\n",
        "print(\"\\n3. Identifying real authors vs foils in dataset columns...\")\n",
        "\n",
        "# Extract item codes from reference lists\n",
        "# Try to match codes from the codes_row with the reference lists\n",
        "real_codes_set = set()\n",
        "foil_codes_set = set()\n",
        "\n",
        "# The reference lists may have author names and/or codes\n",
        "# Let's examine the structure\n",
        "if real_authors_df.shape[1] >= 2:\n",
        "    # Assume column 0 = name, column 1 = code\n",
        "    real_codes_set = set(real_authors_df.iloc[:, 1].dropna().astype(str).str.strip())\n",
        "    real_names_list = real_authors_df.iloc[:, 0].dropna().tolist()\n",
        "else:\n",
        "    real_names_list = real_authors_df.iloc[:, 0].dropna().tolist()\n",
        "\n",
        "if foils_df.shape[1] >= 2:\n",
        "    foil_codes_set = set(foils_df.iloc[:, 1].dropna().astype(str).str.strip())\n",
        "    foil_names_list = foils_df.iloc[:, 0].dropna().tolist()\n",
        "else:\n",
        "    foil_names_list = foils_df.iloc[:, 0].dropna().tolist()\n",
        "\n",
        "print(f\"   Real authors in reference: {len(real_names_list)}\")\n",
        "print(f\"   Foils in reference: {len(foil_names_list)}\")\n",
        "\n",
        "# Match dataset columns with reference lists by name or code\n",
        "# Skip the first 5 demographic columns and the last 'source' column\n",
        "n_demographics = 5\n",
        "n_items = len(labels_row) - n_demographics - 1  # -1 for source column\n",
        "\n",
        "print(f\"   Expected item columns: {n_items}\")\n",
        "\n",
        "# Create mappings for column indices\n",
        "item_labels = labels_row[n_demographics:-1]  # ART item labels (authors/foils)\n",
        "item_codes = codes_row[n_demographics:-1]    # ART item codes\n",
        "\n",
        "# Assign column names to data_df so label-based selection works\n",
        "data_df.columns = labels_row\n",
        "\n",
        "# Participant IDs (1-based index). Replace with a true ID column if available.\n",
        "participant_ids = pd.Index(range(1, len(data_df) + 1), name=\"participant_id\")\n",
        "\n",
        "# Item response matrix (participants x items)\n",
        "item_data = data_df[item_labels].copy()\n",
        "\n",
        "# Source column (last column in raw data: data collection source per participant)\n",
        "source_col = data_df.iloc[:, -1].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n   Sample item labels: {item_labels[:5]}\")\n",
        "print(f\"   Sample item codes: {item_codes[:5]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CLASSIFY ITEMS AS REAL AUTHORS OR FOILS\n",
        "# ============================================================\n",
        "print(\"\\n4. Classifying items as real authors or foils...\")\n",
        "\n",
        "# Strategy: Match by item CODES instead of names (foils are in Russian in reference, English in dataset)\n",
        "# - Foils have codes like \"fill1\", \"fill2\", \"fill 10\", etc.\n",
        "# - Real authors have codes like \"mod1\", \"cla1\", etc., OR just their names\n",
        "\n",
        "# Extract foil codes from reference list (column 0 contains name + code)\n",
        "import re\n",
        "\n",
        "def extract_fill_code(text):\n",
        "    \"\"\"Extract fill code from text like 'Геррит ХугенбуM fill1' or 'fil l21' -> 'fill1'/'fill21'\"\"\"\n",
        "    if pd.isna(text):\n",
        "        return None\n",
        "    s = str(text)\n",
        "    # Reference has typos \"fil l21\" (space in \"fil l\"); normalize first\n",
        "    s = re.sub(r'fil\\s+l(\\d+)', r'fill\\1', s, flags=re.IGNORECASE)\n",
        "    match = re.search(r'fill\\s*\\d+', s, re.IGNORECASE)\n",
        "    return match.group(0).replace(' ', '') if match else None\n",
        "\n",
        "def is_foil_code(code):\n",
        "    \"\"\"True if dataset item code is a foil (fill + digits).\"\"\"\n",
        "    if pd.isna(code):\n",
        "        return False\n",
        "    return bool(re.match(r'^fill\\s*\\d+$', str(code).strip(), re.IGNORECASE))\n",
        "\n",
        "def is_real_author_code(code):\n",
        "    \"\"\"True if dataset item code looks like real author (mod/cla/soc + digits).\"\"\"\n",
        "    if pd.isna(code):\n",
        "        return False\n",
        "    return bool(re.match(r'^(mod|cla|soc)\\d+$', str(code).strip(), re.IGNORECASE))\n",
        "\n",
        "# Get all foil codes from reference\n",
        "foil_codes_from_ref = set()\n",
        "for name in foil_names_list:\n",
        "    code = extract_fill_code(name)\n",
        "    if code:\n",
        "        foil_codes_from_ref.add(code.lower())\n",
        "\n",
        "print(f\"   Foil codes extracted from reference: {len(foil_codes_from_ref)}\")\n",
        "print(f\"   Sample foil codes: {sorted(list(foil_codes_from_ref))[:5]}\")\n",
        "\n",
        "# Normalize names for real author matching\n",
        "def normalize_name(name):\n",
        "    \"\"\"Normalize author name for comparison (remove codes).\"\"\"\n",
        "    if pd.isna(name):\n",
        "        return \"\"\n",
        "    # Remove fill codes if present\n",
        "    text = str(name).strip()\n",
        "    text = re.sub(r'\\s*fill\\s*\\d+', '', text, flags=re.IGNORECASE)\n",
        "    return text.strip().lower()\n",
        "\n",
        "real_names_normalized = set(normalize_name(n) for n in real_names_list)\n",
        "\n",
        "# Classify each item by CODE\n",
        "real_author_indices = []\n",
        "foil_indices = []\n",
        "unclassified_indices = []\n",
        "\n",
        "real_author_names = []\n",
        "foil_names = []\n",
        "\n",
        "for i, (label, code) in enumerate(zip(item_labels, item_codes)):\n",
        "    code_normalized = str(code).strip().lower()\n",
        "    label_normalized = normalize_name(label)\n",
        "    \n",
        "    # Foil: dataset code is fill+digits, OR in reference foil codes, OR label contains fill code\n",
        "    if is_foil_code(code) or code_normalized in foil_codes_from_ref or extract_fill_code(label):\n",
        "        foil_indices.append(i)\n",
        "        foil_names.append(label)\n",
        "    # Real author: name matches reference\n",
        "    elif label_normalized in real_names_normalized:\n",
        "        real_author_indices.append(i)\n",
        "        real_author_names.append(label)\n",
        "    # Fallback: cla/mod codes are real authors (e.g. Henryk Sienkiewicz/cla4, Charlotte Bronte/cla12)\n",
        "    elif is_real_author_code(code):\n",
        "        real_author_indices.append(i)\n",
        "        real_author_names.append(label)\n",
        "    else:\n",
        "        unclassified_indices.append(i)\n",
        "\n",
        "print(f\"\\n   Real author items found: {len(real_author_indices)}\")\n",
        "print(f\"   Foil items found: {len(foil_indices)}\")\n",
        "print(f\"   Unclassified items: {len(unclassified_indices)}\")\n",
        "\n",
        "if unclassified_indices:\n",
        "    print(f\"\\n   WARNING: {len(unclassified_indices)} items could not be classified!\")\n",
        "    print(f\"   Sample unclassified: {[item_labels[i] for i in unclassified_indices[:5]]}\")\n",
        "    print(f\"   Their codes: {[item_codes[i] for i in unclassified_indices[:5]]}\")\n",
        "\n",
        "# Store classification info\n",
        "item_classification = pd.DataFrame({\n",
        "    'item_index': range(len(item_labels)),\n",
        "    'item_label': item_labels,\n",
        "    'item_code': item_codes,\n",
        "    'is_real_author': [i in real_author_indices for i in range(len(item_labels))],\n",
        "    'is_foil': [i in foil_indices for i in range(len(item_labels))]\n",
        "})\n",
        "\n",
        "print(f\"\\n   Classification summary:\")\n",
        "print(item_classification[['is_real_author', 'is_foil']].sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# HELPER FUNCTIONS\n",
        "# ============================================================\n",
        "\n",
        "def save_table(df, step, filename, index=True):\n",
        "    \"\"\"Save DataFrame to CSV in the appropriate step's tables folder.\"\"\"\n",
        "    output_path = OUTPUT_DIRS[step] / 'tables' / filename\n",
        "    df.to_csv(output_path, index=index)\n",
        "    print(f\"  Saved table: {output_path.name}\")\n",
        "    return output_path\n",
        "\n",
        "def save_plot(fig, step, filename, silent=False):\n",
        "    \"\"\"Save figure to PNG in the appropriate step's plots folder.\"\"\"\n",
        "    output_path = OUTPUT_DIRS[step] / 'plots' / filename\n",
        "    fig.savefig(output_path, dpi=PLOT_DPI, bbox_inches='tight', facecolor='white')\n",
        "    plt.close(fig)\n",
        "    if not silent:\n",
        "        print(f\"  Saved plot: {output_path.name}\")\n",
        "    return output_path\n",
        "\n",
        "def print_section_header(title):\n",
        "    \"\"\"Print a formatted section header.\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(f\" {title}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "def print_stats_box(title, stats_dict):\n",
        "    \"\"\"Print statistics in a formatted box.\"\"\"\n",
        "    print(f\"\\n┌{'─'*50}┐\")\n",
        "    print(f\"│ {title:<48} │\")\n",
        "    print(f\"├{'─'*50}┤\")\n",
        "    for key, value in stats_dict.items():\n",
        "        if isinstance(value, float):\n",
        "            print(f\"│ {key:<30} {value:>17.4f} │\")\n",
        "        else:\n",
        "            print(f\"│ {key:<30} {str(value):>17} │\")\n",
        "    print(f\"└{'─'*50}┘\")\n",
        "\n",
        "def describe_distribution(data, name=\"Variable\"):\n",
        "    \"\"\"Print descriptive statistics for a distribution.\"\"\"\n",
        "    stats_dict = {\n",
        "        'N': len(data),\n",
        "        'Mean': np.mean(data),\n",
        "        'Std Dev': np.std(data),\n",
        "        'Min': np.min(data),\n",
        "        'Q1 (25%)': np.percentile(data, 25),\n",
        "        'Median': np.median(data),\n",
        "        'Q3 (75%)': np.percentile(data, 75),\n",
        "        'Max': np.max(data)\n",
        "    }\n",
        "    print_stats_box(f\"{name} Distribution\", stats_dict)\n",
        "\n",
        "# Set global plot style (handle different matplotlib versions)\n",
        "try:\n",
        "    plt.style.use('seaborn-v0_8-whitegrid')\n",
        "except OSError:\n",
        "    try:\n",
        "        plt.style.use('seaborn-whitegrid')\n",
        "    except OSError:\n",
        "        plt.style.use('ggplot')  # Fallback\n",
        "\n",
        "sns.set_palette('colorblind')\n",
        "plt.rcParams['figure.figsize'] = FIGSIZE_MEDIUM\n",
        "plt.rcParams['font.size'] = 11\n",
        "plt.rcParams['axes.titlesize'] = 14\n",
        "plt.rcParams['axes.labelsize'] = 12\n",
        "\n",
        "print(\"Helper functions defined and plot style configured.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DATA CLEANING: HANDLE MISSING DATA AND CONVERT TO BINARY\n",
        "# ============================================================\n",
        "\n",
        "print(\"\\n6. Cleaning data and converting to binary format...\")\n",
        "\n",
        "# --- BEGIN: Define or recover item_data from loaded DataFrame ---\n",
        "# Try to recover item_data from obvious previous dataframes, else raise informative error\n",
        "\n",
        "def _resolve_item_data():\n",
        "    \"\"\"Safely attempts to find item_data DataFrame in global namespace.\"\"\"\n",
        "    candidates = ['item_data', 'responses', 'resp_df', 'df_items']\n",
        "    g = globals()\n",
        "    for name in candidates:\n",
        "        if name in g and g[name] is not None:\n",
        "            # Ensure it's a DataFrame-like object by checking for .isnull()\n",
        "            obj = g[name]\n",
        "            if hasattr(obj, \"isnull\") and callable(getattr(obj, \"isnull\")):\n",
        "                return obj\n",
        "    return None\n",
        "\n",
        "item_data = _resolve_item_data()\n",
        "if item_data is None:\n",
        "    raise NameError(\n",
        "        \"item_data is not defined or is None. Please prepare the DataFrame of item responses (e.g., item_data), \"\n",
        "        \"with items as columns and participants as rows, before running this cleaning step.\\n\"\n",
        "        \"If your main DataFrame uses a different variable name, assign it to `item_data`.\"\n",
        "    )\n",
        "\n",
        "# Proceed with missing data analysis\n",
        "try:\n",
        "    missing_count = item_data.isnull().sum()\n",
        "    missing_by_row = item_data.isnull().sum(axis=1)\n",
        "\n",
        "    print(f\"\\n   Missing data analysis:\")\n",
        "    print(f\"   - Columns with any missing: {(missing_count > 0).sum()}\")\n",
        "    print(f\"   - Total missing cells: {item_data.isnull().sum().sum()}\")\n",
        "    print(f\"   - Participants with any missing: {(missing_by_row > 0).sum()}\")\n",
        "\n",
        "    # Identify columns with high missing rates (e.g., known last 9 items problem)\n",
        "    missing_pct = missing_count / len(item_data) * 100\n",
        "    high_missing_cols = missing_pct[missing_pct > 40].sort_values(ascending=False)\n",
        "\n",
        "    if len(high_missing_cols) > 0:\n",
        "        print(f\"\\n   Columns with >40% missing data ({len(high_missing_cols)} items):\")\n",
        "        for col, pct in high_missing_cols.head(10).items():\n",
        "            print(f\"      {col}: {pct:.1f}% missing\")\n",
        "except Exception as e:\n",
        "    print(f\"  [Error during missing data analysis: {e}]\")\n",
        "    raise\n",
        "\n",
        "# Convert responses to binary (0/1)\n",
        "def convert_to_binary(val):\n",
        "    \"\"\"Convert response to binary: 1 if checked, 0 otherwise.\"\"\"\n",
        "    if pd.isna(val):\n",
        "        return 0\n",
        "    try:\n",
        "        return 1 if float(val) == 1 else 0\n",
        "    except (ValueError, TypeError):\n",
        "        return 0\n",
        "\n",
        "# Apply binary conversion (prefer .map for pandas >=2.1, fallback to .applymap)\n",
        "try:\n",
        "    item_data_binary = item_data.map(convert_to_binary)\n",
        "except Exception as ex:\n",
        "    item_data_binary = item_data.applymap(convert_to_binary)\n",
        "\n",
        "print(f\"\\n   Binary conversion complete.\")\n",
        "print(f\"   Response matrix shape: {item_data_binary.shape}\")\n",
        "print(f\"   Value counts in first column: {item_data_binary.iloc[:, 0].value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EXCLUDE HIGH-MISSING ITEMS FROM FURTHER ANALYSIS\n",
        "# ============================================================\n",
        "MISSING_EXCLUDE_PCT = 40  # Exclude items with >40% missing\n",
        "\n",
        "missing_pct = item_data.isnull().sum() / len(item_data) * 100\n",
        "high_missing = missing_pct[missing_pct > MISSING_EXCLUDE_PCT].sort_values(ascending=False)\n",
        "excluded_set = set(high_missing.index)\n",
        "\n",
        "# Drop every column whose name is in the excluded set (handles duplicate column names)\n",
        "cols_to_drop = [c for c in item_data_binary.columns if c in excluded_set]\n",
        "n_dropped = len(cols_to_drop)\n",
        "\n",
        "if n_dropped > 0:\n",
        "    item_data_binary = item_data_binary.drop(columns=cols_to_drop)\n",
        "    item_data = item_data.drop(columns=cols_to_drop)\n",
        "    # Keep item_labels and item_codes in sync (same order as remaining columns)\n",
        "    indices_keep = [i for i, lab in enumerate(item_labels) if lab not in excluded_set]\n",
        "    item_labels = [item_labels[i] for i in indices_keep]\n",
        "    item_codes = [item_codes[i] for i in indices_keep]\n",
        "    # Rebuild author/foil index lists by name (remaining items only)\n",
        "    real_author_indices = [i for i, lab in enumerate(item_labels) if lab in set(real_author_names)]\n",
        "    foil_indices = [i for i, lab in enumerate(item_labels) if lab in set(foil_names)]\n",
        "    # Update classification table to remaining items only\n",
        "    item_classification = item_classification[~item_classification[\"item_label\"].isin(excluded_set)].reset_index(drop=True)\n",
        "    item_classification[\"item_index\"] = range(len(item_classification))\n",
        "\n",
        "    print(f\"\\n   Excluded {n_dropped} items with >{MISSING_EXCLUDE_PCT}% missing from further analysis:\")\n",
        "    for col, pct in high_missing.items():\n",
        "        print(f\"      {col}: {pct:.1f}% missing\")\n",
        "    print(f\"\\n   Response matrix after exclusion: {item_data_binary.shape[0]} participants x {item_data_binary.shape[1]} items\")\n",
        "    print(f\"   Real author items: {len(real_author_indices)}, Foil items: {len(foil_indices)}\")\n",
        "else:\n",
        "    print(f\"\\n   No items above {MISSING_EXCLUDE_PCT}% missing; none excluded.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SEPARATE REAL AUTHORS AND FOILS INTO MATRICES\n",
        "# ============================================================\n",
        "print(\"\\n7. Creating separate response matrices...\")\n",
        "\n",
        "# Extract real author columns\n",
        "real_author_cols = [item_labels[i] for i in real_author_indices]\n",
        "author_response_matrix = item_data_binary[real_author_cols].copy()\n",
        "author_response_matrix.index = participant_ids\n",
        "\n",
        "# Extract foil columns\n",
        "foil_cols = [item_labels[i] for i in foil_indices]\n",
        "foil_response_matrix = item_data_binary[foil_cols].copy()\n",
        "foil_response_matrix.index = participant_ids\n",
        "\n",
        "print(f\"   Author response matrix: {author_response_matrix.shape[0]} participants x {author_response_matrix.shape[1]} authors\")\n",
        "print(f\"   Foil response matrix: {foil_response_matrix.shape[0]} participants x {foil_response_matrix.shape[1]} foils\")\n",
        "\n",
        "# Calculate foil error rate for each participant\n",
        "foil_count = foil_response_matrix.sum(axis=1)\n",
        "total_foils = foil_response_matrix.shape[1]\n",
        "error_rate = foil_count / total_foils\n",
        "\n",
        "print(f\"\\n   Foil error rate statistics:\")\n",
        "describe_distribution(error_rate.values, \"Error Rate\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CREATE PARTICIPANT SUMMARY\n",
        "# ============================================================\n",
        "print(\"\\n8. Creating participant summary...\")\n",
        "\n",
        "# Calculate classical scores for each participant\n",
        "hits = author_response_matrix.sum(axis=1)  # Number of real authors endorsed\n",
        "false_alarms = foil_response_matrix.sum(axis=1)  # Number of foils endorsed\n",
        "classical_score = hits - false_alarms  # Traditional ART score: hits - false alarms\n",
        "\n",
        "# Create participant summary DataFrame\n",
        "participant_summary = pd.DataFrame({\n",
        "    'participant_id': participant_ids,\n",
        "    'source': source_col.values,\n",
        "    'n_authors_endorsed': hits.values,\n",
        "    'n_foils_endorsed': false_alarms.values,\n",
        "    'classical_score': classical_score.values,\n",
        "    'error_rate': error_rate.values,\n",
        "    'total_endorsements': (hits + false_alarms).values\n",
        "})\n",
        "\n",
        "print(f\"   Participant summary created with {len(participant_summary)} rows\")\n",
        "\n",
        "# Display summary statistics\n",
        "print_stats_box(\"Classical Score Statistics\", {\n",
        "    'N': len(classical_score),\n",
        "    'Mean': classical_score.mean(),\n",
        "    'Std Dev': classical_score.std(),\n",
        "    'Min': classical_score.min(),\n",
        "    'Max': classical_score.max(),\n",
        "    'Range': classical_score.max() - classical_score.min()\n",
        "})\n",
        "\n",
        "# Show score distribution by source\n",
        "print(\"\\n   Scores by data source:\")\n",
        "print(participant_summary.groupby('source')[['classical_score', 'error_rate']].agg(['mean', 'std', 'count']).round(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 0 VISUALIZATIONS\n",
        "# ============================================================\n",
        "print(\"\\n9. Creating Step 0 visualizations...\")\n",
        "\n",
        "# Plot 1: Missing data heatmap (sample of columns with missing data)\n",
        "fig, axes = plt.subplots(1, 2, figsize=FIGSIZE_MEDIUM)\n",
        "\n",
        "# Missing data by column\n",
        "ax1 = axes[0]\n",
        "cols_with_missing = missing_count[missing_count > 0].sort_values(ascending=False)\n",
        "if len(cols_with_missing) > 0:\n",
        "    cols_with_missing.head(20).plot(kind='barh', ax=ax1, color='coral')\n",
        "    ax1.set_xlabel('Number of Missing Values')\n",
        "    ax1.set_ylabel('Item')\n",
        "    ax1.set_title('Top 20 Items with Missing Data')\n",
        "else:\n",
        "    ax1.text(0.5, 0.5, 'No missing data', ha='center', va='center', transform=ax1.transAxes)\n",
        "    ax1.set_title('Missing Data Analysis')\n",
        "\n",
        "# Response distribution\n",
        "ax2 = axes[1]\n",
        "total_endorsements = item_data_binary.sum(axis=1)\n",
        "ax2.hist(total_endorsements, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax2.axvline(total_endorsements.mean(), color='red', linestyle='--', label=f'Mean: {total_endorsements.mean():.1f}')\n",
        "ax2.set_xlabel('Total Items Endorsed')\n",
        "ax2.set_ylabel('Number of Participants')\n",
        "ax2.set_title('Distribution of Total Endorsements per Participant')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()  # Display in notebook before saving (save_plot closes the figure)\n",
        "save_plot(fig, 'step0', 'missing_data_and_response_distribution.png', silent=True)\n",
        "\n",
        "# Plot 2: Classical score distribution\n",
        "fig, axes = plt.subplots(1, 2, figsize=FIGSIZE_MEDIUM)\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.hist(classical_score, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax1.axvline(classical_score.mean(), color='red', linestyle='--', label=f'Mean: {classical_score.mean():.1f}')\n",
        "ax1.set_xlabel('Classical ART Score (Hits - False Alarms)')\n",
        "ax1.set_ylabel('Number of Participants')\n",
        "ax1.set_title('Distribution of Classical ART Scores')\n",
        "ax1.legend()\n",
        "\n",
        "ax2 = axes[1]\n",
        "ax2.hist(error_rate, bins=30, edgecolor='black', alpha=0.7, color='coral')\n",
        "ax2.axvline(error_rate.mean(), color='red', linestyle='--', label=f'Mean: {error_rate.mean():.3f}')\n",
        "ax2.set_xlabel('Error Rate (Foils Endorsed / Total Foils)')\n",
        "ax2.set_ylabel('Number of Participants')\n",
        "ax2.set_title('Distribution of Foil Error Rates')\n",
        "ax2.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()  # Display in notebook before saving (save_plot closes the figure)\n",
        "save_plot(fig, 'step0', 'response_distribution.png', silent=True)\n",
        "print(\"  Saved step0 plots: missing_data_and_response_distribution.png, response_distribution.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE STEP 0 OUTPUTS\n",
        "# ============================================================\n",
        "print(\"\\n10. Saving Step 0 outputs...\")\n",
        "\n",
        "# Save author response matrix\n",
        "save_table(author_response_matrix, 'step0', 'author_response_matrix.csv')\n",
        "\n",
        "# Save foil response matrix\n",
        "save_table(foil_response_matrix, 'step0', 'foil_response_matrix.csv')\n",
        "\n",
        "# Save participant summary\n",
        "save_table(participant_summary, 'step0', 'participant_summary.csv', index=False)\n",
        "\n",
        "# Count participants by source\n",
        "source_counts = source_col.value_counts().sort_index()\n",
        "\n",
        "# Create data quality report\n",
        "data_quality_report = pd.DataFrame({\n",
        "    'metric': [\n",
        "        'Total participants',\n",
        "        'Total items',\n",
        "        'Real author items',\n",
        "        'Foil items',\n",
        "        'Unclassified items',\n",
        "        'Columns with missing data',\n",
        "        'Total missing cells',\n",
        "        'Participants from source 1',\n",
        "        'Participants from source 2',\n",
        "        'Mean classical score',\n",
        "        'Std classical score',\n",
        "        'Mean error rate',\n",
        "        'Std error rate'\n",
        "    ],\n",
        "    'value': [\n",
        "        len(participant_ids),\n",
        "        len(item_labels),\n",
        "        len(real_author_indices),\n",
        "        len(foil_indices),\n",
        "        len(unclassified_indices),\n",
        "        (missing_count > 0).sum(),\n",
        "        item_data.isnull().sum().sum(),\n",
        "        source_counts.iloc[0] if len(source_counts) > 0 else 0,\n",
        "        source_counts.iloc[1] if len(source_counts) > 1 else 0,\n",
        "        round(classical_score.mean(), 4),\n",
        "        round(classical_score.std(), 4),\n",
        "        round(error_rate.mean(), 4),\n",
        "        round(error_rate.std(), 4)\n",
        "    ]\n",
        "})\n",
        "save_table(data_quality_report, 'step0', 'data_quality_report.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" STEP 0 COMPLETE: Data loaded and preprocessed\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n Key outputs:\")\n",
        "print(f\"   - Author response matrix: {author_response_matrix.shape}\")\n",
        "print(f\"   - Foil response matrix: {foil_response_matrix.shape}\")\n",
        "print(f\"   - Participant summary: {participant_summary.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 2: Item Descriptive Checks (Step 1)\n",
        "\n",
        "Compute endorsement rates for each author item, identify floor/ceiling effects, and visualize item difficulty distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1: ITEM DESCRIPTIVE CHECKS\n",
        "# ============================================================\n",
        "print_section_header(\"STEP 1: ITEM DESCRIPTIVE CHECKS\")\n",
        "\n",
        "# Compute endorsement rate for each real author item\n",
        "# Endorsement rate = % of participants who marked each author\n",
        "endorsement_rates = author_response_matrix.mean()  # Mean of binary = proportion\n",
        "\n",
        "print(f\"\\n1. Computing endorsement rates for {len(endorsement_rates)} author items...\")\n",
        "\n",
        "# Create item statistics table\n",
        "item_stats = pd.DataFrame({\n",
        "    'author': endorsement_rates.index,\n",
        "    'endorsement_rate': endorsement_rates.values,\n",
        "    'endorsement_pct': endorsement_rates.values * 100,\n",
        "    'n_endorsed': author_response_matrix.sum().values,\n",
        "    'n_total': len(author_response_matrix)\n",
        "})\n",
        "\n",
        "# Sort by endorsement rate\n",
        "item_stats = item_stats.sort_values('endorsement_rate', ascending=False).reset_index(drop=True)\n",
        "\n",
        "# Display distribution statistics\n",
        "describe_distribution(endorsement_rates.values * 100, \"Endorsement Rate (%)\")\n",
        "\n",
        "# Show quintiles\n",
        "print(\"\\n   Endorsement rate quintiles:\")\n",
        "for q in [0, 20, 40, 60, 80, 100]:\n",
        "    val = np.percentile(endorsement_rates.values * 100, q)\n",
        "    print(f\"      {q}th percentile: {val:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FLAG ITEMS WITH FLOOR/CEILING EFFECTS\n",
        "# ============================================================\n",
        "print(\"\\n2. Flagging items with floor/ceiling effects...\")\n",
        "\n",
        "# Flag items with very low (<5%) or very high (>95%) endorsement\n",
        "item_stats['floor_effect'] = item_stats['endorsement_rate'] < ENDORSEMENT_LOW_THRESHOLD\n",
        "item_stats['ceiling_effect'] = item_stats['endorsement_rate'] > ENDORSEMENT_HIGH_THRESHOLD\n",
        "item_stats['flagged'] = item_stats['floor_effect'] | item_stats['ceiling_effect']\n",
        "\n",
        "# Count flagged items\n",
        "n_floor = item_stats['floor_effect'].sum()\n",
        "n_ceiling = item_stats['ceiling_effect'].sum()\n",
        "n_flagged = item_stats['flagged'].sum()\n",
        "\n",
        "print_stats_box(\"Item Flagging Summary\", {\n",
        "    'Total items': len(item_stats),\n",
        "    'Floor effect (<5%)': n_floor,\n",
        "    'Ceiling effect (>95%)': n_ceiling,\n",
        "    'Total flagged': n_flagged,\n",
        "    'Items OK': len(item_stats) - n_flagged,\n",
        "    'Floor threshold': f\"{ENDORSEMENT_LOW_THRESHOLD*100:.0f}%\",\n",
        "    'Ceiling threshold': f\"{ENDORSEMENT_HIGH_THRESHOLD*100:.0f}%\"\n",
        "})\n",
        "\n",
        "# Show flagged items\n",
        "if n_floor > 0:\n",
        "    print(f\"\\n   Items with FLOOR effect (endorsement < {ENDORSEMENT_LOW_THRESHOLD*100:.0f}%):\")\n",
        "    floor_items = item_stats[item_stats['floor_effect']][['author', 'endorsement_pct']].head(10)\n",
        "    print(tabulate(floor_items, headers='keys', tablefmt='simple', showindex=False, floatfmt='.2f'))\n",
        "\n",
        "if n_ceiling > 0:\n",
        "    print(f\"\\n   Items with CEILING effect (endorsement > {ENDORSEMENT_HIGH_THRESHOLD*100:.0f}%):\")\n",
        "    ceiling_items = item_stats[item_stats['ceiling_effect']][['author', 'endorsement_pct']].head(10)\n",
        "    print(tabulate(ceiling_items, headers='keys', tablefmt='simple', showindex=False, floatfmt='.2f'))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 1 VISUALIZATIONS\n",
        "# ============================================================\n",
        "print(\"\\n3. Creating Step 1 visualizations...\")\n",
        "\n",
        "# Plot 1: Histogram of endorsement rates\n",
        "fig, axes = plt.subplots(1, 2, figsize=FIGSIZE_MEDIUM)\n",
        "\n",
        "ax1 = axes[0]\n",
        "ax1.hist(endorsement_rates * 100, bins=25, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax1.axvline(ENDORSEMENT_LOW_THRESHOLD * 100, color='red', linestyle='--', \n",
        "            label=f'Floor threshold ({ENDORSEMENT_LOW_THRESHOLD*100:.0f}%)')\n",
        "ax1.axvline(ENDORSEMENT_HIGH_THRESHOLD * 100, color='orange', linestyle='--',\n",
        "            label=f'Ceiling threshold ({ENDORSEMENT_HIGH_THRESHOLD*100:.0f}%)')\n",
        "ax1.axvline(endorsement_rates.mean() * 100, color='green', linestyle='-',\n",
        "            label=f'Mean ({endorsement_rates.mean()*100:.1f}%)')\n",
        "ax1.set_xlabel('Endorsement Rate (%)')\n",
        "ax1.set_ylabel('Number of Items')\n",
        "ax1.set_title('Distribution of Item Endorsement Rates')\n",
        "ax1.legend(fontsize=9)\n",
        "\n",
        "# Add difficulty interpretation\n",
        "ax1.text(0.02, 0.98, 'Low endorsement = Hard items\\nHigh endorsement = Easy items',\n",
        "         transform=ax1.transAxes, fontsize=9, verticalalignment='top',\n",
        "         bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
        "\n",
        "# Plot 2: Sorted bar plot of top/bottom items\n",
        "ax2 = axes[1]\n",
        "n_show = 15\n",
        "# Top 15 easiest items\n",
        "top_items = item_stats.nlargest(n_show, 'endorsement_rate')\n",
        "bottom_items = item_stats.nsmallest(n_show, 'endorsement_rate')\n",
        "combined = pd.concat([top_items, bottom_items])\n",
        "\n",
        "colors = ['green' if rate > 0.5 else 'coral' for rate in combined['endorsement_rate']]\n",
        "bars = ax2.barh(range(len(combined)), combined['endorsement_pct'], color=colors, alpha=0.7)\n",
        "ax2.set_yticks(range(len(combined)))\n",
        "ax2.set_yticklabels(combined['author'].str[:20], fontsize=8)  # Truncate long names\n",
        "ax2.set_xlabel('Endorsement Rate (%)')\n",
        "ax2.set_title(f'Top {n_show} Easiest and Hardest Items')\n",
        "ax2.axvline(50, color='gray', linestyle=':', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()  # Display in notebook before saving (save_plot closes the figure)\n",
        "save_plot(fig, 'step1', 'endorsement_histogram.png', silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ADDITIONAL VISUALIZATION: All items sorted by endorsement\n",
        "# ============================================================\n",
        "\n",
        "# Create comprehensive bar plot of all items\n",
        "fig, ax = plt.subplots(figsize=(14, max(10, len(item_stats) * 0.15)))\n",
        "\n",
        "# Sort and plot\n",
        "sorted_stats = item_stats.sort_values('endorsement_rate')\n",
        "colors = ['red' if row['floor_effect'] else 'orange' if row['ceiling_effect'] else 'steelblue' \n",
        "          for _, row in sorted_stats.iterrows()]\n",
        "\n",
        "ax.barh(range(len(sorted_stats)), sorted_stats['endorsement_pct'], color=colors, alpha=0.7)\n",
        "ax.set_yticks(range(len(sorted_stats)))\n",
        "ax.set_yticklabels(sorted_stats['author'].str[:25], fontsize=6)\n",
        "ax.set_xlabel('Endorsement Rate (%)')\n",
        "ax.set_title('All Author Items Sorted by Endorsement Rate\\n(Red=Floor, Orange=Ceiling, Blue=OK)')\n",
        "ax.axvline(ENDORSEMENT_LOW_THRESHOLD * 100, color='red', linestyle='--', alpha=0.5)\n",
        "ax.axvline(ENDORSEMENT_HIGH_THRESHOLD * 100, color='orange', linestyle='--', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()  # Display in notebook before saving (save_plot closes the figure)\n",
        "save_plot(fig, 'step1', 'endorsement_by_author.png', silent=True)\n",
        "print(\"  Saved step1 plots: endorsement_histogram.png, endorsement_by_author.png\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE STEP 1 OUTPUTS\n",
        "# ============================================================\n",
        "print(\"\\n4. Saving Step 1 outputs...\")\n",
        "\n",
        "# Save full item endorsement table\n",
        "save_table(item_stats, 'step1', 'item_endorsement_rates.csv', index=False)\n",
        "\n",
        "# Save flagged items only\n",
        "flagged_items = item_stats[item_stats['flagged']][['author', 'endorsement_pct', 'floor_effect', 'ceiling_effect']]\n",
        "save_table(flagged_items, 'step1', 'flagged_items.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" STEP 1 COMPLETE: Item descriptive checks finished\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n Key findings:\")\n",
        "print(f\"   - Total author items analyzed: {len(item_stats)}\")\n",
        "print(f\"   - Mean endorsement rate: {endorsement_rates.mean()*100:.1f}%\")\n",
        "print(f\"   - Items with floor effect: {n_floor}\")\n",
        "print(f\"   - Items with ceiling effect: {n_ceiling}\")\n",
        "print(f\"   - Items OK for IRT: {len(item_stats) - n_flagged}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 3: Dimensionality Assessment (Step 2)\n",
        "\n",
        "Test whether the ART items measure a single latent trait (unidimensionality) using tetrachoric correlations and exploratory factor analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 2: DIMENSIONALITY ASSESSMENT\n",
        "# ============================================================\n",
        "print_section_header(\"STEP 2: DIMENSIONALITY ASSESSMENT\")\n",
        "\n",
        "print(\"\\nIRT assumes latent unidimensionality (one main trait).\")\n",
        "print(\"We will test whether a single common factor explains most covariance.\\n\")\n",
        "\n",
        "# Filter out items with extreme endorsement rates for correlation stability\n",
        "# Items with near-zero or near-one proportions can cause correlation issues\n",
        "print(\"1. Preparing data for factor analysis...\")\n",
        "\n",
        "# Use items that are not flagged (have reasonable variance)\n",
        "usable_items = item_stats[~item_stats['flagged']]['author'].tolist()\n",
        "print(f\"   Using {len(usable_items)} items (excluding {n_flagged} flagged items)\")\n",
        "\n",
        "# Create filtered response matrix\n",
        "fa_data = author_response_matrix[usable_items].copy()\n",
        "print(f\"   FA data shape: {fa_data.shape}\")\n",
        "\n",
        "# Check for items with zero variance (all 0s or all 1s)\n",
        "item_vars = fa_data.var()\n",
        "zero_var_items = item_vars[item_vars == 0].index.tolist()\n",
        "if zero_var_items:\n",
        "    print(f\"   WARNING: Removing {len(zero_var_items)} items with zero variance\")\n",
        "    fa_data = fa_data.drop(columns=zero_var_items)\n",
        "    usable_items = [i for i in usable_items if i not in zero_var_items]\n",
        "\n",
        "print(f\"   Final FA data shape: {fa_data.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPUTE CORRELATION MATRIX AND FACTORABILITY TESTS\n",
        "# ============================================================\n",
        "print(\"\\n2. Testing factorability of the data...\")\n",
        "\n",
        "# Compute Pearson correlation matrix (approximation for binary data)\n",
        "# Note: Tetrachoric correlations are ideal but Pearson works for EFA screening\n",
        "corr_matrix = fa_data.corr()\n",
        "\n",
        "print(f\"   Correlation matrix shape: {corr_matrix.shape}\")\n",
        "print(f\"   Mean inter-item correlation: {corr_matrix.values[np.triu_indices(len(corr_matrix), k=1)].mean():.4f}\")\n",
        "\n",
        "# Bartlett's test of sphericity\n",
        "# Tests if correlation matrix is identity (null: no relationships between items)\n",
        "try:\n",
        "    chi_square, p_value = calculate_bartlett_sphericity(fa_data)\n",
        "    print(f\"\\n   Bartlett's Test of Sphericity:\")\n",
        "    print(f\"      Chi-square: {chi_square:.2f}\")\n",
        "    print(f\"      p-value: {p_value:.2e}\")\n",
        "    print(f\"      Interpretation: {'Significant - data is factorable' if p_value < 0.05 else 'Not significant - concern!'}\")\n",
        "except Exception as e:\n",
        "    print(f\"   Bartlett's test failed: {e}\")\n",
        "    chi_square, p_value = np.nan, np.nan\n",
        "\n",
        "# Kaiser-Meyer-Olkin (KMO) measure of sampling adequacy\n",
        "# Values > 0.6 are acceptable, > 0.8 are good\n",
        "try:\n",
        "    kmo_all, kmo_model = calculate_kmo(fa_data)\n",
        "    print(f\"\\n   Kaiser-Meyer-Olkin (KMO) Measure:\")\n",
        "    print(f\"      Overall KMO: {kmo_model:.4f}\")\n",
        "    \n",
        "    if kmo_model >= 0.9:\n",
        "        kmo_interp = \"Marvelous\"\n",
        "    elif kmo_model >= 0.8:\n",
        "        kmo_interp = \"Meritorious\"\n",
        "    elif kmo_model >= 0.7:\n",
        "        kmo_interp = \"Middling\"\n",
        "    elif kmo_model >= 0.6:\n",
        "        kmo_interp = \"Mediocre\"\n",
        "    elif kmo_model >= 0.5:\n",
        "        kmo_interp = \"Miserable\"\n",
        "    else:\n",
        "        kmo_interp = \"Unacceptable\"\n",
        "    \n",
        "    print(f\"      Interpretation: {kmo_interp}\")\n",
        "except Exception as e:\n",
        "    print(f\"   KMO calculation failed: {e}\")\n",
        "    kmo_model = np.nan"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EIGENVALUE DECOMPOSITION AND SCREE PLOT\n",
        "# ============================================================\n",
        "print(\"\\n3. Computing eigenvalues for dimensionality assessment...\")\n",
        "\n",
        "# Compute eigenvalues from correlation matrix\n",
        "eigenvalues = np.linalg.eigvalsh(corr_matrix)\n",
        "eigenvalues = np.sort(eigenvalues)[::-1]  # Sort descending\n",
        "\n",
        "# Calculate variance explained\n",
        "total_variance = np.sum(eigenvalues)\n",
        "variance_explained = eigenvalues / total_variance * 100\n",
        "cumulative_variance = np.cumsum(variance_explained)\n",
        "\n",
        "print(f\"\\n   Top 10 Eigenvalues:\")\n",
        "eigenvalue_table = pd.DataFrame({\n",
        "    'Factor': range(1, 11),\n",
        "    'Eigenvalue': eigenvalues[:10],\n",
        "    'Variance %': variance_explained[:10],\n",
        "    'Cumulative %': cumulative_variance[:10]\n",
        "})\n",
        "print(tabulate(eigenvalue_table, headers='keys', tablefmt='simple', showindex=False, floatfmt='.3f'))\n",
        "\n",
        "# Kaiser criterion: eigenvalue > 1\n",
        "n_factors_kaiser = np.sum(eigenvalues > 1)\n",
        "print(f\"\\n   Kaiser criterion (eigenvalue > 1): {n_factors_kaiser} factors\")\n",
        "\n",
        "# Ratio of first to second eigenvalue (evidence for unidimensionality)\n",
        "ratio_1_2 = eigenvalues[0] / eigenvalues[1] if eigenvalues[1] > 0 else np.inf\n",
        "print(f\"   Ratio of 1st to 2nd eigenvalue: {ratio_1_2:.2f}\")\n",
        "print(f\"   (Ratio > 3-4 suggests strong unidimensionality)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SCREE PLOT WITH PARALLEL ANALYSIS\n",
        "# ============================================================\n",
        "print(\"\\n4. Creating scree plot with parallel analysis...\")\n",
        "\n",
        "# Parallel analysis: generate random data with same dimensions\n",
        "# and compute eigenvalues to establish threshold\n",
        "n_iterations = 100\n",
        "n_samples, n_vars = fa_data.shape\n",
        "random_eigenvalues = np.zeros((n_iterations, min(20, n_vars)))\n",
        "\n",
        "print(f\"   Running parallel analysis ({n_iterations} iterations)...\")\n",
        "for i in range(n_iterations):\n",
        "    if (i + 1) % 20 == 0 or i == 0:\n",
        "        print(f\"   Progress: {i+1}/{n_iterations} iterations ({100*(i+1)/n_iterations:.0f}%)\")\n",
        "    random_data = np.random.normal(0, 1, (n_samples, n_vars))\n",
        "    random_corr = np.corrcoef(random_data.T)\n",
        "    random_eig = np.linalg.eigvalsh(random_corr)\n",
        "    random_eig = np.sort(random_eig)[::-1]\n",
        "    random_eigenvalues[i, :] = random_eig[:min(20, n_vars)]\n",
        "print(f\"   Parallel analysis complete ({n_iterations} iterations)\")\n",
        "\n",
        "# Calculate 95th percentile of random eigenvalues\n",
        "parallel_threshold = np.percentile(random_eigenvalues, 95, axis=0)\n",
        "\n",
        "# Determine number of factors by parallel analysis\n",
        "n_factors_parallel = np.sum(eigenvalues[:len(parallel_threshold)] > parallel_threshold)\n",
        "print(f\"   Parallel analysis suggests: {n_factors_parallel} factors\")\n",
        "\n",
        "# Create scree plot\n",
        "fig, ax = plt.subplots(figsize=FIGSIZE_SMALL)\n",
        "\n",
        "x = range(1, min(21, len(eigenvalues) + 1))\n",
        "ax.plot(x, eigenvalues[:len(x)], 'bo-', markersize=8, linewidth=2, label='Actual eigenvalues')\n",
        "ax.plot(x, parallel_threshold[:len(x)], 'r--', linewidth=2, label='Parallel analysis threshold (95%)')\n",
        "ax.axhline(y=1, color='gray', linestyle=':', alpha=0.5, label='Kaiser criterion (eigenvalue=1)')\n",
        "\n",
        "ax.set_xlabel('Factor Number')\n",
        "ax.set_ylabel('Eigenvalue')\n",
        "ax.set_title('Scree Plot with Parallel Analysis')\n",
        "ax.legend()\n",
        "ax.set_xticks(x)\n",
        "ax.grid(True, alpha=0.3)\n",
        "\n",
        "# Annotate the elbow\n",
        "ax.annotate(f'1st: {eigenvalues[0]:.2f}', xy=(1, eigenvalues[0]), \n",
        "            xytext=(2, eigenvalues[0]+0.5), fontsize=9,\n",
        "            arrowprops=dict(arrowstyle='->', color='gray'))\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()  # Display in notebook before saving (save_plot closes the figure)\n",
        "save_plot(fig, 'step2', 'scree_plot.png', silent=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# EXPLORATORY FACTOR ANALYSIS: 1-FACTOR AND 2-FACTOR SOLUTIONS\n",
        "# ============================================================\n",
        "print(\"\\n5. Running exploratory factor analysis...\")\n",
        "\n",
        "# Convert data to numpy array and ensure no NaNs for FactorAnalyzer/scikit-learn compatibility\n",
        "fa_data_array = fa_data.values.astype(float)\n",
        "if np.any(np.isnan(fa_data_array)):\n",
        "    print(\"   Warning: Missing values detected in factor analysis data matrix. Imputing with column mean for each variable.\")\n",
        "    # Simple mean imputation for compatibility; replace with more sophisticated methods if needed\n",
        "    col_means = np.nanmean(fa_data_array, axis=0)\n",
        "    inds = np.where(np.isnan(fa_data_array))\n",
        "    fa_data_array[inds] = np.take(col_means, inds[1])\n",
        "\n",
        "# --- FIX FOR RECURSIVE check_array WRAPPING ---\n",
        "# The RecursionError is caused by multiple redefinitions of check_array in this notebook.\n",
        "# We need to restore the original sklearn check_array function before calling FactorAnalyzer.\n",
        "# (imports removed)\n",
        "\n",
        "# Fit 1-factor model (using 'principal' method to avoid scikit-learn version issues)\n",
        "print(\"\\n   Fitting 1-factor model...\")\n",
        "fa_1 = FactorAnalyzer(n_factors=1, rotation=None, method='principal')\n",
        "fa_1.fit(fa_data_array)\n",
        "\n",
        "# Get loadings and variance explained\n",
        "loadings_1 = fa_1.loadings_\n",
        "variance_1 = fa_1.get_factor_variance()\n",
        "\n",
        "print(f\"   1-Factor model results:\")\n",
        "print(f\"      Variance explained: {variance_1[1][0]*100:.1f}%\")\n",
        "print(f\"      Mean loading: {np.abs(loadings_1).mean():.3f}\")\n",
        "print(f\"      Loadings > 0.3: {np.sum(np.abs(loadings_1) > 0.3)} items\")\n",
        "print(f\"      Loadings > 0.5: {np.sum(np.abs(loadings_1) > 0.5)} items\")\n",
        "\n",
        "# Fit 2-factor model (using 'principal' method to avoid scikit-learn version issues)\n",
        "print(\"\\n   Fitting 2-factor model (with varimax rotation)...\")\n",
        "fa_2 = FactorAnalyzer(n_factors=2, rotation='varimax', method='principal')\n",
        "fa_2.fit(fa_data_array)\n",
        "\n",
        "loadings_2 = fa_2.loadings_\n",
        "variance_2 = fa_2.get_factor_variance()\n",
        "\n",
        "print(f\"   2-Factor model results:\")\n",
        "print(f\"      Factor 1 variance: {variance_2[1][0]*100:.1f}%\")\n",
        "print(f\"      Factor 2 variance: {variance_2[1][1]*100:.1f}%\")\n",
        "print(f\"      Total variance explained: {variance_2[2][1]*100:.1f}%\")\n",
        "\n",
        "# Compare models\n",
        "print(f\"\\n   Model comparison:\")\n",
        "print(f\"      1-factor: {variance_1[2][0]*100:.1f}% variance explained\")\n",
        "print(f\"      2-factor: {variance_2[2][1]*100:.1f}% variance explained\")\n",
        "print(f\"      Additional variance from 2nd factor: {(variance_2[2][1] - variance_1[2][0])*100:.1f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FACTOR LOADINGS VISUALIZATION\n",
        "# ============================================================\n",
        "print(\"\\n6. Creating factor loadings visualizations...\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=FIGSIZE_MEDIUM)\n",
        "\n",
        "# 1-factor loadings distribution\n",
        "ax1 = axes[0]\n",
        "ax1.hist(loadings_1.flatten(), bins=25, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "ax1.axvline(0.3, color='red', linestyle='--', label='0.3 threshold')\n",
        "ax1.axvline(-0.3, color='red', linestyle='--')\n",
        "ax1.axvline(0.5, color='orange', linestyle='--', label='0.5 threshold')\n",
        "ax1.axvline(-0.5, color='orange', linestyle='--')\n",
        "ax1.set_xlabel('Factor Loading')\n",
        "ax1.set_ylabel('Number of Items')\n",
        "ax1.set_title('1-Factor Model: Loading Distribution')\n",
        "ax1.legend(fontsize=9)\n",
        "\n",
        "# 2-factor loadings scatter\n",
        "ax2 = axes[1]\n",
        "ax2.scatter(loadings_2[:, 0], loadings_2[:, 1], alpha=0.6, s=50)\n",
        "ax2.axhline(0, color='gray', linestyle='-', alpha=0.3)\n",
        "ax2.axvline(0, color='gray', linestyle='-', alpha=0.3)\n",
        "ax2.set_xlabel('Factor 1 Loading')\n",
        "ax2.set_ylabel('Factor 2 Loading')\n",
        "ax2.set_title('2-Factor Model: Item Loadings')\n",
        "\n",
        "# Add reference lines\n",
        "ax2.axhline(0.3, color='red', linestyle=':', alpha=0.5)\n",
        "ax2.axhline(-0.3, color='red', linestyle=':', alpha=0.5)\n",
        "ax2.axvline(0.3, color='red', linestyle=':', alpha=0.5)\n",
        "ax2.axvline(-0.3, color='red', linestyle=':', alpha=0.5)\n",
        "\n",
        "plt.tight_layout()\n",
        "save_plot(fig, 'step2', 'factor_loadings_heatmap.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# DIMENSIONALITY DECISION AND SAVE OUTPUTS\n",
        "# ============================================================\n",
        "print(\"\\n7. Dimensionality decision...\")\n",
        "\n",
        "# Determine if unidimensional\n",
        "# Criteria: \n",
        "# 1. First eigenvalue dominates (ratio > 3)\n",
        "# 2. First factor explains substantial variance (>20%)\n",
        "# 3. Parallel analysis suggests 1-2 factors\n",
        "\n",
        "is_unidimensional = (ratio_1_2 > 3) or (variance_1[1][0] > 0.15)\n",
        "\n",
        "print(f\"\\n   Decision criteria:\")\n",
        "print(f\"      Eigenvalue ratio (1st/2nd): {ratio_1_2:.2f} (criterion: > 3)\")\n",
        "print(f\"      1st factor variance: {variance_1[1][0]*100:.1f}% (criterion: > 15-20%)\")\n",
        "print(f\"      Kaiser factors: {n_factors_kaiser}\")\n",
        "print(f\"      Parallel analysis factors: {n_factors_parallel}\")\n",
        "\n",
        "if is_unidimensional:\n",
        "    print(f\"\\n   DECISION: Treat as UNIDIMENSIONAL\")\n",
        "    print(f\"   Rationale: The first factor is dominant and explains sufficient variance.\")\n",
        "    print(f\"   Proceeding with unidimensional 2PL IRT model.\")\n",
        "else:\n",
        "    print(f\"\\n   DECISION: Consider MULTIDIMENSIONALITY\")\n",
        "    print(f\"   Rationale: Multiple factors may be meaningful.\")\n",
        "    print(f\"   For this analysis, we proceed with unidimensional IRT (common practice)\")\n",
        "    print(f\"   but document the potential multidimensionality.\")\n",
        "\n",
        "# Save outputs\n",
        "print(\"\\n8. Saving Step 2 outputs...\")\n",
        "\n",
        "# Save eigenvalues\n",
        "eigenvalue_df = pd.DataFrame({\n",
        "    'factor': range(1, len(eigenvalues) + 1),\n",
        "    'eigenvalue': eigenvalues,\n",
        "    'variance_pct': variance_explained,\n",
        "    'cumulative_pct': cumulative_variance,\n",
        "    'parallel_threshold': list(parallel_threshold) + [np.nan] * (len(eigenvalues) - len(parallel_threshold))\n",
        "})\n",
        "save_table(eigenvalue_df, 'step2', 'eigenvalues.csv', index=False)\n",
        "\n",
        "# Save factor loadings\n",
        "loadings_df = pd.DataFrame({\n",
        "    'item': usable_items,\n",
        "    'loading_1factor': loadings_1.flatten(),\n",
        "    'loading_f1_2factor': loadings_2[:, 0],\n",
        "    'loading_f2_2factor': loadings_2[:, 1]\n",
        "})\n",
        "save_table(loadings_df, 'step2', 'factor_loadings.csv', index=False)\n",
        "\n",
        "# Save model fit comparison\n",
        "model_fit = pd.DataFrame({\n",
        "    'model': ['1-factor', '2-factor'],\n",
        "    'variance_explained_pct': [variance_1[2][0]*100, variance_2[2][1]*100],\n",
        "    'n_items': [len(usable_items), len(usable_items)],\n",
        "    'kmo': [kmo_model if 'kmo_model' in dir() else np.nan, kmo_model if 'kmo_model' in dir() else np.nan],\n",
        "    'bartlett_chi2': [chi_square if 'chi_square' in dir() else np.nan, chi_square if 'chi_square' in dir() else np.nan],\n",
        "    'bartlett_p': [p_value if 'p_value' in dir() else np.nan, p_value if 'p_value' in dir() else np.nan]\n",
        "})\n",
        "save_table(model_fit, 'step2', 'model_fit_comparison.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" STEP 2 COMPLETE: Dimensionality assessment finished\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 4: Fit 2PL IRT Model (Step 3)\n",
        "\n",
        "Fit a two-parameter logistic (2PL) IRT model to estimate item discrimination (a) and difficulty (b) parameters."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 3: FIT 2PL IRT MODEL\n",
        "# ============================================================\n",
        "print_section_header(\"STEP 3: FIT 2PL IRT MODEL\")\n",
        "\n",
        "print(\"\"\"\n",
        "The 2PL (Two-Parameter Logistic) model estimates:\n",
        "  - Discrimination (a): How well item differentiates ability levels\n",
        "  - Difficulty (b): Ability level where P(correct) = 0.5\n",
        "\n",
        "Model: P(X=1|θ) = 1 / (1 + exp(-a(θ - b)))\n",
        "\"\"\")\n",
        "\n",
        "# Prepare data for girth\n",
        "# girth expects: items x subjects (transpose of our matrix)\n",
        "print(\"1. Preparing data for IRT model...\")\n",
        "\n",
        "# Use all author items (including flagged ones for full calibration)\n",
        "irt_data = author_response_matrix.values.T  # Shape: items x subjects\n",
        "print(f\"   IRT data shape: {irt_data.shape} (items x subjects)\")\n",
        "print(f\"   Number of items: {irt_data.shape[0]}\")\n",
        "print(f\"   Number of subjects: {irt_data.shape[1]}\")\n",
        "\n",
        "# Check data validity\n",
        "print(f\"\\n   Data checks:\")\n",
        "print(f\"      Min value: {irt_data.min()}\")\n",
        "print(f\"      Max value: {irt_data.max()}\")\n",
        "print(f\"      Contains NaN: {np.isnan(irt_data).any()}\")\n",
        "print(f\"      Items with zero endorsements: {np.sum(irt_data.sum(axis=1) == 0)}\")\n",
        "print(f\"      Items with 100% endorsements: {np.sum(irt_data.sum(axis=1) == irt_data.shape[1])}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# FIT 2PL MODEL USING GIRTH\n",
        "# ============================================================\n",
        "print(\"\\n2. Fitting 2PL IRT model using Marginal Maximum Likelihood...\")\n",
        "print(\"   This may take a few minutes for large datasets...\\n\")\n",
        "\n",
        "# Remove items with zero variance (all 0s or all 1s) for IRT\n",
        "item_sums = irt_data.sum(axis=1)\n",
        "valid_items_mask = (item_sums > 0) & (item_sums < irt_data.shape[1])\n",
        "n_removed = (~valid_items_mask).sum()\n",
        "\n",
        "if n_removed > 0:\n",
        "    print(f\"   Removing {n_removed} items with no variance (all 0s or all 1s)\")\n",
        "    irt_data_clean = irt_data[valid_items_mask]\n",
        "    valid_item_names = [author_response_matrix.columns[i] for i in range(len(valid_items_mask)) if valid_items_mask[i]]\n",
        "else:\n",
        "    irt_data_clean = irt_data\n",
        "    valid_item_names = list(author_response_matrix.columns)\n",
        "\n",
        "print(f\"   Items for IRT: {irt_data_clean.shape[0]}\")\n",
        "\n",
        "# Fit 2PL model\n",
        "try:\n",
        "    # twopl_mml returns a dictionary with 'Discrimination' and 'Difficulty'\n",
        "    irt_results = twopl_mml(irt_data_clean)\n",
        "    \n",
        "    # Extract parameters\n",
        "    discriminations = irt_results['Discrimination']\n",
        "    difficulties = irt_results['Difficulty']\n",
        "    \n",
        "    print(f\"\\n   Model converged successfully!\")\n",
        "    print(f\"   Estimated {len(discriminations)} discrimination parameters\")\n",
        "    print(f\"   Estimated {len(difficulties)} difficulty parameters\")\n",
        "    \n",
        "    model_converged = True\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"\\n   ERROR: Model fitting failed: {e}\")\n",
        "    print(\"   Attempting alternative estimation...\")\n",
        "    model_converged = False\n",
        "    discriminations = None\n",
        "    difficulties = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SUMMARIZE IRT PARAMETERS\n",
        "# ============================================================\n",
        "print(\"\\n3. Summarizing IRT parameter estimates...\")\n",
        "\n",
        "if model_converged:\n",
        "    # Create item parameters DataFrame\n",
        "    irt_params = pd.DataFrame({\n",
        "        'item': valid_item_names,\n",
        "        'discrimination_a': discriminations,\n",
        "        'difficulty_b': difficulties\n",
        "    })\n",
        "    \n",
        "    # Add endorsement rate for reference\n",
        "    endorsement_dict = dict(zip(item_stats['author'], item_stats['endorsement_rate']))\n",
        "    irt_params['endorsement_rate'] = irt_params['item'].map(endorsement_dict)\n",
        "    \n",
        "    # Summary statistics for discrimination\n",
        "    print_stats_box(\"Discrimination (a) Parameters\", {\n",
        "        'N items': len(discriminations),\n",
        "        'Mean': np.mean(discriminations),\n",
        "        'Std Dev': np.std(discriminations),\n",
        "        'Min': np.min(discriminations),\n",
        "        'Max': np.max(discriminations),\n",
        "        'Items a < 0.5': np.sum(discriminations < 0.5),\n",
        "        'Items a > 2.0': np.sum(discriminations > 2.0)\n",
        "    })\n",
        "    \n",
        "    # Summary statistics for difficulty\n",
        "    print_stats_box(\"Difficulty (b) Parameters\", {\n",
        "        'N items': len(difficulties),\n",
        "        'Mean': np.mean(difficulties),\n",
        "        'Std Dev': np.std(difficulties),\n",
        "        'Min': np.min(difficulties),\n",
        "        'Max': np.max(difficulties),\n",
        "        'Items |b| > 3': np.sum(np.abs(difficulties) > 3)\n",
        "    })\n",
        "    \n",
        "    # Show correlation between b and endorsement rate\n",
        "    # Higher endorsement = easier = lower b (should be negative correlation)\n",
        "    corr_b_endorse = np.corrcoef(irt_params['difficulty_b'].dropna(), \n",
        "                                  irt_params['endorsement_rate'].dropna())[0, 1]\n",
        "    print(f\"\\n   Correlation between difficulty (b) and endorsement rate: {corr_b_endorse:.3f}\")\n",
        "    print(f\"   (Expected: negative, as higher endorsement = easier = lower b)\")\n",
        "\n",
        "else:\n",
        "    print(\"   Model did not converge. Cannot summarize parameters.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE STEP 3 OUTPUTS\n",
        "# ============================================================\n",
        "print(\"\\n4. Saving Step 3 outputs...\")\n",
        "\n",
        "if model_converged:\n",
        "    # Save item parameters\n",
        "    save_table(irt_params, 'step3', 'item_parameters.csv', index=False)\n",
        "    \n",
        "    # Save convergence info\n",
        "    convergence_info = pd.DataFrame({\n",
        "        'metric': ['model_type', 'converged', 'n_items', 'n_subjects', \n",
        "                   'mean_discrimination', 'mean_difficulty'],\n",
        "        'value': ['2PL MML', 'Yes', len(discriminations), irt_data_clean.shape[1],\n",
        "                  np.mean(discriminations), np.mean(difficulties)]\n",
        "    })\n",
        "    save_table(convergence_info, 'step3', 'model_convergence.csv', index=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" STEP 3 COMPLETE: 2PL IRT model fitted successfully\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"   Skipping save - model did not converge\")\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" STEP 3 INCOMPLETE: Model fitting failed\")\n",
        "    print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 5: Item Parameter Analysis (Step 4)\n",
        "\n",
        "Analyze the estimated item parameters, identify problematic items, and visualize the relationship between discrimination and difficulty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 4: ITEM PARAMETER ANALYSIS\n",
        "# ============================================================\n",
        "print_section_header(\"STEP 4: ITEM PARAMETER ANALYSIS\")\n",
        "\n",
        "if model_converged:\n",
        "    print(\"\\n1. Flagging problematic items...\")\n",
        "    \n",
        "    # Flag items based on IRT criteria\n",
        "    irt_params['low_discrimination'] = irt_params['discrimination_a'] < DISCRIMINATION_LOW_THRESHOLD\n",
        "    irt_params['extreme_difficulty'] = np.abs(irt_params['difficulty_b']) > DIFFICULTY_EXTREME_THRESHOLD\n",
        "    irt_params['irt_flagged'] = irt_params['low_discrimination'] | irt_params['extreme_difficulty']\n",
        "    \n",
        "    n_low_disc = irt_params['low_discrimination'].sum()\n",
        "    n_extreme_diff = irt_params['extreme_difficulty'].sum()\n",
        "    n_irt_flagged = irt_params['irt_flagged'].sum()\n",
        "    \n",
        "    print_stats_box(\"Item Quality Flags\", {\n",
        "        'Total items': len(irt_params),\n",
        "        f'Low discrimination (a < {DISCRIMINATION_LOW_THRESHOLD})': n_low_disc,\n",
        "        f'Extreme difficulty (|b| > {DIFFICULTY_EXTREME_THRESHOLD})': n_extreme_diff,\n",
        "        'Total IRT-flagged': n_irt_flagged,\n",
        "        'Items OK': len(irt_params) - n_irt_flagged\n",
        "    })\n",
        "    \n",
        "    # Show top 10 best discriminating items\n",
        "    print(\"\\n   Top 10 BEST discriminating items:\")\n",
        "    top_disc = irt_params.nlargest(10, 'discrimination_a')[['item', 'discrimination_a', 'difficulty_b', 'endorsement_rate']]\n",
        "    print(tabulate(top_disc, headers='keys', tablefmt='simple', showindex=False, floatfmt='.3f'))\n",
        "    \n",
        "    # Show top 10 worst discriminating items\n",
        "    print(\"\\n   Top 10 WORST discriminating items:\")\n",
        "    bottom_disc = irt_params.nsmallest(10, 'discrimination_a')[['item', 'discrimination_a', 'difficulty_b', 'endorsement_rate']]\n",
        "    print(tabulate(bottom_disc, headers='keys', tablefmt='simple', showindex=False, floatfmt='.3f'))\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ITEM PARAMETER VISUALIZATIONS\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n2. Creating item parameter visualizations...\")\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=FIGSIZE_LARGE)\n",
        "    \n",
        "    # Plot 1: Discrimination (a) distribution\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.hist(irt_params['discrimination_a'], bins=25, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "    ax1.axvline(DISCRIMINATION_LOW_THRESHOLD, color='red', linestyle='--', \n",
        "                label=f'Low threshold ({DISCRIMINATION_LOW_THRESHOLD})')\n",
        "    ax1.axvline(irt_params['discrimination_a'].mean(), color='green', linestyle='-',\n",
        "                label=f\"Mean ({irt_params['discrimination_a'].mean():.2f})\")\n",
        "    ax1.set_xlabel('Discrimination (a)')\n",
        "    ax1.set_ylabel('Number of Items')\n",
        "    ax1.set_title('Distribution of Item Discrimination')\n",
        "    ax1.legend(fontsize=9)\n",
        "    \n",
        "    # Plot 2: Difficulty (b) distribution\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.hist(irt_params['difficulty_b'], bins=25, edgecolor='black', alpha=0.7, color='coral')\n",
        "    ax2.axvline(-DIFFICULTY_EXTREME_THRESHOLD, color='red', linestyle='--')\n",
        "    ax2.axvline(DIFFICULTY_EXTREME_THRESHOLD, color='red', linestyle='--', \n",
        "                label=f'Extreme threshold (±{DIFFICULTY_EXTREME_THRESHOLD})')\n",
        "    ax2.axvline(0, color='gray', linestyle='-', alpha=0.5, label='Mean ability (θ=0)')\n",
        "    ax2.set_xlabel('Difficulty (b)')\n",
        "    ax2.set_ylabel('Number of Items')\n",
        "    ax2.set_title('Distribution of Item Difficulty')\n",
        "    ax2.legend(fontsize=9)\n",
        "    \n",
        "    # Plot 3: Scatter plot of difficulty vs discrimination\n",
        "    ax3 = axes[1, 0]\n",
        "    colors = ['red' if flag else 'steelblue' for flag in irt_params['irt_flagged']]\n",
        "    ax3.scatter(irt_params['difficulty_b'], irt_params['discrimination_a'], \n",
        "                c=colors, alpha=0.6, s=50)\n",
        "    ax3.axhline(DISCRIMINATION_LOW_THRESHOLD, color='red', linestyle='--', alpha=0.5)\n",
        "    ax3.axvline(-DIFFICULTY_EXTREME_THRESHOLD, color='red', linestyle='--', alpha=0.5)\n",
        "    ax3.axvline(DIFFICULTY_EXTREME_THRESHOLD, color='red', linestyle='--', alpha=0.5)\n",
        "    ax3.set_xlabel('Difficulty (b)')\n",
        "    ax3.set_ylabel('Discrimination (a)')\n",
        "    ax3.set_title('Item Parameter Space (Red = Flagged)')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Difficulty vs Endorsement Rate\n",
        "    ax4 = axes[1, 1]\n",
        "    ax4.scatter(irt_params['endorsement_rate'] * 100, irt_params['difficulty_b'], \n",
        "                alpha=0.6, s=50, c='steelblue')\n",
        "    z = np.polyfit(irt_params['endorsement_rate'].dropna() * 100, \n",
        "                   irt_params['difficulty_b'].dropna(), 1)\n",
        "    p = np.poly1d(z)\n",
        "    x_line = np.linspace(0, 100, 100)\n",
        "    ax4.plot(x_line, p(x_line), 'r--', alpha=0.7, label=f'r = {corr_b_endorse:.2f}')\n",
        "    ax4.set_xlabel('Endorsement Rate (%)')\n",
        "    ax4.set_ylabel('Difficulty (b)')\n",
        "    ax4.set_title('Difficulty vs Endorsement Rate')\n",
        "    ax4.legend()\n",
        "    ax4.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, 'step4', 'item_parameter_distributions.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE STEP 4 OUTPUTS\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n3. Saving Step 4 outputs...\")\n",
        "    \n",
        "    # Save complete item parameter table\n",
        "    save_table(irt_params, 'step4', 'item_parameter_table.csv', index=False)\n",
        "    \n",
        "    # Save flagged items only\n",
        "    flagged_irt_items = irt_params[irt_params['irt_flagged']][\n",
        "        ['item', 'discrimination_a', 'difficulty_b', 'endorsement_rate', \n",
        "         'low_discrimination', 'extreme_difficulty']\n",
        "    ]\n",
        "    save_table(flagged_irt_items, 'step4', 'item_quality_flags.csv', index=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" STEP 4 COMPLETE: Item parameter analysis finished\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n Key findings:\")\n",
        "    print(f\"   - Mean discrimination: {irt_params['discrimination_a'].mean():.3f}\")\n",
        "    print(f\"   - Mean difficulty: {irt_params['difficulty_b'].mean():.3f}\")\n",
        "    print(f\"   - Items with low discrimination: {n_low_disc}\")\n",
        "    print(f\"   - Items with extreme difficulty: {n_extreme_diff}\")\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 6: Visual Diagnostics (Step 5)\n",
        "\n",
        "Plot Item Characteristic Curves (ICC) and Test Information Function (TIF) to understand item and test behavior across the ability range."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 5: VISUAL DIAGNOSTICS - ICC AND TIF\n",
        "# ============================================================\n",
        "print_section_header(\"STEP 5: VISUAL DIAGNOSTICS\")\n",
        "\n",
        "if model_converged:\n",
        "    print(\"\\n1. Defining IRT plotting functions...\")\n",
        "    \n",
        "    def icc_2pl(theta, a, b):\n",
        "        \"\"\"\n",
        "        Compute Item Characteristic Curve for 2PL model.\n",
        "        P(X=1|θ) = 1 / (1 + exp(-a(θ - b)))\n",
        "        \"\"\"\n",
        "        return expit(a * (theta - b))\n",
        "    \n",
        "    def item_information_2pl(theta, a, b):\n",
        "        \"\"\"\n",
        "        Compute Item Information Function for 2PL model.\n",
        "        I(θ) = a² * P(θ) * Q(θ)\n",
        "        where Q(θ) = 1 - P(θ)\n",
        "        \"\"\"\n",
        "        p = icc_2pl(theta, a, b)\n",
        "        q = 1 - p\n",
        "        return (a ** 2) * p * q\n",
        "    \n",
        "    # Define theta range for plotting\n",
        "    theta_range = np.linspace(-4, 4, 200)\n",
        "    \n",
        "    print(\"   ICC and information functions defined.\")\n",
        "    print(f\"   Theta range for plotting: [{theta_range.min()}, {theta_range.max()}]\")\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# PLOT ALL ITEM CHARACTERISTIC CURVES (ICC)\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n2. Creating ICC plots...\")\n",
        "    \n",
        "    # Calculate grid dimensions for all items\n",
        "    n_items = len(irt_params)\n",
        "    n_cols = 10\n",
        "    n_rows = int(np.ceil(n_items / n_cols))\n",
        "    \n",
        "    # Create ICC grid plot (all items)\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(20, n_rows * 2))\n",
        "    axes = axes.flatten()\n",
        "    \n",
        "    print(f\"   Plotting {n_items} ICCs in a {n_rows}x{n_cols} grid...\")\n",
        "    \n",
        "    for i, (_, row) in enumerate(tqdm(irt_params.iterrows(), total=n_items, desc=\"   Plotting ICCs\")):\n",
        "        ax = axes[i]\n",
        "        a = row['discrimination_a']\n",
        "        b = row['difficulty_b']\n",
        "        \n",
        "        # Calculate ICC\n",
        "        p = icc_2pl(theta_range, a, b)\n",
        "        \n",
        "        # Color based on quality\n",
        "        color = 'red' if row['irt_flagged'] else 'steelblue'\n",
        "        ax.plot(theta_range, p, color=color, linewidth=1)\n",
        "        ax.axhline(0.5, color='gray', linestyle=':', alpha=0.5)\n",
        "        ax.axvline(b, color='gray', linestyle=':', alpha=0.5)\n",
        "        ax.set_ylim(0, 1)\n",
        "        ax.set_xlim(-4, 4)\n",
        "        ax.set_title(row['item'][:15], fontsize=6)\n",
        "        ax.tick_params(labelsize=5)\n",
        "        \n",
        "        # Only label edge plots\n",
        "        if i >= n_items - n_cols:\n",
        "            ax.set_xlabel('θ', fontsize=6)\n",
        "        if i % n_cols == 0:\n",
        "            ax.set_ylabel('P(X=1)', fontsize=6)\n",
        "    \n",
        "    # Hide empty subplots\n",
        "    for j in range(n_items, len(axes)):\n",
        "        axes[j].set_visible(False)\n",
        "    \n",
        "    plt.suptitle('Item Characteristic Curves for All Items (Red = Flagged)', fontsize=14, y=1.02)\n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, 'step5', 'icc_all_items.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPUTE AND PLOT TEST INFORMATION FUNCTION\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n3. Computing Test Information Function...\")\n",
        "    \n",
        "    # Calculate item information for each item across theta range\n",
        "    item_info_matrix = np.zeros((len(irt_params), len(theta_range)))\n",
        "    \n",
        "    for i, (_, row) in enumerate(irt_params.iterrows()):\n",
        "        item_info_matrix[i, :] = item_information_2pl(theta_range, \n",
        "                                                       row['discrimination_a'], \n",
        "                                                       row['difficulty_b'])\n",
        "    \n",
        "    # Test Information = sum of all item information\n",
        "    test_information = item_info_matrix.sum(axis=0)\n",
        "    \n",
        "    # Find peak information\n",
        "    peak_theta = theta_range[np.argmax(test_information)]\n",
        "    peak_info = test_information.max()\n",
        "    \n",
        "    print(f\"   Peak information: {peak_info:.2f} at θ = {peak_theta:.2f}\")\n",
        "    print(f\"   Information at θ = -2: {test_information[np.argmin(np.abs(theta_range - (-2)))]:.2f}\")\n",
        "    print(f\"   Information at θ = 0: {test_information[np.argmin(np.abs(theta_range - 0))]:.2f}\")\n",
        "    print(f\"   Information at θ = +2: {test_information[np.argmin(np.abs(theta_range - 2))]:.2f}\")\n",
        "    \n",
        "    # Standard error of measurement\n",
        "    sem = 1 / np.sqrt(test_information)\n",
        "    \n",
        "    # Create TIF plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=FIGSIZE_MEDIUM)\n",
        "    \n",
        "    # Test Information Function\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(theta_range, test_information, 'b-', linewidth=2, label='Test Information')\n",
        "    ax1.fill_between(theta_range, test_information, alpha=0.3)\n",
        "    ax1.axvline(peak_theta, color='red', linestyle='--', label=f'Peak at θ={peak_theta:.2f}')\n",
        "    ax1.set_xlabel('Ability (θ)')\n",
        "    ax1.set_ylabel('Information')\n",
        "    ax1.set_title('Test Information Function (TIF)')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Standard Error of Measurement\n",
        "    ax2 = axes[1]\n",
        "    ax2.plot(theta_range, sem, 'r-', linewidth=2)\n",
        "    ax2.fill_between(theta_range, sem, alpha=0.3, color='coral')\n",
        "    ax2.set_xlabel('Ability (θ)')\n",
        "    ax2.set_ylabel('Standard Error')\n",
        "    ax2.set_title('Standard Error of Measurement')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    ax2.set_ylim(0, min(2, sem.max() * 1.1))\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, 'step5', 'test_information_function.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# ITEM INFORMATION CURVES\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n4. Creating Item Information Curves...\")\n",
        "    \n",
        "    fig, ax = plt.subplots(figsize=FIGSIZE_SMALL)\n",
        "    \n",
        "    # Plot a sample of items (top 20 by discrimination)\n",
        "    top_items = irt_params.nlargest(20, 'discrimination_a')\n",
        "    \n",
        "    for _, row in top_items.iterrows():\n",
        "        info = item_information_2pl(theta_range, row['discrimination_a'], row['difficulty_b'])\n",
        "        ax.plot(theta_range, info, alpha=0.5, linewidth=1)\n",
        "    \n",
        "    # Plot total (scaled for visibility)\n",
        "    ax.plot(theta_range, test_information / len(irt_params), 'k-', linewidth=3, \n",
        "            label=f'Mean Item Info (TIF/{len(irt_params)})')\n",
        "    \n",
        "    ax.set_xlabel('Ability (θ)')\n",
        "    ax.set_ylabel('Information')\n",
        "    ax.set_title(f'Item Information Curves (Top 20 Discriminating Items)')\n",
        "    ax.legend()\n",
        "    ax.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, 'step5', 'item_information_curves.png')\n",
        "    \n",
        "    # Analyze information coverage\n",
        "    print(\"\\n   Information coverage analysis:\")\n",
        "    ability_bands = [(-4, -2), (-2, -1), (-1, 0), (0, 1), (1, 2), (2, 4)]\n",
        "    \n",
        "    for low, high in ability_bands:\n",
        "        mask = (theta_range >= low) & (theta_range < high)\n",
        "        band_info = test_information[mask].mean()\n",
        "        print(f\"      θ ∈ [{low:+.0f}, {high:+.0f}): Mean info = {band_info:.2f}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" STEP 5 COMPLETE: Visual diagnostics created\")\n",
        "    print(\"=\"*70)\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 7: Latent Scores Extraction (Step 6)\n",
        "\n",
        "Compute latent ability estimates (θ) for all participants using Expected A Posteriori (EAP) estimation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 6: LATENT SCORES EXTRACTION\n",
        "# ============================================================\n",
        "print_section_header(\"STEP 6: LATENT SCORES EXTRACTION\")\n",
        "\n",
        "if model_converged:\n",
        "    print(\"\\n1. Computing EAP (Expected A Posteriori) ability estimates...\")\n",
        "    print(\"   This uses numerical integration over the ability distribution.\\n\")\n",
        "    \n",
        "    def compute_eap_theta(response_pattern, a_params, b_params, theta_grid, prior_std=1.0):\n",
        "        \"\"\"\n",
        "        Compute EAP (Expected A Posteriori) estimate of theta for a response pattern.\n",
        "        \n",
        "        Uses numerical integration with a normal prior on theta.\n",
        "        θ_EAP = ∫ θ * L(θ) * p(θ) dθ / ∫ L(θ) * p(θ) dθ\n",
        "        \"\"\"\n",
        "        # Prior: standard normal\n",
        "        prior = stats.norm.pdf(theta_grid, 0, prior_std)\n",
        "        \n",
        "        # Likelihood for each theta value\n",
        "        likelihood = np.ones(len(theta_grid))\n",
        "        for j, (a, b, x) in enumerate(zip(a_params, b_params, response_pattern)):\n",
        "            p = icc_2pl(theta_grid, a, b)\n",
        "            # L(x|θ) = p^x * (1-p)^(1-x)\n",
        "            if x == 1:\n",
        "                likelihood *= p\n",
        "            else:\n",
        "                likelihood *= (1 - p)\n",
        "        \n",
        "        # Posterior (unnormalized)\n",
        "        posterior = likelihood * prior\n",
        "        \n",
        "        # EAP estimate\n",
        "        normalizer = np.trapz(posterior, theta_grid)\n",
        "        if normalizer > 0:\n",
        "            theta_eap = np.trapz(theta_grid * posterior, theta_grid) / normalizer\n",
        "            # Posterior variance for SE\n",
        "            theta_var = np.trapz((theta_grid - theta_eap)**2 * posterior, theta_grid) / normalizer\n",
        "            theta_se = np.sqrt(theta_var)\n",
        "        else:\n",
        "            theta_eap = 0.0\n",
        "            theta_se = 1.0\n",
        "        \n",
        "        return theta_eap, theta_se\n",
        "    \n",
        "    # Prepare parameters\n",
        "    a_params = irt_params['discrimination_a'].values\n",
        "    b_params = irt_params['difficulty_b'].values\n",
        "    \n",
        "    # Use the filtered response data (same items as IRT model)\n",
        "    # Note: irt_data_clean is items x subjects, we need subjects x items\n",
        "    response_data = irt_data_clean.T  # Now subjects x items\n",
        "    \n",
        "    # Theta grid for integration\n",
        "    theta_grid = np.linspace(-5, 5, 101)\n",
        "    \n",
        "    print(f\"   Computing theta for {response_data.shape[0]} participants...\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPUTE THETA FOR ALL PARTICIPANTS\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    # Compute EAP theta for each participant\n",
        "    theta_estimates = []\n",
        "    theta_ses = []\n",
        "    \n",
        "    for i in tqdm(range(response_data.shape[0]), desc=\"   Computing EAP θ\"):\n",
        "        response_pattern = response_data[i, :]\n",
        "        theta, se = compute_eap_theta(response_pattern, a_params, b_params, theta_grid)\n",
        "        theta_estimates.append(theta)\n",
        "        theta_ses.append(se)\n",
        "    \n",
        "    theta_estimates = np.array(theta_estimates)\n",
        "    theta_ses = np.array(theta_ses)\n",
        "    \n",
        "    print(f\"\\n   Theta estimation complete!\")\n",
        "    \n",
        "    # Summary statistics\n",
        "    describe_distribution(theta_estimates, \"Theta (θ) Estimates\")\n",
        "    \n",
        "    # Compute reliability (ratio of true variance to total variance)\n",
        "    # Total variance = Var(θ_observed)\n",
        "    # Error variance = Mean(SE²)\n",
        "    total_var = np.var(theta_estimates)\n",
        "    error_var = np.mean(theta_ses ** 2)\n",
        "    reliability = (total_var - error_var) / total_var if total_var > error_var else 0\n",
        "    \n",
        "    print(f\"\\n   Reliability estimate (EAP): {reliability:.3f}\")\n",
        "    print(f\"   (Proportion of variance in θ estimates due to true ability)\")\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CREATE PARTICIPANT THETA SCORES TABLE\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n2. Creating participant theta scores table...\")\n",
        "    \n",
        "    # Create comprehensive scoring table\n",
        "    theta_scores = pd.DataFrame({\n",
        "        'participant_id': participant_ids,\n",
        "        'theta_irt': theta_estimates,\n",
        "        'theta_se': theta_ses,\n",
        "        'classical_score': classical_score.values,\n",
        "        'n_authors_endorsed': hits.values,\n",
        "        'n_foils_endorsed': false_alarms.values,\n",
        "        'error_rate': error_rate.values,\n",
        "        'source': source_col.values\n",
        "    })\n",
        "    \n",
        "    print(f\"   Created scoring table with {len(theta_scores)} participants\")\n",
        "    \n",
        "    # Show summary by source\n",
        "    print(\"\\n   Theta scores by data source:\")\n",
        "    source_summary = theta_scores.groupby('source').agg({\n",
        "        'theta_irt': ['mean', 'std', 'count'],\n",
        "        'classical_score': ['mean', 'std']\n",
        "    }).round(3)\n",
        "    print(source_summary)\n",
        "    \n",
        "    # Visualizations\n",
        "    print(\"\\n3. Creating theta distribution visualizations...\")\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=FIGSIZE_MEDIUM)\n",
        "    \n",
        "    # Theta distribution histogram\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.hist(theta_estimates, bins=30, edgecolor='black', alpha=0.7, color='steelblue')\n",
        "    ax1.axvline(theta_estimates.mean(), color='red', linestyle='--', \n",
        "                label=f'Mean: {theta_estimates.mean():.2f}')\n",
        "    ax1.set_xlabel('Theta (θ)')\n",
        "    ax1.set_ylabel('Number of Participants')\n",
        "    ax1.set_title('Distribution of IRT Ability Estimates')\n",
        "    ax1.legend()\n",
        "    \n",
        "    # Theta vs SE\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.scatter(theta_estimates, theta_ses, alpha=0.3, s=20)\n",
        "    ax2.set_xlabel('Theta (θ)')\n",
        "    ax2.set_ylabel('Standard Error')\n",
        "    ax2.set_title('Theta vs Standard Error')\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Theta by source\n",
        "    ax3 = axes[1, 0]\n",
        "    for src in theta_scores['source'].unique():\n",
        "        mask = theta_scores['source'] == src\n",
        "        ax3.hist(theta_scores.loc[mask, 'theta_irt'], bins=20, alpha=0.5, label=str(src)[:30])\n",
        "    ax3.set_xlabel('Theta (θ)')\n",
        "    ax3.set_ylabel('Number of Participants')\n",
        "    ax3.set_title('Theta Distribution by Data Source')\n",
        "    ax3.legend(fontsize=8)\n",
        "    \n",
        "    # QQ plot\n",
        "    ax4 = axes[1, 1]\n",
        "    stats.probplot(theta_estimates, dist=\"norm\", plot=ax4)\n",
        "    ax4.set_title('Q-Q Plot (Theta vs Normal)')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, 'step6', 'theta_distribution.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE STEP 6 OUTPUTS\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n4. Saving Step 6 outputs...\")\n",
        "    \n",
        "    # Save participant theta scores\n",
        "    save_table(theta_scores, 'step6', 'participant_theta_scores.csv', index=False)\n",
        "    \n",
        "    # Save theta summary statistics\n",
        "    theta_summary = pd.DataFrame({\n",
        "        'statistic': ['N', 'Mean', 'Std Dev', 'Min', 'Q1', 'Median', 'Q3', 'Max',\n",
        "                      'Mean SE', 'Reliability'],\n",
        "        'value': [\n",
        "            len(theta_estimates),\n",
        "            theta_estimates.mean(),\n",
        "            theta_estimates.std(),\n",
        "            theta_estimates.min(),\n",
        "            np.percentile(theta_estimates, 25),\n",
        "            np.median(theta_estimates),\n",
        "            np.percentile(theta_estimates, 75),\n",
        "            theta_estimates.max(),\n",
        "            theta_ses.mean(),\n",
        "            reliability\n",
        "        ]\n",
        "    })\n",
        "    save_table(theta_summary, 'step6', 'theta_summary_stats.csv', index=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" STEP 6 COMPLETE: Latent scores extracted\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n Key outputs:\")\n",
        "    print(f\"   - Theta estimates for {len(theta_estimates)} participants\")\n",
        "    print(f\"   - Mean θ: {theta_estimates.mean():.3f}\")\n",
        "    print(f\"   - Reliability: {reliability:.3f}\")\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 8: Scoring Methods Comparison (Step 7)\n",
        "\n",
        "Compare IRT-based theta scores with classical ART scores and error rates to assess convergent validity."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 7: SCORING METHODS COMPARISON\n",
        "# ============================================================\n",
        "print_section_header(\"STEP 7: SCORING METHODS COMPARISON\")\n",
        "\n",
        "if model_converged:\n",
        "    print(\"\\n1. Computing correlations between scoring methods...\")\n",
        "    \n",
        "    # Extract scores for correlation\n",
        "    theta_irt = theta_scores['theta_irt'].values\n",
        "    classical = theta_scores['classical_score'].values\n",
        "    err_rate = theta_scores['error_rate'].values\n",
        "    n_hits = theta_scores['n_authors_endorsed'].values\n",
        "    \n",
        "    # Compute correlations\n",
        "    corr_theta_classical = np.corrcoef(theta_irt, classical)[0, 1]\n",
        "    corr_theta_error = np.corrcoef(theta_irt, err_rate)[0, 1]\n",
        "    corr_classical_error = np.corrcoef(classical, err_rate)[0, 1]\n",
        "    corr_theta_hits = np.corrcoef(theta_irt, n_hits)[0, 1]\n",
        "    \n",
        "    print_stats_box(\"Scoring Method Correlations\", {\n",
        "        'θ_IRT vs Classical': corr_theta_classical,\n",
        "        'θ_IRT vs Error Rate': corr_theta_error,\n",
        "        'θ_IRT vs Hits': corr_theta_hits,\n",
        "        'Classical vs Error Rate': corr_classical_error\n",
        "    })\n",
        "    \n",
        "    # Compute R-squared values\n",
        "    print(f\"\\n   R² values (variance explained):\")\n",
        "    print(f\"      θ_IRT predicting Classical: {corr_theta_classical**2:.3f} ({corr_theta_classical**2*100:.1f}%)\")\n",
        "    print(f\"      θ_IRT predicting Hits: {corr_theta_hits**2:.3f} ({corr_theta_hits**2*100:.1f}%)\")\n",
        "    \n",
        "    # Statistical tests for correlations\n",
        "    from scipy.stats import pearsonr\n",
        "    r, p = pearsonr(theta_irt, classical)\n",
        "    print(f\"\\n   θ_IRT vs Classical correlation test:\")\n",
        "    print(f\"      r = {r:.4f}, p = {p:.2e}\")\n",
        "    print(f\"      Interpretation: {'Significant' if p < 0.05 else 'Not significant'} at α=0.05\")\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SCORING COMPARISON VISUALIZATIONS\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n2. Creating scoring comparison visualizations...\")\n",
        "    \n",
        "    fig, axes = plt.subplots(2, 2, figsize=FIGSIZE_LARGE)\n",
        "    \n",
        "    # Plot 1: Theta vs Classical Score\n",
        "    ax1 = axes[0, 0]\n",
        "    ax1.scatter(classical, theta_irt, alpha=0.3, s=20, c='steelblue')\n",
        "    # Add regression line\n",
        "    z = np.polyfit(classical, theta_irt, 1)\n",
        "    p_fit = np.poly1d(z)\n",
        "    x_line = np.linspace(classical.min(), classical.max(), 100)\n",
        "    ax1.plot(x_line, p_fit(x_line), 'r-', linewidth=2, \n",
        "             label=f'r = {corr_theta_classical:.3f}')\n",
        "    ax1.set_xlabel('Classical ART Score (Hits - False Alarms)')\n",
        "    ax1.set_ylabel('IRT Theta (θ)')\n",
        "    ax1.set_title('IRT θ vs Classical Score')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 2: Theta vs Error Rate\n",
        "    ax2 = axes[0, 1]\n",
        "    ax2.scatter(err_rate * 100, theta_irt, alpha=0.3, s=20, c='coral')\n",
        "    z = np.polyfit(err_rate, theta_irt, 1)\n",
        "    p_fit = np.poly1d(z)\n",
        "    x_line = np.linspace(0, err_rate.max(), 100)\n",
        "    ax2.plot(x_line * 100, p_fit(x_line), 'r-', linewidth=2,\n",
        "             label=f'r = {corr_theta_error:.3f}')\n",
        "    ax2.set_xlabel('Error Rate (%)')\n",
        "    ax2.set_ylabel('IRT Theta (θ)')\n",
        "    ax2.set_title('IRT θ vs Error Rate')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 3: Residual plot (Theta - predicted Theta from Classical)\n",
        "    ax3 = axes[1, 0]\n",
        "    predicted_theta = p_fit(classical)\n",
        "    residuals = theta_irt - np.polyval(np.polyfit(classical, theta_irt, 1), classical)\n",
        "    ax3.scatter(classical, residuals, alpha=0.3, s=20, c='green')\n",
        "    ax3.axhline(0, color='red', linestyle='--')\n",
        "    ax3.set_xlabel('Classical Score')\n",
        "    ax3.set_ylabel('Residual (θ_IRT - Predicted)')\n",
        "    ax3.set_title('Residuals: IRT vs Classical Prediction')\n",
        "    ax3.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Plot 4: Correlation matrix heatmap\n",
        "    ax4 = axes[1, 1]\n",
        "    score_data = pd.DataFrame({\n",
        "        'θ_IRT': theta_irt,\n",
        "        'Classical': classical,\n",
        "        'Hits': n_hits,\n",
        "        'Error%': err_rate * 100\n",
        "    })\n",
        "    corr_matrix_scores = score_data.corr()\n",
        "    sns.heatmap(corr_matrix_scores, annot=True, fmt='.3f', cmap='RdBu_r',\n",
        "                center=0, vmin=-1, vmax=1, ax=ax4)\n",
        "    ax4.set_title('Scoring Methods Correlation Matrix')\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, 'step7', 'scoring_comparison_matrix.png')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE STEP 7 OUTPUTS\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n3. Saving Step 7 outputs...\")\n",
        "    \n",
        "    # Save correlation table\n",
        "    corr_table = pd.DataFrame({\n",
        "        'comparison': ['θ_IRT vs Classical', 'θ_IRT vs Error Rate', \n",
        "                       'θ_IRT vs Hits', 'Classical vs Error Rate'],\n",
        "        'correlation': [corr_theta_classical, corr_theta_error, \n",
        "                        corr_theta_hits, corr_classical_error],\n",
        "        'r_squared': [corr_theta_classical**2, corr_theta_error**2,\n",
        "                      corr_theta_hits**2, corr_classical_error**2]\n",
        "    })\n",
        "    save_table(corr_table, 'step7', 'score_correlations.csv', index=False)\n",
        "    \n",
        "    # Save scoring methods summary\n",
        "    scoring_summary = pd.DataFrame({\n",
        "        'method': ['IRT Theta', 'Classical (Hits-FA)', 'Hits Only', 'Error Rate'],\n",
        "        'mean': [theta_irt.mean(), classical.mean(), n_hits.mean(), err_rate.mean()],\n",
        "        'std': [theta_irt.std(), classical.std(), n_hits.std(), err_rate.std()],\n",
        "        'min': [theta_irt.min(), classical.min(), n_hits.min(), err_rate.min()],\n",
        "        'max': [theta_irt.max(), classical.max(), n_hits.max(), err_rate.max()],\n",
        "        'range': [theta_irt.max() - theta_irt.min(), \n",
        "                  classical.max() - classical.min(),\n",
        "                  n_hits.max() - n_hits.min(),\n",
        "                  err_rate.max() - err_rate.min()]\n",
        "    })\n",
        "    save_table(scoring_summary, 'step7', 'scoring_methods_summary.csv', index=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" STEP 7 COMPLETE: Scoring comparison finished\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n Key findings:\")\n",
        "    print(f\"   - θ_IRT and Classical score correlation: {corr_theta_classical:.3f}\")\n",
        "    print(f\"   - θ_IRT explains {corr_theta_classical**2*100:.1f}% of Classical variance\")\n",
        "    print(f\"   - θ_IRT and Error Rate correlation: {corr_theta_error:.3f}\")\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 9: Short Scale Extraction (Step 8)\n",
        "\n",
        "Select a subset of items that maintain good psychometric properties while reducing test length."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 8: SHORT SCALE EXTRACTION\n",
        "# ============================================================\n",
        "print_section_header(\"STEP 8: SHORT SCALE EXTRACTION\")\n",
        "\n",
        "if model_converged:\n",
        "    print(\"\\n1. Selecting items for short scale...\")\n",
        "    print(\"\"\"\n",
        "    Selection criteria:\n",
        "    - Keep items with high discrimination (top 60-80%)\n",
        "    - Maintain coverage across difficulty range\n",
        "    - Remove items with floor/ceiling effects\n",
        "    \"\"\")\n",
        "    \n",
        "    # Calculate discrimination percentile threshold\n",
        "    disc_threshold_percentile = 40  # Keep top 60%\n",
        "    disc_threshold = np.percentile(irt_params['discrimination_a'], disc_threshold_percentile)\n",
        "    \n",
        "    print(f\"   Discrimination threshold (top 60%): a > {disc_threshold:.3f}\")\n",
        "    \n",
        "    # Select items meeting criteria\n",
        "    short_scale_mask = (\n",
        "        (irt_params['discrimination_a'] >= disc_threshold) &\n",
        "        (~irt_params['irt_flagged'])  # Exclude items with extreme difficulty\n",
        "    )\n",
        "    \n",
        "    short_scale_items = irt_params[short_scale_mask].copy()\n",
        "    n_short = len(short_scale_items)\n",
        "    \n",
        "    print(f\"   Items selected for short scale: {n_short} (from {len(irt_params)} total)\")\n",
        "    print(f\"   Items removed: {len(irt_params) - n_short}\")\n",
        "    \n",
        "    # Check difficulty coverage\n",
        "    print(\"\\n2. Checking difficulty coverage in short scale...\")\n",
        "    \n",
        "    difficulty_bins = [(-np.inf, -2), (-2, -1), (-1, 0), (0, 1), (1, 2), (2, np.inf)]\n",
        "    bin_labels = ['Very Easy', 'Easy', 'Medium-Easy', 'Medium-Hard', 'Hard', 'Very Hard']\n",
        "    \n",
        "    print(\"\\n   Difficulty coverage:\")\n",
        "    for (low, high), label in zip(difficulty_bins, bin_labels):\n",
        "        full_count = ((irt_params['difficulty_b'] >= low) & (irt_params['difficulty_b'] < high)).sum()\n",
        "        short_count = ((short_scale_items['difficulty_b'] >= low) & (short_scale_items['difficulty_b'] < high)).sum()\n",
        "        print(f\"      {label} (b ∈ [{low}, {high})): Full={full_count}, Short={short_count}\")\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# COMPARE FULL VS SHORT SCALE INFORMATION\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n3. Comparing test information: Full vs Short scale...\")\n",
        "    \n",
        "    # Compute short scale test information\n",
        "    short_info = np.zeros(len(theta_range))\n",
        "    for _, row in short_scale_items.iterrows():\n",
        "        short_info += item_information_2pl(theta_range, row['discrimination_a'], row['difficulty_b'])\n",
        "    \n",
        "    # Calculate information retained\n",
        "    info_retained_pct = (short_info.sum() / test_information.sum()) * 100\n",
        "    \n",
        "    # Find peak information locations\n",
        "    full_peak_theta = theta_range[np.argmax(test_information)]\n",
        "    short_peak_theta = theta_range[np.argmax(short_info)]\n",
        "    \n",
        "    print_stats_box(\"Information Comparison\", {\n",
        "        'Full scale items': len(irt_params),\n",
        "        'Short scale items': n_short,\n",
        "        'Items removed': len(irt_params) - n_short,\n",
        "        'Reduction %': f\"{(1 - n_short/len(irt_params))*100:.1f}%\",\n",
        "        'Info retained %': f\"{info_retained_pct:.1f}%\",\n",
        "        'Full peak θ': full_peak_theta,\n",
        "        'Short peak θ': short_peak_theta\n",
        "    })\n",
        "    \n",
        "    # Create comparison plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=FIGSIZE_MEDIUM)\n",
        "    \n",
        "    # TIF comparison\n",
        "    ax1 = axes[0]\n",
        "    ax1.plot(theta_range, test_information, 'b-', linewidth=2, label=f'Full Scale ({len(irt_params)} items)')\n",
        "    ax1.plot(theta_range, short_info, 'r--', linewidth=2, label=f'Short Scale ({n_short} items)')\n",
        "    ax1.fill_between(theta_range, short_info, alpha=0.3, color='coral')\n",
        "    ax1.set_xlabel('Ability (θ)')\n",
        "    ax1.set_ylabel('Test Information')\n",
        "    ax1.set_title('Test Information: Full vs Short Scale')\n",
        "    ax1.legend()\n",
        "    ax1.grid(True, alpha=0.3)\n",
        "    \n",
        "    # Efficiency plot (info per item)\n",
        "    ax2 = axes[1]\n",
        "    full_efficiency = test_information / len(irt_params)\n",
        "    short_efficiency = short_info / n_short\n",
        "    ax2.plot(theta_range, full_efficiency, 'b-', linewidth=2, label='Full Scale')\n",
        "    ax2.plot(theta_range, short_efficiency, 'r--', linewidth=2, label='Short Scale')\n",
        "    ax2.set_xlabel('Ability (θ)')\n",
        "    ax2.set_ylabel('Information per Item')\n",
        "    ax2.set_title('Measurement Efficiency (Info per Item)')\n",
        "    ax2.legend()\n",
        "    ax2.grid(True, alpha=0.3)\n",
        "    \n",
        "    plt.tight_layout()\n",
        "    save_plot(fig, 'step8', 'short_vs_full_tif.png')\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE STEP 8 OUTPUTS\n",
        "# ============================================================\n",
        "if model_converged:\n",
        "    print(\"\\n4. Saving Step 8 outputs...\")\n",
        "    \n",
        "    # Save short scale candidate items\n",
        "    short_scale_output = short_scale_items[['item', 'discrimination_a', 'difficulty_b', 'endorsement_rate']].copy()\n",
        "    short_scale_output['selected'] = True\n",
        "    save_table(short_scale_output, 'step8', 'short_scale_candidates.csv', index=False)\n",
        "    \n",
        "    # Save information comparison\n",
        "    info_comparison = pd.DataFrame({\n",
        "        'metric': ['N items', 'Total information', 'Peak information', 'Peak theta',\n",
        "                   'Info at θ=-2', 'Info at θ=0', 'Info at θ=+2'],\n",
        "        'full_scale': [\n",
        "            len(irt_params),\n",
        "            test_information.sum(),\n",
        "            test_information.max(),\n",
        "            full_peak_theta,\n",
        "            test_information[np.argmin(np.abs(theta_range - (-2)))],\n",
        "            test_information[np.argmin(np.abs(theta_range - 0))],\n",
        "            test_information[np.argmin(np.abs(theta_range - 2))]\n",
        "        ],\n",
        "        'short_scale': [\n",
        "            n_short,\n",
        "            short_info.sum(),\n",
        "            short_info.max(),\n",
        "            short_peak_theta,\n",
        "            short_info[np.argmin(np.abs(theta_range - (-2)))],\n",
        "            short_info[np.argmin(np.abs(theta_range - 0))],\n",
        "            short_info[np.argmin(np.abs(theta_range - 2))]\n",
        "        ]\n",
        "    })\n",
        "    save_table(info_comparison, 'step8', 'short_scale_info_comparison.csv', index=False)\n",
        "    \n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\" STEP 8 COMPLETE: Short scale extraction finished\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\n Key findings:\")\n",
        "    print(f\"   - Short scale: {n_short} items (reduced from {len(irt_params)})\")\n",
        "    print(f\"   - Information retained: {info_retained_pct:.1f}%\")\n",
        "    print(f\"   - Recommended for: Initial screening, time-limited assessment\")\n",
        "else:\n",
        "    print(\"   Skipping - IRT model did not converge\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Section 10: Summary Report (Step 9)\n",
        "\n",
        "Generate a comprehensive analysis report documenting all findings, decisions, and recommendations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# STEP 9: GENERATE ANALYSIS SUMMARY REPORT\n",
        "# ============================================================\n",
        "print_section_header(\"STEP 9: ANALYSIS SUMMARY REPORT\")\n",
        "\n",
        "print(\"\\n1. Generating comprehensive analysis report...\")\n",
        "\n",
        "# Create markdown report\n",
        "report_content = f\"\"\"# IRT Psychometric Analysis Report\n",
        "## Russian Author Recognition Test (ART) Pretest\n",
        "\n",
        "**Analysis Date:** {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Data Summary\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Total Participants | {len(participant_ids)} |\n",
        "| Real Author Items | {len(real_author_indices)} |\n",
        "| Foil Items | {len(foil_indices)} |\n",
        "| Data Sources | {len(source_counts)} |\n",
        "\n",
        "### Data Quality\n",
        "- Missing data columns: {(missing_count > 0).sum()}\n",
        "- Total missing cells: {item_data.isnull().sum().sum()}\n",
        "- Items with floor effect (<5%): {n_floor}\n",
        "- Items with ceiling effect (>95%): {n_ceiling}\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Dimensionality Assessment\n",
        "\n",
        "| Metric | Value |\n",
        "|--------|-------|\n",
        "| Items in EFA | {len(usable_items)} |\n",
        "| KMO Measure | {kmo_model:.4f if 'kmo_model' in dir() and not pd.isna(kmo_model) else 'N/A'} |\n",
        "| Bartlett's Chi-square | {chi_square:.2f if 'chi_square' in dir() and not pd.isna(chi_square) else 'N/A'} |\n",
        "| Eigenvalue Ratio (1st/2nd) | {ratio_1_2:.2f} |\n",
        "| Parallel Analysis Factors | {n_factors_parallel} |\n",
        "\n",
        "**Decision:** {'Unidimensional' if is_unidimensional else 'Potentially multidimensional'} - proceeding with unidimensional IRT\n",
        "\n",
        "---\n",
        "\n",
        "## 3. IRT Model Results\n",
        "\n",
        "| Parameter | Mean | SD | Min | Max |\n",
        "|-----------|------|-----|-----|-----|\n",
        "| Discrimination (a) | {irt_params['discrimination_a'].mean():.3f if model_converged else 'N/A'} | {irt_params['discrimination_a'].std():.3f if model_converged else 'N/A'} | {irt_params['discrimination_a'].min():.3f if model_converged else 'N/A'} | {irt_params['discrimination_a'].max():.3f if model_converged else 'N/A'} |\n",
        "| Difficulty (b) | {irt_params['difficulty_b'].mean():.3f if model_converged else 'N/A'} | {irt_params['difficulty_b'].std():.3f if model_converged else 'N/A'} | {irt_params['difficulty_b'].min():.3f if model_converged else 'N/A'} | {irt_params['difficulty_b'].max():.3f if model_converged else 'N/A'} |\n",
        "\n",
        "### Item Quality Flags\n",
        "- Low discrimination (a < {DISCRIMINATION_LOW_THRESHOLD}): {n_low_disc if model_converged else 'N/A'}\n",
        "- Extreme difficulty (|b| > {DIFFICULTY_EXTREME_THRESHOLD}): {n_extreme_diff if model_converged else 'N/A'}\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Theta Scores\n",
        "\n",
        "| Statistic | Value |\n",
        "|-----------|-------|\n",
        "| Mean θ | {theta_estimates.mean():.3f if model_converged else 'N/A'} |\n",
        "| SD θ | {theta_estimates.std():.3f if model_converged else 'N/A'} |\n",
        "| Range | [{theta_estimates.min():.3f if model_converged else 'N/A'}, {theta_estimates.max():.3f if model_converged else 'N/A'}] |\n",
        "| Reliability | {reliability:.3f if model_converged else 'N/A'} |\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Scoring Method Comparison\n",
        "\n",
        "| Comparison | Correlation | R² |\n",
        "|------------|-------------|-----|\n",
        "| θ_IRT vs Classical | {corr_theta_classical:.3f if model_converged else 'N/A'} | {corr_theta_classical**2:.3f if model_converged else 'N/A'} |\n",
        "| θ_IRT vs Error Rate | {corr_theta_error:.3f if model_converged else 'N/A'} | {corr_theta_error**2:.3f if model_converged else 'N/A'} |\n",
        "| θ_IRT vs Hits | {corr_theta_hits:.3f if model_converged else 'N/A'} | {corr_theta_hits**2:.3f if model_converged else 'N/A'} |\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Short Scale\n",
        "\n",
        "| Metric | Full Scale | Short Scale |\n",
        "|--------|------------|-------------|\n",
        "| N Items | {len(irt_params) if model_converged else 'N/A'} | {n_short if model_converged else 'N/A'} |\n",
        "| Information Retained | 100% | {info_retained_pct:.1f}% |\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Output Files\n",
        "\n",
        "### Step 0: Data Prep\n",
        "- `author_response_matrix.csv` - Binary response matrix for real authors\n",
        "- `foil_response_matrix.csv` - Binary response matrix for foils\n",
        "- `participant_summary.csv` - Participant-level summary\n",
        "- `data_quality_report.csv` - Data quality metrics\n",
        "\n",
        "### Step 1: Item Descriptives\n",
        "- `item_endorsement_rates.csv` - Endorsement rates for all items\n",
        "- `flagged_items.csv` - Items with floor/ceiling effects\n",
        "\n",
        "### Step 2: Dimensionality\n",
        "- `eigenvalues.csv` - Eigenvalue decomposition\n",
        "- `factor_loadings.csv` - Factor loadings (1 and 2 factor)\n",
        "- `model_fit_comparison.csv` - Model fit statistics\n",
        "\n",
        "### Step 3: IRT Model\n",
        "- `item_parameters.csv` - Estimated a and b parameters\n",
        "- `model_convergence.csv` - Convergence diagnostics\n",
        "\n",
        "### Step 4: Item Analysis\n",
        "- `item_parameter_table.csv` - Full item analysis\n",
        "- `item_quality_flags.csv` - Flagged items\n",
        "\n",
        "### Step 6: Theta Scores\n",
        "- `participant_theta_scores.csv` - All participant scores\n",
        "- `theta_summary_stats.csv` - Summary statistics\n",
        "\n",
        "### Step 7: Scoring Comparison\n",
        "- `score_correlations.csv` - Correlation matrix\n",
        "- `scoring_methods_summary.csv` - Method summaries\n",
        "\n",
        "### Step 8: Short Scale\n",
        "- `short_scale_candidates.csv` - Selected items\n",
        "- `short_scale_info_comparison.csv` - Information comparison\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Recommendations\n",
        "\n",
        "1. **For full assessment:** Use all {len(irt_params) if model_converged else 'N/A'} real author items\n",
        "2. **For screening:** Consider the {n_short if model_converged else 'N/A'}-item short scale\n",
        "3. **Items to review:** {n_irt_flagged if model_converged else 'N/A'} items flagged for low discrimination or extreme difficulty\n",
        "4. **Scoring:** IRT θ scores recommended for research; classical scores acceptable for quick screening\n",
        "\n",
        "---\n",
        "\n",
        "*Report generated by IRT Analysis Notebook*\n",
        "\"\"\"\n",
        "\n",
        "print(\"   Report content generated.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# SAVE FINAL REPORT AND CONSOLIDATED OUTPUTS\n",
        "# ============================================================\n",
        "print(\"\\n2. Saving final outputs...\")\n",
        "\n",
        "# Save markdown report\n",
        "report_path = OUTPUT_DIRS['step9'] / 'analysis_summary.md'\n",
        "with open(report_path, 'w') as f:\n",
        "    f.write(report_content)\n",
        "print(f\"  Saved report: {report_path.name}\")\n",
        "\n",
        "# Create consolidated results summary\n",
        "if model_converged:\n",
        "    full_results = pd.DataFrame({\n",
        "        'category': [\n",
        "            'Data', 'Data', 'Data', 'Data',\n",
        "            'Dimensionality', 'Dimensionality', 'Dimensionality',\n",
        "            'IRT', 'IRT', 'IRT', 'IRT', 'IRT',\n",
        "            'Theta', 'Theta', 'Theta',\n",
        "            'Validity', 'Validity',\n",
        "            'Short Scale', 'Short Scale'\n",
        "        ],\n",
        "        'metric': [\n",
        "            'Total participants', 'Real author items', 'Foil items', 'Items flagged (endorsement)',\n",
        "            'KMO', 'Eigenvalue ratio', 'Parallel analysis factors',\n",
        "            'Mean discrimination', 'Mean difficulty', 'Low discrimination items', 'Extreme difficulty items', 'Reliability',\n",
        "            'Mean theta', 'SD theta', 'Theta range',\n",
        "            'Theta-Classical r', 'Theta-Error r',\n",
        "            'Short scale items', 'Info retained %'\n",
        "        ],\n",
        "        'value': [\n",
        "            len(participant_ids), len(real_author_indices), len(foil_indices), n_flagged,\n",
        "            round(kmo_model, 4) if 'kmo_model' in dir() and not pd.isna(kmo_model) else None, \n",
        "            round(ratio_1_2, 2), n_factors_parallel,\n",
        "            round(irt_params['discrimination_a'].mean(), 3), \n",
        "            round(irt_params['difficulty_b'].mean(), 3), \n",
        "            n_low_disc, n_extreme_diff, round(reliability, 3),\n",
        "            round(theta_estimates.mean(), 3), round(theta_estimates.std(), 3),\n",
        "            f\"[{theta_estimates.min():.2f}, {theta_estimates.max():.2f}]\",\n",
        "            round(corr_theta_classical, 3), round(corr_theta_error, 3),\n",
        "            n_short, round(info_retained_pct, 1)\n",
        "        ]\n",
        "    })\n",
        "    save_table(full_results, 'step9', 'full_results_summary.csv', index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\" ANALYSIS COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\"\"\n",
        "Summary of deliverables:\n",
        "  - Cleaned participant × item matrix: results/step0_data_prep/tables/\n",
        "  - Factor analysis results: results/step2_dimensionality/tables/\n",
        "  - Item parameters (a, b): results/step3_irt_model/tables/\n",
        "  - ICC and TIF plots: results/step5_diagnostics/plots/\n",
        "  - Participant θ scores: results/step6_theta_scores/tables/\n",
        "  - Scoring comparisons: results/step7_scoring_comparison/tables/\n",
        "  - Short scale candidates: results/step8_short_scale/tables/\n",
        "  - Full report: results/step9_report/analysis_summary.md\n",
        "\n",
        "Analysis timestamp: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Analysis Complete\n",
        "\n",
        "This notebook has completed the full IRT psychometric analysis pipeline:\n",
        "\n",
        "1. **Data Prep** - Loaded and cleaned the merged ART pretest data\n",
        "2. **Item Descriptives** - Computed endorsement rates, identified floor/ceiling effects\n",
        "3. **Dimensionality** - Tested unidimensionality via EFA and parallel analysis\n",
        "4. **2PL IRT Model** - Estimated discrimination and difficulty parameters\n",
        "5. **Item Analysis** - Identified problematic items, analyzed parameter distributions\n",
        "6. **Visual Diagnostics** - Created ICC and TIF plots\n",
        "7. **Theta Extraction** - Computed EAP ability estimates for all participants\n",
        "8. **Scoring Comparison** - Compared IRT vs classical scores\n",
        "9. **Short Scale** - Selected items for an efficient short form\n",
        "10. **Report** - Generated comprehensive documentation\n",
        "\n",
        "All outputs are saved in the `results/` directory with organized subfolders for each step."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
