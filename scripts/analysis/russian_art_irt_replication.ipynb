{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "7448c083",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PROJECT_ROOT: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor\n",
            "CLEANED_CSV: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/art_cleaned/ART_pretest_merged_EN_cleaned.csv\n",
            "ITEM_META_CSV: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/art_cleaned/item_metadata.csv\n",
            "OUTPUT_DIR: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/irt_art_results\n",
            "\n",
            "=== Parsed dataset summary ===\n",
            "Rows incl. headers     : 1837\n",
            "Participants (N)       : 1835\n",
            "Total columns          : 211\n",
            "Demographic columns    : ['Submited', 'age', 'sex ', 'humanities or not', 'education and profession']\n",
            "Source column          : source\n",
            "Item columns           : 205\n",
            "Authors (metadata)     : 102\n",
            "Foils (metadata)       : 103\n",
            "Item labels aligned    : True\n",
            "Source waves found     : ['ART_prestest_responses', 'pretest_EN']\n",
            "Genre tags found       : ['cla', 'det', 'fan', 'fill', 'mod', 'rom', 'sci', 'sfi', 'soc']\n",
            "\n",
            "First 8 author items:\n",
            " item_index             item_label item_code genre\n",
            "          1        Khaled Hosseini      mod1   mod\n",
            "          2            Donna Tartt      mod2   mod\n",
            "          4       Archibald Cronin      cla1   cla\n",
            "          5          Gillian Flynn      det1   det\n",
            "         11 Gabriel Garsia Marquez      cla2   cla\n",
            "         12  James Fenimore Cooper      cla3   cla\n",
            "         13     Henryk Sienkiewicz      cla4   cla\n",
            "         15          Paula Hawkins      det2   det\n",
            "\n",
            "First 8 foil items:\n",
            " item_index             item_label item_code genre\n",
            "          0 Gerrit HoogenbuM fill1     fill1  fill\n",
            "          3   Gonzalo Hervas fill2     fill2  fill\n",
            "          6    Richard Gould fill3     fill3  fill\n",
            "          7     Brian Callis fill4     fill4  fill\n",
            "          8      Petr Sabluk fill5     fill5  fill\n",
            "          9      Neil Bourke fill6     fill6  fill\n",
            "         10   Chris Reynolds fill7     fill7  fill\n",
            "         14     Paul Merwick fill8     fill8  fill\n"
          ]
        }
      ],
      "source": [
        "# Cell 1 — Load cleaned data + metadata; identify authors and foils\n",
        "\n",
        "import os\n",
        "import re\n",
        "import warnings\n",
        "from pathlib import Path\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "PROJECT_ROOT = Path(\"../..\").resolve()\n",
        "CLEANED_CSV = PROJECT_ROOT / \"data\" / \"processed\" / \"art_cleaned\" / \"ART_pretest_merged_EN_cleaned.csv\"\n",
        "ITEM_META_CSV = PROJECT_ROOT / \"data\" / \"processed\" / \"art_cleaned\" / \"item_metadata.csv\"\n",
        "OUTPUT_DIR = PROJECT_ROOT / \"data\" / \"processed\" / \"irt_art_results\"\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "print(f\"PROJECT_ROOT: {PROJECT_ROOT}\")\n",
        "print(f\"CLEANED_CSV: {CLEANED_CSV}\")\n",
        "print(f\"ITEM_META_CSV: {ITEM_META_CSV}\")\n",
        "print(f\"OUTPUT_DIR: {OUTPUT_DIR}\\n\")\n",
        "\n",
        "raw = pd.read_csv(CLEANED_CSV, header=None, dtype=str)\n",
        "labels = raw.iloc[0].tolist()\n",
        "codes = raw.iloc[1].tolist()\n",
        "data = raw.iloc[2:].reset_index(drop=True).copy()\n",
        "data.columns = labels\n",
        "\n",
        "meta = pd.read_csv(ITEM_META_CSV)\n",
        "\n",
        "# Structure from prep notebook/cleaning decisions:\n",
        "# first 5 cols = demographics, then 205 item cols, last col = source\n",
        "demo_cols = labels[:5]\n",
        "source_col = labels[-1]\n",
        "item_labels = labels[5:-1]\n",
        "item_codes = codes[5:-1]\n",
        "\n",
        "if len(meta) != len(item_labels):\n",
        "    raise ValueError(f\"Metadata rows ({len(meta)}) != item columns ({len(item_labels)})\")\n",
        "\n",
        "if list(meta[\"item_label\"].astype(str)) != [str(x) for x in item_labels]:\n",
        "    raise ValueError(\"item_metadata item_label order does not match cleaned CSV item columns\")\n",
        "\n",
        "# Work on item block by position to avoid duplicate-label ambiguity.\n",
        "item_block = data.iloc[:, 5:-1].copy()\n",
        "item_block.columns = range(item_block.shape[1])\n",
        "item_block = item_block.apply(pd.to_numeric, errors=\"coerce\").fillna(0).astype(int)\n",
        "\n",
        "# Robust genre extraction from code (e.g., cla12 -> cla, fill93 -> fill)\n",
        "def extract_genre(code: str) -> str:\n",
        "    s = re.sub(r\"\\s+\", \"\", str(code).lower())\n",
        "    m = re.match(r\"([a-z]+)\", s)\n",
        "    return m.group(1) if m else \"unknown\"\n",
        "\n",
        "meta = meta.copy()\n",
        "meta[\"genre\"] = meta[\"item_code\"].astype(str).apply(extract_genre)\n",
        "\n",
        "author_meta = meta[meta[\"is_real_author\"]].copy()\n",
        "foil_meta = meta[meta[\"is_foil\"]].copy()\n",
        "author_idx = author_meta[\"item_index\"].astype(int).tolist()\n",
        "foil_idx = foil_meta[\"item_index\"].astype(int).tolist()\n",
        "\n",
        "N = len(data)\n",
        "print(\"=== Parsed dataset summary ===\")\n",
        "print(f\"Rows incl. headers     : {raw.shape[0]}\")\n",
        "print(f\"Participants (N)       : {N}\")\n",
        "print(f\"Total columns          : {len(labels)}\")\n",
        "print(f\"Demographic columns    : {demo_cols}\")\n",
        "print(f\"Source column          : {source_col}\")\n",
        "print(f\"Item columns           : {len(item_labels)}\")\n",
        "print(f\"Authors (metadata)     : {len(author_idx)}\")\n",
        "print(f\"Foils (metadata)       : {len(foil_idx)}\")\n",
        "print(f\"Item labels aligned    : True\")\n",
        "print(f\"Source waves found     : {sorted(data[source_col].astype(str).dropna().unique().tolist())}\")\n",
        "print(f\"Genre tags found       : {sorted(meta['genre'].unique().tolist())}\")\n",
        "print(\"\\nFirst 8 author items:\")\n",
        "print(author_meta[[\"item_index\", \"item_label\", \"item_code\", \"genre\"]].head(8).to_string(index=False))\n",
        "print(\"\\nFirst 8 foil items:\")\n",
        "print(foil_meta[[\"item_index\", \"item_label\", \"item_code\", \"genre\"]].head(8).to_string(index=False))\n",
        "\n",
        "# Convenience maps used by later cells\n",
        "meta_by_idx = meta.set_index(\"item_index\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "d5dac541",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "TABLE 1 (initial) — Score summary\n",
            "================================================================================\n",
            "                                     Scale  N items    N     M   SD  Min   Max  Range  Skew\n",
            "Full 102-author scale — Standard ART score      102 1835 39.08 20.7 -8.0  99.0  107.0  0.48\n",
            "        Full 102-author scale — Name score      102 1835 47.72 20.1  1.0 101.0  100.0  0.12\n",
            "\n",
            "Saved: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/irt_art_results/Table_1_initial.csv\n",
            "\n",
            "================================================================================\n",
            "TABLE 2 (pre-IRT) — Author selection rates\n",
            "================================================================================\n",
            "      item_index             Author Name Item Code Genre  Percent Selected\n",
            "Rank                                                                      \n",
            "1             40             Jack London      cla8   cla              97.4\n",
            "2             85         Agatha Christie      det4   det              97.2\n",
            "3             59      Arthur Conan Doyle     cla10   cla              97.1\n",
            "4             84         Alexandre Dumas     cla14   cla              95.9\n",
            "5            104            Ray Bradbury      sci2   sci              95.6\n",
            "6             63       John R.R. Tolkien      fan3   fan              95.3\n",
            "7             27         Charles Dickens      cla7   cla              95.1\n",
            "8             82     Eric Maria Remarque     cla13   cla              94.1\n",
            "9            105         Haruki Murakami     mod13   mod              90.6\n",
            "10            16           George Orwell      sci1   sci              90.1\n",
            "11            17             Jane Austen      cla6   cla              85.1\n",
            "12            53            Isaac Asimov      sfi2   sfi              83.7\n",
            "13            30       Sergey Lukyanenko      fan1   fan              82.1\n",
            "14            54      George R.R. Martin      fan2   fan              78.1\n",
            "15           100        Tatiana Ustinova      det6   det              76.0\n",
            "16            11  Gabriel Garsia Marquez      cla2   cla              75.7\n",
            "17           175            Yury  Olesha     soc12   soc              74.9\n",
            "18            81        Charlotte Bronte     cla12   cla              73.4\n",
            "19           186             Isaac Babel     soc14   soc              71.0\n",
            "20           112           Aldous Huxley      sci3   sci              69.2\n",
            "\n",
            "Saved: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/irt_art_results/Table_2_selection_rates.csv\n",
            "Mean author selection rate: 46.79%\n",
            "Easiest author: Jack London (97.4%)\n",
            "Hardest author: Yuri Tsypkin (2.8%)\n",
            "\n",
            "================================================================================\n",
            "TABLE 3 — False-alarm (foil selection) distribution\n",
            "================================================================================\n",
            " item_index                            Foil Name Item Code Genre  Percent Selected\n",
            "        167               Christian Lowe fill 85    fill85  fill              42.1\n",
            "        171               Margaret Swing fill 87    fill87  fill              40.9\n",
            "        172              Julian Weissman fill 89    fill89  fill              34.9\n",
            "        203               Sergey Siptits fill 99    fill99  fill              32.0\n",
            "        125              Vladimir Bautin fill 57    fill57  fill              31.5\n",
            "        113             German Sadulayev fill 52    fill52  fill              30.7\n",
            "        162         Stanistlav Sidorchuk fill 84    fill84  fill              30.4\n",
            "        138              Dmitry Parkmali fill 65    fill65  fill              29.5\n",
            "        121             Raghavena Bhatta fill 56    fill56  fill              28.4\n",
            "        204         Valentina Shirokova fill 100   fill100  fill              27.7\n",
            "        182                 Jeremy Watts fill 90    fill90  fill              25.7\n",
            "        147                Derek Gottman fill 73    fill74  fill              23.9\n",
            "        120                David Gladsky fill 55    fill55  fill              23.1\n",
            "        149                 Eric Ravelli fill 75    fill75  fill              23.0\n",
            "        157              Javier Giraldes fill 80    fill80  fill              22.5\n",
            "         83                Paul Williams fill 37    fill37  fill              22.1\n",
            "         49               Valentin Levin fill 24    fill24  fill              19.8\n",
            "        161            Yakushkina Gilyan fill 83    fill83  fill              19.3\n",
            "         32           Vladimir Pozdnyakov fill13    fill13  fill              16.6\n",
            "         35              Alexander Fomin fill 15    fill15  fill              12.4\n",
            "         25           Valentin Vershinin fill 14    fill14  fill              10.1\n",
            "         77               Alexey Gordeev fill 34    fill34  fill              10.1\n",
            "        184               Anthony Novitz fill 91    fill91  fill               9.3\n",
            "         79                Nigel Woodson fill 35    fill35  fill               8.7\n",
            "         48             Georgy Savchenko fil l23    fill23  fill               8.6\n",
            "         14                   Paul Merwick fill8     fill8  fill               8.4\n",
            "         60                Debra Winslow fill 27    fill27  fill               8.2\n",
            "         88                Margaret Nash fill 40    fill40  fill               8.2\n",
            "         51            Edwin Mac Gregory fill 25    fill25  fill               8.1\n",
            "        169            Gennady Proskurin fill 86    fill86  fill               8.0\n",
            "         56                 Davis Carter fill 26    fill26  fill               7.7\n",
            "        159            Elizabeth Harlett fill 82    fill82  fill               7.7\n",
            "          6                  Richard Gould fill3     fill3  fill               7.4\n",
            "         99               Stanley Fowler fill 47    fill47  fill               7.4\n",
            "         28               Alexander Rosin fill12    fill12  fill               7.3\n",
            "         37             Michael D. Kasler fill16    fill16  fill               7.2\n",
            "         10                 Chris Reynolds fill7     fill7  fill               6.5\n",
            "         62                Randy Dinkins fill 29    fill29  fill               6.4\n",
            "        148                   Luis Silva fill 74    fill74  fill               6.4\n",
            "        130                  Max Mandler fill 60    fill60  fill               6.2\n",
            "        155              Leonid Golovnin fill 78    fill78  fill               6.2\n",
            "        156              Shabtai Bittman fill 79    fill79  fill               5.9\n",
            "         34              Mark Kravchenko fill 14    fill14  fill               5.6\n",
            "        151                   Peter Lund fill 76    fill76  fill               5.2\n",
            "         20           Vladimir Sidorenko fill 10    fill10  fill               5.2\n",
            "        115               Diego Rubiales fill 53   fill 53  fill               5.0\n",
            "         93            Gennady Romanenko fill 44    fill44  fill               4.6\n",
            "         24                 Jan Dijkstra fill 13    fill13  fill               4.5\n",
            "        142                 Victor Mints fill 69    fill68  fill               4.5\n",
            "        139             Joseph Eitzinger fill 66    fill66  fill               4.3\n",
            "          9                    Neil Bourke fill6     fill6  fill               4.1\n",
            "        134 Salvattore J.-A.S. Ceccareli fill 62    fill62  fill               4.1\n",
            "        145                 Glenn Marion fill 71    fill71  fill               4.1\n",
            "        131              Evgenia Udalova fill 61    fill61  fill               4.1\n",
            "         47                  Mikhail Ron fil l22    fill22  fill               3.9\n",
            "        143                 Walter Coles fill 69    fill69  fill               3.8\n",
            "         23                   Xiulian Jin fill12    fill12  fill               3.7\n",
            "         87               Igor Zakharyev fill 39    fill39  fill               3.6\n",
            "        140              Rimma Ilyinskaya fill67    fill67  fill               3.5\n",
            "        127            Lawrence Kinsdale fill 58    fill58  fill               3.5\n",
            "        200                 Holger Magel fill 97    fill97  fill               3.4\n",
            "        196                 Andrea Segre fill 93    fill93  fill               3.1\n",
            "        201                  Vasily Uzun fill 98    fill98  fill               3.1\n",
            "        144                  Alison Sage fill 70    fill70  fill               3.1\n",
            "         94        Veniamin Khoroshevsky fill 45    fill45  fill               2.9\n",
            "        103              Roland Heyninen fill 49    fill49  fill               2.9\n",
            "         89            Varvara Gordienko fill 41    fill41  fill               2.8\n",
            "         75         Ekaterina Mikhailina fill 33    fill33  fill               2.7\n",
            "        137                  Diana Raven fill 64    fill64  fill               2.7\n",
            "         22         Stanislav Stepanovich fill11    fill11  fill               2.7\n",
            "         41                Anatoly Agarin fill18    fill18  fill               2.5\n",
            "          3                 Gonzalo Hervas fill2     fill2  fill               2.5\n",
            "         61                Martha Schave fill 28    fill28  fill               2.5\n",
            "        101             Gonzalo K. Hijar fill 48    fill48  fill               2.5\n",
            "        146             Lawrence Shallot fill 72    fill72  fill               2.5\n",
            "        199                Ivan Ushachev fill 96    fill96  fill               2.4\n",
            "        198                Ivan Buzdalov fill 95    fill95  fill               2.2\n",
            "         44            Alexander Hristov fill 20    fill20  fill               2.2\n",
            "         74                 Daniil Zubin fill 32    fill32  fill               2.2\n",
            "        109           Alexander Petrikov fill 50    fill50  fill               2.1\n",
            "          7                   Brian Callis fill4     fill4  fill               2.0\n",
            "        129              Dmitry Malovets fill 59    fill59  fill               2.0\n",
            "        195              Victor Khlystun fill 92    fill92  fill               1.8\n",
            "        119                Linda Abelson fill 54    fill54  fill               1.8\n",
            "        154             Simone Orlandini fill 77    fill77  fill               1.8\n",
            "        132                  Owen Fenton fill 62    fill62  fill               1.8\n",
            "         70                Terry Aveling fill 30    fill30  fill               1.7\n",
            "         38              Hans-Peter Piefo fill17    fill17  fill               1.4\n",
            "         90                   Ajay Kohli fill 42    fill42  fill               1.3\n",
            "         71                   Wenxue Wei fill 31    fill31  fill               1.2\n",
            "        110           Nikolay Dolgushkin fill 51    fill51  fill               1.2\n",
            "          8                    Petr Sabluk fill5     fill5  fill               1.1\n",
            "        197             Natalya Shagaida fill 94    fill94  fill               1.0\n",
            "          0               Gerrit HoogenbuM fill1     fill1  fill               0.9\n",
            "         46                  Ed Charmley fil l21    fill21  fill               0.9\n",
            "         97                  Malau Aduli fill 46    fill46  fill               0.9\n",
            "        158          Sokratis Stergiadis fill 81    fill81  fill               0.9\n",
            "         86         Adibe L. M. Abdallah fill 38    fill38  fill               0.9\n",
            "         19                Secundino Lopez fill9     fill9  fill               0.7\n",
            "        135               Kerstin Panten fill 63    fill63  fill               0.7\n",
            "         91                   Fangsen Xu fill 43    fill43  fill               0.7\n",
            "         80             Dandeniya Varshi fill 36    fill36  fill               0.6\n",
            "         43             Tatyana Kazennova fill19    fill19  fill               0.5\n",
            "\n",
            "Saved: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/irt_art_results/Table_3.csv\n",
            "Mean false alarms per participant: 8.64 (SD=9.08)\n",
            "Participants with 0 false alarms: 223 (12.2%)\n",
            "\n",
            "False-alarm count distribution (errors per participant):\n",
            " Errors  N participants  % of sample\n",
            "      0             223         12.2\n",
            "      1             232         12.6\n",
            "      2             157          8.6\n",
            "      3             112          6.1\n",
            "      4              64          3.5\n",
            "      5              54          2.9\n",
            "      6              89          4.9\n",
            "      7              61          3.3\n",
            "      8              78          4.3\n",
            "      9              66          3.6\n",
            "     10              57          3.1\n",
            "     11              58          3.2\n",
            "     12              60          3.3\n",
            "     13              50          2.7\n",
            "     14              62          3.4\n",
            "     15              68          3.7\n",
            "     16              63          3.4\n",
            "     17              50          2.7\n",
            "     18              41          2.2\n",
            "     19              29          1.6\n",
            "     20              21          1.1\n",
            "     21              25          1.4\n",
            "     22              12          0.7\n",
            "     23              10          0.5\n",
            "     24               9          0.5\n",
            "     25              11          0.6\n",
            "     26               8          0.4\n",
            "     27               4          0.2\n",
            "     28               6          0.3\n",
            "     29               6          0.3\n",
            "     30               5          0.3\n",
            "     31               4          0.2\n",
            "     32               2          0.1\n",
            "     33               2          0.1\n",
            "     34               7          0.4\n",
            "     35               3          0.2\n",
            "     36               2          0.1\n",
            "     37               1          0.1\n",
            "     39               2          0.1\n",
            "     41               1          0.1\n",
            "     42               2          0.1\n",
            "     43               1          0.1\n",
            "     45               3          0.2\n",
            "     46               1          0.1\n",
            "     47               1          0.1\n",
            "     53               1          0.1\n",
            "     54               1          0.1\n",
            "     56               1          0.1\n",
            "     57               1          0.1\n",
            "     58               1          0.1\n",
            "     59               1          0.1\n",
            "     61               1          0.1\n",
            "     63               1          0.1\n",
            "     67               1          0.1\n",
            "     69               1          0.1\n",
            "     72               1          0.1\n",
            "     74               1          0.1\n"
          ]
        }
      ],
      "source": [
        "# Cell 2 — Step 1: Descriptive statistics (Tables 1–3)\n",
        "\n",
        "# Per-participant scores\n",
        "hits = item_block.iloc[:, author_idx].sum(axis=1)\n",
        "false_alarms = item_block.iloc[:, foil_idx].sum(axis=1)\n",
        "\n",
        "data[\"hits\"] = hits\n",
        "data[\"false_alarms\"] = false_alarms\n",
        "data[\"standard_ART\"] = data[\"hits\"] - data[\"false_alarms\"]\n",
        "data[\"name_score\"] = data[\"hits\"]\n",
        "\n",
        "# --- Table 1 (initial) ---\n",
        "table1_rows = []\n",
        "for label, col, nitems in [\n",
        "    (f\"Full {len(author_idx)}-author scale — Standard ART score\", \"standard_ART\", len(author_idx)),\n",
        "    (f\"Full {len(author_idx)}-author scale — Name score\", \"name_score\", len(author_idx)),\n",
        "]:\n",
        "    s = data[col]\n",
        "    table1_rows.append({\n",
        "        \"Scale\": label,\n",
        "        \"N items\": nitems,\n",
        "        \"N\": N,\n",
        "        \"M\": round(s.mean(), 2),\n",
        "        \"SD\": round(s.std(), 2),\n",
        "        \"Min\": float(s.min()),\n",
        "        \"Max\": float(s.max()),\n",
        "        \"Range\": float(s.max() - s.min()),\n",
        "        \"Skew\": round(s.skew(), 2),\n",
        "    })\n",
        "\n",
        "table1 = pd.DataFrame(table1_rows)\n",
        "table1_path = OUTPUT_DIR / \"Table_1_initial.csv\"\n",
        "table1.to_csv(table1_path, index=False)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"TABLE 1 (initial) — Score summary\")\n",
        "print(\"=\" * 80)\n",
        "print(table1.to_string(index=False))\n",
        "print(f\"\\nSaved: {table1_path}\")\n",
        "\n",
        "# --- Table 2 (selection rates only, IRT params added in Cell 4) ---\n",
        "author_stats = author_meta[[\"item_index\", \"item_label\", \"item_code\", \"genre\"]].copy()\n",
        "author_stats[\"Percent Selected\"] = [round(item_block.iloc[:, i].mean() * 100, 1) for i in author_stats[\"item_index\"]]\n",
        "author_stats = author_stats.rename(columns={\n",
        "    \"item_label\": \"Author Name\",\n",
        "    \"item_code\": \"Item Code\",\n",
        "    \"genre\": \"Genre\",\n",
        "}).sort_values(\"Percent Selected\", ascending=False).reset_index(drop=True)\n",
        "author_stats.index = author_stats.index + 1\n",
        "author_stats.index.name = \"Rank\"\n",
        "\n",
        "table2_sel_path = OUTPUT_DIR / \"Table_2_selection_rates.csv\"\n",
        "author_stats.to_csv(table2_sel_path)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TABLE 2 (pre-IRT) — Author selection rates\")\n",
        "print(\"=\" * 80)\n",
        "print(author_stats.head(20).to_string())\n",
        "print(f\"\\nSaved: {table2_sel_path}\")\n",
        "print(f\"Mean author selection rate: {author_stats['Percent Selected'].mean():.2f}%\")\n",
        "print(f\"Easiest author: {author_stats.iloc[0]['Author Name']} ({author_stats.iloc[0]['Percent Selected']}%)\")\n",
        "print(f\"Hardest author: {author_stats.iloc[-1]['Author Name']} ({author_stats.iloc[-1]['Percent Selected']}%)\")\n",
        "\n",
        "# --- Table 3 (foil selection: per-foil rates with foil names) ---\n",
        "fa_counts = data[\"false_alarms\"].value_counts().sort_index()\n",
        "fa_dist = pd.DataFrame({\n",
        "    \"Errors\": fa_counts.index,\n",
        "    \"N participants\": fa_counts.values,\n",
        "    \"% of sample\": [round(v / N * 100, 1) for v in fa_counts.values],\n",
        "})\n",
        "\n",
        "foil_rates = foil_meta[[\"item_index\", \"item_label\", \"item_code\", \"genre\"]].copy()\n",
        "foil_rates[\"Percent Selected\"] = [round(item_block.iloc[:, i].mean() * 100, 1) for i in foil_rates[\"item_index\"]]\n",
        "foil_rates = foil_rates.rename(columns={\"item_label\": \"Foil Name\", \"item_code\": \"Item Code\", \"genre\": \"Genre\"})\n",
        "foil_rates = foil_rates.sort_values(\"Percent Selected\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "table3_path = OUTPUT_DIR / \"Table_3.csv\"\n",
        "foil_rates.to_csv(table3_path, index=False)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"TABLE 3 — False-alarm (foil selection) distribution\")\n",
        "print(\"=\" * 80)\n",
        "print(foil_rates.to_string(index=False))\n",
        "print(f\"\\nSaved: {table3_path}\")\n",
        "print(f\"Mean false alarms per participant: {data['false_alarms'].mean():.2f} (SD={data['false_alarms'].std():.2f})\")\n",
        "print(f\"Participants with 0 false alarms: {(data['false_alarms'] == 0).sum()} ({(data['false_alarms'] == 0).mean() * 100:.1f}%)\")\n",
        "print(\"\\nFalse-alarm count distribution (errors per participant):\")\n",
        "print(fa_dist.to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "10480b46",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bartlett's test of sphericity:\n",
            "  chi2 = 88020.9, p = 0.0000e+00\n",
            "KMO sampling adequacy: 0.969\n",
            "\n",
            "Top 10 eigenvalues:\n",
            "  Factor  1:  22.18\n",
            "  Factor  2:   9.88\n",
            "  Factor  3:   3.54\n",
            "  Factor  4:   3.19\n",
            "  Factor  5:   2.39\n",
            "  Factor  6:   2.15\n",
            "  Factor  7:   1.79\n",
            "  Factor  8:   1.61\n",
            "  Factor  9:   1.40\n",
            "  Factor 10:   1.31\n",
            "\n",
            "================================================================================\n",
            "STEP 2 — Guessing-factor filtering\n",
            "================================================================================\n",
            "Guessing factor chosen: Factor_2\n",
            "Low-select threshold  : 10.0%\n",
            "Load threshold        : 0.4\n",
            "Items flagged/removal : 0\n",
            "\n",
            "================================================================================\n",
            "TABLE 4 — Two-factor loadings on retained items (n=102)\n",
            "================================================================================\n",
            "           Author Name Item Code Genre  pct_selected  Factor_1  Factor_2\n",
            "     Michel Houlleback     mod24   mod        42.943     0.715    -0.278\n",
            "     William Thackeray      cla7   cla        52.207     0.705    -0.001\n",
            "      Somerset Maugham     cla14   cla        66.104     0.702     0.017\n",
            "           John Fowles     cla11   cla        50.954     0.697     0.030\n",
            "         Milorad Pavic     cla15   cla        33.951     0.678     0.040\n",
            "           Isaac Babel     soc14   soc        71.008     0.673    -0.242\n",
            "      Colin McCullough     cla23   cla        44.578     0.668    -0.221\n",
            "        Victor Erofeev      soc3   soc        43.815     0.638     0.068\n",
            "    Henryk Sienkiewicz      cla4   cla        42.125     0.635     0.002\n",
            "Gabriel Garsia Marquez      cla2   cla        75.695     0.633     0.055\n",
            "        Mikhail Weller      mod5   mod        46.757     0.632     0.025\n",
            "     Catherine Stokett     mod15   mod        30.790     0.629    -0.651\n",
            "          Pavel Sanaev      mod7   mod        40.218     0.626     0.017\n",
            "     Arkady Averchenko      soc5   soc        43.651     0.613     0.009\n",
            "          Yury  Olesha     soc12   soc        74.932     0.597    -0.329\n",
            " Gregory David Roberts     mod17   mod        37.929     0.597    -0.703\n",
            "    Vladimir Voinovich      soc6   soc        47.738     0.592     0.018\n",
            "          Arthur Haley     cla10   cla        51.172     0.592     0.006\n",
            "    Alexandra Marinina      det5   det        57.875     0.582    -0.028\n",
            "     Evgeny Vodolazkin      mod4   mod        32.044     0.580     0.053\n",
            "         Guzel Yakhina     mod10   mod        29.646     0.576     0.013\n",
            "       Vasily Aksyonov      soc4   soc        56.839     0.572     0.024\n",
            "       Herman Melville      cla9   cla        30.136     0.571     0.055\n",
            "         Alexey Ivanov      mod9   mod        30.354     0.570     0.036\n",
            "         Daniil Granin      soc8   soc        49.809     0.570     0.016\n",
            "\n",
            "Factor intercorrelation: r = 0.364\n",
            "1-factor cumulative variance explained: 21.03%\n",
            "2-factor cumulative variance explained: 30.52%\n",
            "Saved: /home/polina/Documents/Cursor_Projects/Russian Author Recognition Test Cursor/data/processed/irt_art_results/Table_4.csv\n",
            "\n",
            "Top 10 Factor_1 loaders:\n",
            "           Author Name Genre  Factor_1\n",
            "     Michel Houlleback   mod     0.715\n",
            "     William Thackeray   cla     0.705\n",
            "      Somerset Maugham   cla     0.702\n",
            "           John Fowles   cla     0.697\n",
            "         Milorad Pavic   cla     0.678\n",
            "           Isaac Babel   soc     0.673\n",
            "      Colin McCullough   cla     0.668\n",
            "        Victor Erofeev   soc     0.638\n",
            "    Henryk Sienkiewicz   cla     0.635\n",
            "Gabriel Garsia Marquez   cla     0.633\n",
            "\n",
            "Top 10 Factor_2 loaders:\n",
            "     Author Name Genre  Factor_2\n",
            "     Jules Verne   cla     0.969\n",
            "    Bernard Shaw   cla     0.944\n",
            "       Dan Bruwn   det     0.813\n",
            "      Harper Lee   cla     0.785\n",
            "    Dmitry Bykov   mod     0.780\n",
            " Zakhar Prilepin   mod     0.769\n",
            "     Neil Gaiman   fan     0.758\n",
            "Chingiz Aitmanov   soc     0.686\n",
            "      Alan Milne   cla     0.686\n",
            "     Dina Rubina   mod     0.683\n"
          ]
        }
      ],
      "source": [
        "# Cell 3 — Step 2: Dimensionality assessment (EFA)\n",
        "\n",
        "# Compatibility shim for sklearn 1.6+ (ensure_all_finite) vs factor_analyzer (force_all_finite).\n",
        "# Rerun-safe: restore sklearn.validation first, then patch only factor_analyzer's local symbol.\n",
        "import importlib\n",
        "import sklearn.utils.validation as _skl_val\n",
        "_skl_val = importlib.reload(_skl_val)\n",
        "_orig_check = _skl_val.check_array\n",
        "\n",
        "def _check_array_compat(*args, **kwargs):\n",
        "    fin = kwargs.pop(\"force_all_finite\", kwargs.get(\"ensure_all_finite\", True))\n",
        "    kwargs[\"ensure_all_finite\"] = fin\n",
        "    return _orig_check(*args, **kwargs)\n",
        "\n",
        "import factor_analyzer.factor_analyzer as _fa_mod\n",
        "_fa_mod.check_array = _check_array_compat\n",
        "\n",
        "from factor_analyzer import FactorAnalyzer\n",
        "from factor_analyzer.factor_analyzer import calculate_bartlett_sphericity, calculate_kmo\n",
        "\n",
        "X_authors = item_block.iloc[:, author_idx].values.astype(float)\n",
        "# Ensure finite (factor_analyzer requires sklearn>=1.3 for check_array; no NaNs keeps analysis well-defined)\n",
        "X_authors = np.nan_to_num(X_authors, nan=0.0, posinf=0.0, neginf=0.0)\n",
        "author_labels = [meta_by_idx.loc[i, \"item_label\"] for i in author_idx]\n",
        "author_genres = [meta_by_idx.loc[i, \"genre\"] for i in author_idx]\n",
        "\n",
        "chi2, p_bart = calculate_bartlett_sphericity(X_authors)\n",
        "kmo_all, kmo_model = calculate_kmo(X_authors)\n",
        "print(\"Bartlett's test of sphericity:\")\n",
        "print(f\"  chi2 = {chi2:.1f}, p = {p_bart:.4e}\")\n",
        "print(f\"KMO sampling adequacy: {kmo_model:.3f}\")\n",
        "\n",
        "fa_init = FactorAnalyzer(\n",
        "    n_factors=min(10, len(author_idx)),\n",
        "    rotation=None,\n",
        "    method=\"minres\",\n",
        "    is_corr_matrix=False,\n",
        ")\n",
        "fa_init.fit(X_authors)\n",
        "ev, _ = fa_init.get_eigenvalues()\n",
        "\n",
        "print(\"\\nTop 10 eigenvalues:\")\n",
        "for i, e in enumerate(ev[:10], 1):\n",
        "    print(f\"  Factor {i:2d}: {e:6.2f}\")\n",
        "\n",
        "fa2 = FactorAnalyzer(n_factors=2, rotation=\"promax\", method=\"minres\", is_corr_matrix=False)\n",
        "fa2.fit(X_authors)\n",
        "\n",
        "loadings_all = pd.DataFrame(\n",
        "    fa2.loadings_,\n",
        "    index=author_idx,\n",
        "    columns=[\"Factor_1\", \"Factor_2\"],\n",
        ")\n",
        "loadings_all[\"Author Name\"] = author_labels\n",
        "loadings_all[\"Genre\"] = author_genres\n",
        "loadings_all[\"pct_selected\"] = [item_block.iloc[:, i].mean() * 100 for i in author_idx]\n",
        "\n",
        "f1_high = loadings_all[loadings_all[\"Factor_1\"].abs() > 0.4]\n",
        "f2_high = loadings_all[loadings_all[\"Factor_2\"].abs() > 0.4]\n",
        "mean_f1 = f1_high[\"pct_selected\"].mean() if len(f1_high) else np.inf\n",
        "mean_f2 = f2_high[\"pct_selected\"].mean() if len(f2_high) else np.inf\n",
        "\n",
        "guess_factor = \"Factor_2\" if mean_f2 <= mean_f1 else \"Factor_1\"\n",
        "GUESS_LOAD_THRESH = 0.40\n",
        "LOW_SELECT_THRESH = 10.0\n",
        "\n",
        "guessing_items = loadings_all[\n",
        "    (loadings_all[guess_factor].abs() > GUESS_LOAD_THRESH)\n",
        "    & (loadings_all[\"pct_selected\"] < LOW_SELECT_THRESH)\n",
        "].index.tolist()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 2 — Guessing-factor filtering\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Guessing factor chosen: {guess_factor}\")\n",
        "print(f\"Low-select threshold  : {LOW_SELECT_THRESH}%\")\n",
        "print(f\"Load threshold        : {GUESS_LOAD_THRESH}\")\n",
        "print(f\"Items flagged/removal : {len(guessing_items)}\")\n",
        "if guessing_items:\n",
        "    cols_show = [\"Author Name\", \"Genre\", guess_factor, \"pct_selected\"]\n",
        "    print(loadings_all.loc[guessing_items, cols_show].sort_values(\"pct_selected\").round(3).to_string())\n",
        "\n",
        "retained_author_idx = [i for i in author_idx if i not in guessing_items]\n",
        "X_retained = item_block.iloc[:, retained_author_idx].values.astype(float)\n",
        "\n",
        "fa2r = FactorAnalyzer(n_factors=2, rotation=\"promax\", method=\"minres\", is_corr_matrix=False)\n",
        "fa2r.fit(X_retained)\n",
        "\n",
        "loadings_ret = pd.DataFrame(\n",
        "    fa2r.loadings_,\n",
        "    index=retained_author_idx,\n",
        "    columns=[\"Factor_1\", \"Factor_2\"],\n",
        ")\n",
        "loadings_ret[\"Author Name\"] = [meta_by_idx.loc[i, \"item_label\"] for i in retained_author_idx]\n",
        "loadings_ret[\"Item Code\"] = [meta_by_idx.loc[i, \"item_code\"] for i in retained_author_idx]\n",
        "loadings_ret[\"Genre\"] = [meta_by_idx.loc[i, \"genre\"] for i in retained_author_idx]\n",
        "loadings_ret[\"pct_selected\"] = [item_block.iloc[:, i].mean() * 100 for i in retained_author_idx]\n",
        "\n",
        "phi = fa2r.phi_\n",
        "factor_corr = float(phi[0, 1]) if phi is not None else np.nan\n",
        "\n",
        "loadings_table = loadings_ret[[\"Author Name\", \"Item Code\", \"Genre\", \"pct_selected\", \"Factor_1\", \"Factor_2\"]].copy()\n",
        "loadings_table = loadings_table.sort_values(\"Factor_1\", ascending=False)\n",
        "\n",
        "table4_path = OUTPUT_DIR / \"Table_4.csv\"\n",
        "loadings_table.to_csv(table4_path, index=False)\n",
        "\n",
        "fa1 = FactorAnalyzer(n_factors=1, rotation=None, method=\"minres\", is_corr_matrix=False)\n",
        "fa1.fit(X_retained)\n",
        "var1 = fa1.get_factor_variance()\n",
        "var2 = fa2r.get_factor_variance()\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(f\"TABLE 4 — Two-factor loadings on retained items (n={len(retained_author_idx)})\")\n",
        "print(\"=\" * 80)\n",
        "print(loadings_table.head(25).round(3).to_string(index=False))\n",
        "print(f\"\\nFactor intercorrelation: r = {factor_corr:.3f}\")\n",
        "print(f\"1-factor cumulative variance explained: {var1[2][0] * 100:.2f}%\")\n",
        "print(f\"2-factor cumulative variance explained: {var2[2][1] * 100:.2f}%\")\n",
        "print(f\"Saved: {table4_path}\")\n",
        "\n",
        "print(\"\\nTop 10 Factor_1 loaders:\")\n",
        "print(loadings_table.nlargest(10, \"Factor_1\")[[\"Author Name\", \"Genre\", \"Factor_1\"]].round(3).to_string(index=False))\n",
        "print(\"\\nTop 10 Factor_2 loaders:\")\n",
        "print(loadings_table.nlargest(10, \"Factor_2\")[[\"Author Name\", \"Genre\", \"Factor_2\"]].round(3).to_string(index=False))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "714996ee",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 4 — Step 3: 2PL IRT model + Fig. 1\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from girth import twopl_mml\n",
        "\n",
        "X_ret = item_block.iloc[:, retained_author_idx].values.astype(int)\n",
        "estimates = twopl_mml(X_ret.T)\n",
        "\n",
        "a_params = np.asarray(estimates[\"Discrimination\"])\n",
        "b_params = np.asarray(estimates[\"Difficulty\"])\n",
        "\n",
        "ret_labels = [meta_by_idx.loc[i, \"item_label\"] for i in retained_author_idx]\n",
        "ret_codes = [meta_by_idx.loc[i, \"item_code\"] for i in retained_author_idx]\n",
        "ret_genres = [meta_by_idx.loc[i, \"genre\"] for i in retained_author_idx]\n",
        "ret_pct = [round(item_block.iloc[:, i].mean() * 100, 1) for i in retained_author_idx]\n",
        "\n",
        "table2_irt = pd.DataFrame({\n",
        "    \"Author Name\": ret_labels,\n",
        "    \"Item Code\": ret_codes,\n",
        "    \"Genre\": ret_genres,\n",
        "    \"Percent Selected\": ret_pct,\n",
        "    \"a (discrimination)\": np.round(a_params, 3),\n",
        "    \"b (difficulty)\": np.round(b_params, 3),\n",
        "})\n",
        "table2_irt = table2_irt.sort_values(\"Percent Selected\", ascending=False).reset_index(drop=True)\n",
        "table2_irt.index = table2_irt.index + 1\n",
        "table2_irt.index.name = \"Rank\"\n",
        "\n",
        "table2_path = OUTPUT_DIR / \"Table_2.csv\"\n",
        "table2_irt.to_csv(table2_path)\n",
        "\n",
        "print(\"=\" * 90)\n",
        "print(f\"TABLE 2 — Selection rates + 2PL parameters (n={len(retained_author_idx)} retained authors)\")\n",
        "print(\"=\" * 90)\n",
        "print(table2_irt.head(25).to_string())\n",
        "print(f\"\\nSaved: {table2_path}\")\n",
        "print(f\"\\nDiscrimination (a): mean={a_params.mean():.3f}, sd={a_params.std():.3f}, range=[{a_params.min():.3f}, {a_params.max():.3f}]\")\n",
        "print(f\"Difficulty (b): mean={b_params.mean():.3f}, sd={b_params.std():.3f}, range=[{b_params.min():.3f}, {b_params.max():.3f}]\")\n",
        "\n",
        "def icc(theta, a, b):\n",
        "    return 1.0 / (1.0 + np.exp(-a * (theta - b)))\n",
        "\n",
        "theta = np.linspace(-4, 4, 200)\n",
        "\n",
        "high_a = table2_irt.nlargest(min(8, len(table2_irt)), \"a (discrimination)\").sort_values(\"b (difficulty)\")\n",
        "if len(high_a) >= 4:\n",
        "    eff_idx = [high_a.index[0], high_a.index[len(high_a)//3], high_a.index[(2*len(high_a))//3], high_a.index[-1]]\n",
        "else:\n",
        "    eff_idx = high_a.index.tolist()\n",
        "eff_items = high_a.loc[eff_idx].drop_duplicates(subset=[\"Author Name\"]).head(4)\n",
        "ineff_items = table2_irt.nsmallest(min(4, len(table2_irt)), \"a (discrimination)\")\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n",
        "\n",
        "ax = axes[0]\n",
        "ax.set_title(\"Effective items (high discrimination)\")\n",
        "for _, row in eff_items.iterrows():\n",
        "    y = icc(theta, row[\"a (discrimination)\"], row[\"b (difficulty)\"])\n",
        "    ax.plot(theta, y, lw=2, label=f\"{row['Author Name']} (a={row['a (discrimination)']:.2f})\")\n",
        "ax.set_xlabel(\"Ability (theta)\")\n",
        "ax.set_ylabel(\"P(correct)\")\n",
        "ax.axhline(0.5, color=\"gray\", ls=\"--\", alpha=0.5)\n",
        "ax.grid(alpha=0.3)\n",
        "ax.legend(fontsize=8)\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "ax = axes[1]\n",
        "ax.set_title(\"Ineffective items (low discrimination)\")\n",
        "for _, row in ineff_items.iterrows():\n",
        "    y = icc(theta, row[\"a (discrimination)\"], row[\"b (difficulty)\"])\n",
        "    ax.plot(theta, y, lw=2, label=f\"{row['Author Name']} (a={row['a (discrimination)']:.2f})\")\n",
        "ax.set_xlabel(\"Ability (theta)\")\n",
        "ax.axhline(0.5, color=\"gray\", ls=\"--\", alpha=0.5)\n",
        "ax.grid(alpha=0.3)\n",
        "ax.legend(fontsize=8)\n",
        "ax.set_ylim(-0.05, 1.05)\n",
        "\n",
        "plt.suptitle(\"Fig. 1 — Item Characteristic Curves\", y=1.02)\n",
        "plt.tight_layout()\n",
        "fig1_path = OUTPUT_DIR / \"Fig_1.png\"\n",
        "plt.savefig(fig1_path, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"Saved: {fig1_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8e3d2935",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 5 — Step 4: Test Information Function + Fig. 2\n",
        "\n",
        "theta = np.linspace(-4, 4, 300)\n",
        "item_info_matrix = np.zeros((len(retained_author_idx), len(theta)))\n",
        "\n",
        "for j in range(len(retained_author_idx)):\n",
        "    p = icc(theta, a_params[j], b_params[j])\n",
        "    item_info_matrix[j, :] = (a_params[j] ** 2) * p * (1 - p)\n",
        "\n",
        "tif = item_info_matrix.sum(axis=0)\n",
        "sem = 1.0 / np.sqrt(np.clip(tif, 1e-9, None))\n",
        "\n",
        "fig, ax1 = plt.subplots(figsize=(10, 5))\n",
        "color_tif = \"#2166ac\"\n",
        "color_sem = \"#b2182b\"\n",
        "\n",
        "ax1.plot(theta, tif, color=color_tif, lw=2.5, label=\"Test Information\")\n",
        "ax1.fill_between(theta, tif, alpha=0.15, color=color_tif)\n",
        "ax1.set_xlabel(\"Ability (theta)\")\n",
        "ax1.set_ylabel(\"Information\", color=color_tif)\n",
        "ax1.tick_params(axis=\"y\", labelcolor=color_tif)\n",
        "ax1.grid(alpha=0.3)\n",
        "\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(theta, sem, color=color_sem, lw=2, ls=\"--\", label=\"SE of measurement\")\n",
        "ax2.set_ylabel(\"Standard Error\", color=color_sem)\n",
        "ax2.tick_params(axis=\"y\", labelcolor=color_sem)\n",
        "\n",
        "ax1.set_title(\"Fig. 2 — Test Information Function\")\n",
        "lines1, labels1 = ax1.get_legend_handles_labels()\n",
        "lines2, labels2 = ax2.get_legend_handles_labels()\n",
        "ax1.legend(lines1 + lines2, labels1 + labels2, loc=\"upper left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "fig2_path = OUTPUT_DIR / \"Fig_2.png\"\n",
        "plt.savefig(fig2_path, dpi=200, bbox_inches=\"tight\")\n",
        "plt.show()\n",
        "print(f\"Saved: {fig2_path}\")\n",
        "\n",
        "peak_theta = float(theta[np.argmax(tif)])\n",
        "peak_info = float(tif.max())\n",
        "print(f\"\\nPeak information = {peak_info:.3f} at theta = {peak_theta:.3f}\")\n",
        "for pt in (-2, 0, 2):\n",
        "    idx = int(np.argmin(np.abs(theta - pt)))\n",
        "    print(f\"Information at theta={pt:+}: {tif[idx]:.3f}\")\n",
        "\n",
        "if peak_theta > 0.5:\n",
        "    print(\"\\nInterpretation: test is more informative at higher ability; consider adding easier items.\")\n",
        "elif peak_theta < -0.5:\n",
        "    print(\"\\nInterpretation: test is more informative at lower ability; consider adding harder items.\")\n",
        "else:\n",
        "    print(\"\\nInterpretation: test information is roughly centered around average ability.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e228e34",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 6 — Step 5: Ability estimation (EAP) + scoring comparisons\n",
        "\n",
        "from girth import ability_eap\n",
        "\n",
        "theta_eap = ability_eap(X_ret.T, estimates[\"Difficulty\"], estimates[\"Discrimination\"])\n",
        "\n",
        "author_sum_ret = item_block.iloc[:, retained_author_idx].sum(axis=1).astype(float)\n",
        "mu_sum, sd_sum = float(author_sum_ret.mean()), float(author_sum_ret.std())\n",
        "mu_eap, sd_eap = float(theta_eap.mean()), float(theta_eap.std())\n",
        "\n",
        "if sd_eap == 0:\n",
        "    raise ValueError(\"EAP SD is zero; cannot rescale ability estimates.\")\n",
        "\n",
        "eap_rescaled = (theta_eap - mu_eap) / sd_eap * sd_sum + mu_sum\n",
        "fa = data[\"false_alarms\"].astype(float).values\n",
        "\n",
        "data[\"standard_ART_ret\"] = author_sum_ret - fa\n",
        "data[\"name_score_ret\"] = author_sum_ret\n",
        "data[\"IRT_no_penalty\"] = eap_rescaled\n",
        "data[\"IRT_minus1\"] = eap_rescaled - 1 * fa\n",
        "data[\"IRT_minus2\"] = eap_rescaled - 2 * fa\n",
        "\n",
        "score_cols_info = [\n",
        "    (f\"Full {len(author_idx)}-author — Standard ART\", \"standard_ART\", len(author_idx)),\n",
        "    (f\"Full {len(author_idx)}-author — Name score\", \"name_score\", len(author_idx)),\n",
        "    (f\"Retained {len(retained_author_idx)}-author — Standard ART\", \"standard_ART_ret\", len(retained_author_idx)),\n",
        "    (f\"Retained {len(retained_author_idx)}-author — Name score\", \"name_score_ret\", len(retained_author_idx)),\n",
        "    (f\"Retained {len(retained_author_idx)}-author — IRT (no penalty)\", \"IRT_no_penalty\", len(retained_author_idx)),\n",
        "    (f\"Retained {len(retained_author_idx)}-author — IRT (-1 x errors)\", \"IRT_minus1\", len(retained_author_idx)),\n",
        "    (f\"Retained {len(retained_author_idx)}-author — IRT (-2 x errors)\", \"IRT_minus2\", len(retained_author_idx)),\n",
        "]\n",
        "\n",
        "rows = []\n",
        "for label, col, ni in score_cols_info:\n",
        "    s = data[col]\n",
        "    rows.append({\n",
        "        \"Scale\": label,\n",
        "        \"N items\": ni,\n",
        "        \"N\": N,\n",
        "        \"M\": round(float(s.mean()), 2),\n",
        "        \"SD\": round(float(s.std()), 2),\n",
        "        \"Min\": round(float(s.min()), 2),\n",
        "        \"Max\": round(float(s.max()), 2),\n",
        "        \"Range\": round(float(s.max() - s.min()), 2),\n",
        "        \"Skew\": round(float(s.skew()), 2),\n",
        "    })\n",
        "\n",
        "table1_ext = pd.DataFrame(rows)\n",
        "table1_final_path = OUTPUT_DIR / \"Table_1.csv\"\n",
        "table1_ext.to_csv(table1_final_path, index=False)\n",
        "\n",
        "print(\"=\" * 95)\n",
        "print(\"TABLE 1 (extended) — All scoring methods\")\n",
        "print(\"=\" * 95)\n",
        "print(table1_ext.to_string(index=False))\n",
        "print(f\"\\nSaved: {table1_final_path}\")\n",
        "\n",
        "score_keys = [\"standard_ART_ret\", \"name_score_ret\", \"IRT_no_penalty\", \"IRT_minus1\", \"IRT_minus2\"]\n",
        "corr_matrix = data[score_keys].corr().round(3)\n",
        "print(\"\\nCorrelations among retained-item scoring methods:\")\n",
        "print(corr_matrix.to_string())\n",
        "\n",
        "participant_scores = pd.DataFrame({\n",
        "    \"participant_id\": np.arange(1, N + 1),\n",
        "    \"source\": data[source_col].astype(str).values,\n",
        "    \"hits\": data[\"hits\"].values,\n",
        "    \"false_alarms\": data[\"false_alarms\"].values,\n",
        "    \"standard_ART\": data[\"standard_ART\"].values,\n",
        "    \"name_score\": data[\"name_score\"].values,\n",
        "    \"standard_ART_ret\": data[\"standard_ART_ret\"].values,\n",
        "    \"name_score_ret\": data[\"name_score_ret\"].values,\n",
        "    \"IRT_no_penalty\": data[\"IRT_no_penalty\"].values,\n",
        "    \"IRT_minus1\": data[\"IRT_minus1\"].values,\n",
        "    \"IRT_minus2\": data[\"IRT_minus2\"].values,\n",
        "})\n",
        "\n",
        "participant_scores_path = OUTPUT_DIR / \"participant_scores.csv\"\n",
        "participant_scores.to_csv(participant_scores_path, index=False)\n",
        "print(f\"Saved: {participant_scores_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ec94b69",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 7 — Step 7: Genre analysis + wave stability (first-two and all-pairs)\n",
        "\n",
        "from itertools import combinations\n",
        "from scipy import stats\n",
        "\n",
        "# Item-level summary for retained author items\n",
        "item_summary = pd.DataFrame({\n",
        "    \"item_index\": retained_author_idx,\n",
        "    \"author\": [meta_by_idx.loc[i, \"item_label\"] for i in retained_author_idx],\n",
        "    \"item_code\": [meta_by_idx.loc[i, \"item_code\"] for i in retained_author_idx],\n",
        "    \"genre\": [meta_by_idx.loc[i, \"genre\"] for i in retained_author_idx],\n",
        "    \"pct_sel\": [item_block.iloc[:, i].mean() * 100 for i in retained_author_idx],\n",
        "    \"a\": a_params,\n",
        "    \"b\": b_params,\n",
        "})\n",
        "\n",
        "genre_summary = item_summary.groupby(\"genre\", dropna=False).agg(\n",
        "    n_items=(\"author\", \"count\"),\n",
        "    mean_pct_sel=(\"pct_sel\", \"mean\"),\n",
        "    mean_a=(\"a\", \"mean\"),\n",
        "    sd_a=(\"a\", \"std\"),\n",
        "    mean_b=(\"b\", \"mean\"),\n",
        "    sd_b=(\"b\", \"std\"),\n",
        ").round(3).sort_values(\"mean_b\")\n",
        "\n",
        "genre_path = OUTPUT_DIR / \"genre_comparison.csv\"\n",
        "genre_summary.to_csv(genre_path)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"STEP 7a — IRT parameters by genre\")\n",
        "print(\"=\" * 80)\n",
        "print(genre_summary.to_string())\n",
        "print(f\"\\nSaved: {genre_path}\")\n",
        "\n",
        "# One-way ANOVA on b and a across genres with >=2 items\n",
        "valid_genres_b = [g for g, grp in item_summary.groupby(\"genre\") if len(grp) >= 2]\n",
        "if len(valid_genres_b) >= 2:\n",
        "    groups_b = [item_summary.loc[item_summary[\"genre\"] == g, \"b\"].values for g in valid_genres_b]\n",
        "    F_b, p_b = stats.f_oneway(*groups_b)\n",
        "    print(f\"\\nANOVA difficulty (b): F={F_b:.3f}, p={p_b:.5f}\")\n",
        "else:\n",
        "    print(\"\\nNot enough genres with >=2 items to run ANOVA on difficulty (b).\")\n",
        "\n",
        "valid_genres_a = [g for g, grp in item_summary.groupby(\"genre\") if len(grp) >= 2]\n",
        "if len(valid_genres_a) >= 2:\n",
        "    groups_a = [item_summary.loc[item_summary[\"genre\"] == g, \"a\"].values for g in valid_genres_a]\n",
        "    F_a, p_a = stats.f_oneway(*groups_a)\n",
        "    print(f\"ANOVA discrimination (a): F={F_a:.3f}, p={p_a:.5f}\")\n",
        "else:\n",
        "    print(\"Not enough genres with >=2 items to run ANOVA on discrimination (a).\")\n",
        "\n",
        "# Wave stability\n",
        "waves = sorted(data[source_col].astype(str).dropna().unique().tolist())\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"STEP 7b — Wave stability\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"Waves found: {waves}\")\n",
        "\n",
        "if len(waves) >= 2:\n",
        "    wave_rates = {}\n",
        "    for w in waves:\n",
        "        mask = data[source_col].astype(str) == w\n",
        "        rates = item_block.iloc[mask.values, retained_author_idx].mean(axis=0) * 100\n",
        "        wave_rates[w] = pd.Series(rates.values, index=retained_author_idx)\n",
        "        print(f\"Wave '{w}': N={int(mask.sum())}, mean author selection={rates.mean():.2f}%\")\n",
        "\n",
        "    # First-two waves (for direct comparability with prior script)\n",
        "    w1, w2 = waves[0], waves[1]\n",
        "    r12, p12 = stats.pearsonr(wave_rates[w1].values, wave_rates[w2].values)\n",
        "    print(f\"\\nFirst-two wave correlation ({w1} vs {w2}): r={r12:.4f}, p={p12:.3e}\")\n",
        "\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    ax.scatter(wave_rates[w1].values, wave_rates[w2].values, alpha=0.7, s=28)\n",
        "    lim = max(ax.get_xlim()[1], ax.get_ylim()[1])\n",
        "    ax.plot([0, lim], [0, lim], \"k--\", alpha=0.4, label=\"identity\")\n",
        "    ax.set_xlabel(f\"Selection rate — {w1} (%)\")\n",
        "    ax.set_ylabel(f\"Selection rate — {w2} (%)\")\n",
        "    ax.set_title(f\"Fig. 5 — Wave stability ({w1} vs {w2})\\nr={r12:.3f}, p={p12:.2e}\")\n",
        "    ax.grid(alpha=0.3)\n",
        "    ax.legend()\n",
        "    plt.tight_layout()\n",
        "    fig5_path = OUTPUT_DIR / \"Fig_5.png\"\n",
        "    plt.savefig(fig5_path, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.show()\n",
        "    print(f\"Saved: {fig5_path}\")\n",
        "\n",
        "    t_stat, p_t = stats.ttest_ind(\n",
        "        data.loc[data[source_col].astype(str) == w1, \"standard_ART\"],\n",
        "        data.loc[data[source_col].astype(str) == w2, \"standard_ART\"],\n",
        "        equal_var=False,\n",
        "        nan_policy=\"omit\",\n",
        "    )\n",
        "    print(f\"Mean standard_ART by wave: {w1}={data.loc[data[source_col].astype(str) == w1, 'standard_ART'].mean():.3f}, {w2}={data.loc[data[source_col].astype(str) == w2, 'standard_ART'].mean():.3f}\")\n",
        "    print(f\"Welch t-test ({w1} vs {w2}): t={t_stat:.3f}, p={p_t:.5f}\")\n",
        "\n",
        "    # All pairwise wave comparisons\n",
        "    pair_rows = []\n",
        "    def safe_logit_pct(pct):\n",
        "        p = np.clip(np.asarray(pct) / 100.0, 0.001, 0.999)\n",
        "        return np.log(p / (1 - p))\n",
        "\n",
        "    for wa, wb in combinations(waves, 2):\n",
        "        a_vals = wave_rates[wa].values\n",
        "        b_vals = wave_rates[wb].values\n",
        "        r, p = stats.pearsonr(a_vals, b_vals)\n",
        "        logit_diff = safe_logit_pct(b_vals) - safe_logit_pct(a_vals)\n",
        "        pair_rows.append({\n",
        "            \"wave_a\": wa,\n",
        "            \"wave_b\": wb,\n",
        "            \"pearson_r\": r,\n",
        "            \"pearson_p\": p,\n",
        "            \"mean_abs_delta_pct\": float(np.mean(np.abs(b_vals - a_vals))),\n",
        "            \"mean_delta_logit\": float(np.mean(logit_diff)),\n",
        "            \"sd_delta_logit\": float(np.std(logit_diff)),\n",
        "        })\n",
        "\n",
        "    pairwise_df = pd.DataFrame(pair_rows).sort_values([\"wave_a\", \"wave_b\"]).round(5)\n",
        "    pairwise_path = OUTPUT_DIR / \"wave_stability_pairwise.csv\"\n",
        "    pairwise_df.to_csv(pairwise_path, index=False)\n",
        "    print(\"\\nAll-pairs wave stability:\")\n",
        "    print(pairwise_df.to_string(index=False))\n",
        "    print(f\"Saved: {pairwise_path}\")\n",
        "else:\n",
        "    print(\"Only one wave found; skipping wave stability analysis.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9b02fa5c",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell 8 — Summary and output inventory\n",
        "\n",
        "import glob\n",
        "\n",
        "output_files = sorted(\n",
        "    glob.glob(str(OUTPUT_DIR / \"Table_*.csv\"))\n",
        "    + glob.glob(str(OUTPUT_DIR / \"Fig_*.png\"))\n",
        "    + glob.glob(str(OUTPUT_DIR / \"participant_scores.csv\"))\n",
        "    + glob.glob(str(OUTPUT_DIR / \"genre_comparison.csv\"))\n",
        "    + glob.glob(str(OUTPUT_DIR / \"wave_stability_pairwise.csv\"))\n",
        ")\n",
        "output_files = list(dict.fromkeys(output_files))\n",
        "\n",
        "print(\"=\" * 90)\n",
        "print(\"ALL OUTPUT FILES\")\n",
        "print(\"=\" * 90)\n",
        "for fp in output_files:\n",
        "    size_kb = os.path.getsize(fp) / 1024\n",
        "    print(f\"{Path(fp).name:35s}  {size_kb:8.2f} KB\")\n",
        "\n",
        "print(\"\\nAnalysis complete.\")\n",
        "print(f\"Participants: {N}\")\n",
        "print(f\"Total cleaned items: {len(item_labels)}\")\n",
        "print(f\"Authors in metadata: {len(author_idx)}\")\n",
        "print(f\"Foils in metadata: {len(foil_idx)}\")\n",
        "print(f\"Retained authors after Step 2: {len(retained_author_idx)}\")\n",
        "print(f\"Guessing items removed: {len(guessing_items)}\")\n",
        "print(f\"Output directory: {OUTPUT_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
