% main.tex â€” Russian ART IRT Validation Report (APA-ish LaTeX template)
% Compile: pdflatex main.tex (twice) OR latexmk -pdf main.tex
%
% Put your figures in ./figures/ with these filenames:
%   scree_plot.png
%   factor_loadings_heatmap.png
%   endorsement_histogram.png
%   item_parameter_distributions.png
%   test_information_function.png
%   item_information_curves.png
%   icc_all_items.png
%   theta_distribution.png
%   scoring_comparison_matrix.png
%   short_vs_full_tif.png
%
% Put your tables (exported from CSV) as LaTeX in ./tables/:
%   table_data_quality.tex
%   table_eigenvalues.tex
%   table_model_fit.tex
%   table_flags.tex
%   table_hard_items.tex
%   table_easy_items.tex
%   table_short_scale.tex
%   table_score_corr.tex
%
% Tip: If you prefer not to pre-export tables, you can paste tabulars directly.

\documentclass[11pt]{article}

% ---------- Page + typography ----------
\usepackage[margin=1in]{geometry}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc} % remove if using lualatex/xelatex
\usepackage{lmodern}
\usepackage{microtype}
\usepackage{setspace}
\onehalfspacing

% ---------- Figures + tables ----------
\usepackage{graphicx}
\usepackage{float}
\usepackage{booktabs}
\usepackage{threeparttable}
\usepackage{longtable}
\usepackage{array}
\usepackage{caption}
\usepackage{subcaption}

% ---------- Links + refs ----------
\usepackage[hidelinks]{hyperref}
\usepackage[nameinlink,noabbrev]{cleveref}

% ---------- APA-ish headings ----------
\usepackage{titlesec}
\titleformat{\section}{\large\bfseries}{\thesection.}{0.5em}{}
\titleformat{\subsection}{\normalsize\bfseries}{\thesubsection.}{0.5em}{}
\titlespacing*{\section}{0pt}{1.0em}{0.4em}
\titlespacing*{\subsection}{0pt}{0.8em}{0.3em}

% ---------- Citations (APA-like author-year) ----------
\usepackage[authoryear,round]{natbib}
\bibliographystyle{apalike}

% ---------- Convenience macros ----------
\newcommand{\N}{1{,}835} % update if needed
\newcommand{\FullItems}{101}
\newcommand{\ShortItems}{60}

\begin{document}

% ===================== Title Page =====================
\begin{center}
{\small Running head: RUSSIAN AUTHOR RECOGNITION TEST (ART) --- IRT VALIDATION}\\[1.2em]

{\LARGE \textbf{Development and Item Response Theory Validation of a Russian Author Recognition Test (ART)}}\\[0.8em]

{\large \textbf{[Author Name 1]}, \textbf{[Author Name 2]}}\\
{\normalsize [Affiliation 1]}\\
{\normalsize [Affiliation 2]}\\[1.0em]

{\normalsize Corresponding author: [Name], [Email]}\\[0.5em]
\end{center}

\vspace{1em}

\section*{Abstract}
\noindent
The Author Recognition Test (ART) is an established proxy measure of print exposure. This report summarizes psychometric analyses of a Russian-language ART administered online to \textbf{N = \N} adults. Author items were analyzed with a two-parameter logistic (2PL) Item Response Theory (IRT) model. Exploratory factor analysis (EFA) indicated a dominant factor consistent with a largely unidimensional print-exposure construct. Item parameters showed a wide difficulty range and generally strong discrimination (mean discrimination \(\approx 1.47\)). The test information function peaked near average ability levels (around \(\theta \approx 0\)), with lower precision at the extremes. A \ShortItems-item short form retained approximately 77\% of full-test information and correlated strongly with full-scale ability estimates (\(r \approx .97\)). Results support the Russian ART as a reliable, culturally grounded indicator of print exposure.

\noindent\textbf{Keywords:} print exposure; Author Recognition Test; item response theory; 2PL model; test information function; Russian reading habits

\newpage

% ===================== Main Paper =====================

\section{Introduction}
Print exposure---accumulated experience with written texts---predicts vocabulary growth, reading fluency, and comprehension. The Author Recognition Test (ART) provides a fast proxy for print exposure by measuring recognition of real author names while discouraging indiscriminate responding via foils (non-authors). Because author familiarity is culturally contingent, each language community requires a locally grounded ART.

The present report focuses on the psychometric evaluation of a Russian ART pretest dataset using IRT. We emphasize interpretability (difficulty, discrimination, information), short-form construction, and structural comparison to English ART IRT work (e.g., \citealp{moore2015reading}), without mapping item identities across languages.

\section{Method}

\subsection{Participants}
The analyzed dataset includes \textbf{N = \N} adult native speakers of Russian recruited online. [Add recruitment channels, age range, inclusion/exclusion, ethics/informed consent.]

\subsection{Materials: Russian ART}
The full instrument contained \textbf{\FullItems} scored author items and foils. Author selection followed Russian ART development criteria: fiction-only inclusion; exclusion of non-fiction/public-intellectual names; and empirical grounding in reading-market and reader-platform visibility rather than school curricula. Foils were constructed to be plausible personal names but not fiction authors. [Add exact sources and exclusion rules from your Russian ART paper.]

\subsection{Procedure}
Participants completed an online checklist ART. They marked names they believed were real authors and were warned that selecting foils would reduce their score. [Add details: randomized order, image-based presentation to prevent searching, time constraints if any.]

\subsection{Analytic Strategy}
\textbf{Dimensionality.} We examined unidimensionality using EFA and parallel analysis.\\
\textbf{IRT model.} We fitted a 2PL model with item difficulty \(b\) and discrimination \(a\).\\
\textbf{Precision.} We evaluated the test information function (TIF) and standard errors across \(\theta\).\\
\textbf{Short form.} We constructed a \ShortItems-item form prioritizing high-information items and coverage across the difficulty continuum.\\
\textbf{Scoring.} We compared IRT \(\theta\) to classical scores (hits minus false alarms) and related variants.

\section{Results}

\subsection{Data Quality and Convergence}
The dataset included responses from two recruitment sources (Source 1: $n = 800$; Source 2: $n = 1{,}035$). A small proportion of item responses were missing (1,168 cells across 19 columns). Classical scores (hits minus false alarms) averaged $M = 38.43$ ($SD = 20.52$), with mean false-alarm rate of 8.4\%.

\begin{table}[H]
\centering
\caption{Data quality and convergence summary}
\label{tab:data_quality}
\input{tables/table_data_quality.tex}
\end{table}

\subsection{Dimensionality (EFA)}
EFA showed a strong first factor (eigenvalue = 21.44, explaining 23.1\% of variance) and a steep eigenvalue drop thereafter (second factor eigenvalue = 9.81), consistent with a dominant print-exposure dimension. The Kaiser-Meyer-Olkin measure of sampling adequacy was excellent (KMO = 0.97). Parallel analysis suggested that additional factors, if present, were substantially smaller and likely reflect residual structure (e.g., clusters of author familiarity by genre or era).

\begin{figure}[H]
\centering
\includegraphics[width=0.92\linewidth]{figures/scree_plot.png}
\caption{Scree plot with parallel analysis threshold.}
\label{fig:scree}
\end{figure}

\begin{table}[H]
\centering
\caption{Top eigenvalues from EFA with parallel analysis}
\label{tab:eigenvalues}
\input{tables/table_eigenvalues.tex}
\end{table}

\subsection{Model Fit}
We compared a 1-factor versus 2-factor structure for descriptive purposes. The 2-factor model improved variance explained modestly (from 23.1\% to 33.6\%), but the dominant first factor supports reporting a primary \(\theta\) estimate for most applications.

\begin{table}[H]
\centering
\caption{Model fit comparison}
\label{tab:model_fit}
\input{tables/table_model_fit.tex}
\end{table}

\subsection{Item Parameters and Item Quality Flags}
Difficulty \(b\) indexes how widely known an author is (higher \(b\) = harder), and discrimination \(a\) indexes how diagnostic an item is (higher \(a\) = stronger separation). Difficulty parameters ranged from $b = -3.10$ (Agatha Christie) to $b = +5.11$ (Jostein Gaarder), with mean $b = 0.34$ ($SD = 1.57$). Discrimination parameters ranged from $a = 0.21$ to $a = 3.36$, with mean $a = 1.47$ ($SD = 0.62$).

We flagged items with low discrimination ($a < 0.5$; $n = 5$) or extreme difficulty ($|b| > 3.0$; $n = 7$) as candidates for revision or omission in short forms.

\begin{figure}[H]
\centering
\includegraphics[width=0.92\linewidth]{figures/endorsement_histogram.png}
\caption{Distribution of author endorsement (recognition) rates.}
\label{fig:endorse}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.92\linewidth]{figures/item_parameter_distributions.png}
\caption{Item parameter distributions (difficulty and discrimination) and difficulty--endorsement relationship.}
\label{fig:itemparams}
\end{figure}

\begin{table}[H]
\centering
\caption{Item quality flags (counts)}
\label{tab:flags}
\input{tables/table_flags.tex}
\end{table}

\subsection{High-Reader vs Low-Reader Signal Items (Authors)}
To make test design transparent, we identify two sets of items with ``decent'' discrimination (e.g., \(a \ge 1.0\)): (a) the hardest discriminating items (high-reader signal) and (b) the easiest discriminating items (low-reader signal). These sets anchor interpretation at both ends of the ability continuum.

\begin{table}[H]
\centering
\caption{Top 10 hardest items with decent discrimination (high-reader signal)}
\label{tab:hard_items}
\input{tables/table_hard_items.tex}
\end{table}

\begin{table}[H]
\centering
\caption{Top 10 easiest items with decent discrimination (low-reader signal)}
\label{tab:easy_items}
\input{tables/table_easy_items.tex}
\end{table}

\subsection{Item Characteristic Curves and Item Information}
ICCs show how recognition probability changes with \(\theta\). Item information curves show where each item contributes measurement precision.

\begin{figure}[H]
\centering
\includegraphics[width=0.92\linewidth]{figures/icc_all_items.png}
\caption{Item characteristic curves (ICCs) for author items; flagged items highlighted.}
\label{fig:icc}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.92\linewidth]{figures/item_information_curves.png}
\caption{Item information curves for highly discriminating items.}
\label{fig:iic}
\end{figure}

\subsection{Test Information and Theta Distribution}
The TIF indicates where the test measures most precisely. The test information function peaked at $\theta \approx 0.14$ with maximum information of 38.81. Precision is maximal where information is highest and standard error is lowest.

Participant ability estimates were approximately normally distributed ($M = 0.007$, $SD = 1.00$, range $= [-3.40, 3.96]$). The reliability estimate was excellent (0.965).

\begin{figure}[H]
\centering
\includegraphics[width=0.92\linewidth]{figures/test_information_function.png}
\caption{Test information function (TIF) and implied standard error across \(\theta\).}
\label{fig:tif}
\end{figure}

\begin{figure}[H]
\centering
\includegraphics[width=0.92\linewidth]{figures/theta_distribution.png}
\caption{Distribution of IRT ability estimates (\(\theta\)) and relationship to standard error.}
\label{fig:theta}
\end{figure}

\subsection{Short Form (\ShortItems\ items): Information Retention and Agreement}
The \ShortItems-item short form was constructed to retain high-information items and maintain coverage across difficulty levels. The short form retained approximately 77.3\% of full-test information (total information: 2,730 vs.\ 3,531 for full scale) and showed very high agreement with the full scale ($r \approx .97$).

\begin{figure}[H]
\centering
\includegraphics[width=0.92\linewidth]{figures/short_vs_full_tif.png}
\caption{Full versus short-form test information (precision) and efficiency.}
\label{fig:shortfull}
\end{figure}

\begin{table}[H]
\centering
\caption{Short-form candidate list and information weights}
\label{tab:short_candidates}
\input{tables/table_short_scale.tex}
\end{table}

\subsection{Scoring Comparisons}
We compared IRT \(\theta\) to classical scoring (hits minus false alarms) and related indices. IRT \(\theta\) showed strong correlation with classical scores ($r = .90$) and raw hit counts ($r = .99$). The weak correlation between \(\theta\) and false-alarm rate ($r = .13$) indicates that IRT scoring is relatively robust to guessing behavior.

\begin{figure}[H]
\centering
\includegraphics[width=0.92\linewidth]{figures/scoring_comparison_matrix.png}
\caption{Scoring comparison matrix and correlations among score variants.}
\label{fig:scoring}
\end{figure}

\begin{table}[H]
\centering
\caption{Correlations among scoring methods}
\label{tab:scorecorr}
\input{tables/table_score_corr.tex}
\end{table}

\section{Discussion}
Overall, the Russian ART shows psychometric properties consistent with ART research in other languages: a dominant print-exposure factor; broad difficulty coverage; generally strong discrimination; and peak precision near the center of the ability distribution. The short form provides a practical alternative with limited loss of precision.

\subsection{Comparison to English ART IRT Work (Structure, Not Authors)}
English ART IRT studies (e.g., \citealp{moore2015reading}) report similar patterns: most information in the mid-range of ability, reduced precision at extremes, and strong agreement between classical and IRT scoring. The Russian ART aligns structurally with these findings while remaining culturally specific at the item-content level.

\subsection{Limitations and Next Steps}
[Add: representativeness of recruitment channels; DIF across demographics; planned cross-validation with reading speed, vocabulary, comprehension, morphosyntax; stability across time.]

\section{Conclusion}
The Russian ART provides a reliable, interpretable, and time-efficient measure of print exposure suitable for psycholinguistic research. IRT results support both full and short-form usage and enable transparent item-level diagnostics.

% ===================== References =====================
\newpage
\bibliography{references}

% ===================== Appendices =====================
\newpage
\appendix

\section{Appendix A: Full Item Parameter Table}
\noindent
The complete item parameter table for all 101 authors is available in the supplementary materials. Parameters include discrimination ($a$), difficulty ($b$), and endorsement rate for each author item, sorted by endorsement frequency.

% Example placeholder:
% \input{tables/table_item_parameters_full_longtable.tex}

\section{Appendix B: Short Form (\ShortItems\ items)}
\noindent
This appendix lists the \ShortItems items selected for the short form, with difficulty, discrimination, endorsement rate, and information weights. See Table \ref{tab:short_candidates} for the complete list.

\section{Appendix C: Item Flagging Rules}
\noindent
Items were flagged based on the following criteria:
\begin{itemize}
    \item \textbf{Low discrimination:} $a < 0.5$ (5 items flagged)
    \item \textbf{Extreme difficulty:} $|b| > 3.0$ (7 items flagged)
    \item \textbf{Floor effect:} endorsement rate $< 5\%$ (1 item flagged)
    \item \textbf{Ceiling effect:} endorsement rate $> 95\%$ (7 items flagged)
\end{itemize}
Flagged items were excluded from the short-form selection process but retained in the full test for research purposes.

\end{document}
